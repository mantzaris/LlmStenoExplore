{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: /home/meow/Documents/repos/LlmStenoExplore/models/phi3/Phi-3-mini-4k-instruct-q4.gguf exists: True\n",
      "BOS token id: 1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Sequence\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "#   notebook is in LlmStenoExplore/notebooks\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "MODEL_PATH = REPO_ROOT / \"models/phi3/Phi-3-mini-4k-instruct-q4.gguf\"\n",
    "print(\"Using model:\", MODEL_PATH, \"exists:\", MODEL_PATH.exists())\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=str(MODEL_PATH),\n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=0,      # CPU-only\n",
    "    logits_all=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "def token_bos_id() -> int:\n",
    "    return llm.token_bos()\n",
    "\n",
    "def encode_text(text: str, add_bos: bool = True) -> List[int]:\n",
    "    return llm.tokenize(text.encode(\"utf-8\"), add_bos=add_bos, special=False)\n",
    "\n",
    "def decode_tokens(tokens: List[int], prev_tokens: List[int] | None = None) -> str:\n",
    "    b = llm.detokenize(tokens, prev_tokens=prev_tokens, special=False)\n",
    "    return b.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "print(\"BOS token id:\", token_bos_id())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = \"THE CURRENT SYSTEM HAS REPEATEDLY FAILED\"\n",
    "k = \"Here it is: the infamous British roasted board with mint sauce. How to make it perfect.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32064\n",
      "Number of word-like tokens: 15562\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Simple word tokenizer: lowercase, only a-z, no punctuation, no numbers\n",
    "WORD_PATTERN = re.compile(r\"[a-z]+\")\n",
    "\n",
    "def simple_word_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert a string to a list of simple word tokens.\n",
    "    - Lowercase\n",
    "    - Drop punctuation and numbers\n",
    "    - Only keep [a-z]+ spans\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    return WORD_PATTERN.findall(text_lower)\n",
    "\n",
    "\n",
    "def build_word_token_maps(model: Llama) -> Tuple[Dict[str, int], Dict[int, str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build a mapping between plain words and model token ids.\n",
    "\n",
    "    We only keep tokens that detokenize to something that, after stripping\n",
    "    whitespace and lowercasing, matches [a-z]+ exactly.\n",
    "    \"\"\"\n",
    "    word_to_token_id: Dict[str, int] = {}\n",
    "    token_id_to_word: Dict[int, str] = {}\n",
    "    candidate_token_ids: List[int] = []\n",
    "\n",
    "    vocabulary_size = model.n_vocab()\n",
    "    print(f\"Vocabulary size: {vocabulary_size}\")\n",
    "\n",
    "    for token_id in range(vocabulary_size):\n",
    "        token_text = decode_tokens([token_id])          # model -> text\n",
    "        token_text_clean = token_text.strip().lower()   # remove leading/trailing spaces\n",
    "\n",
    "        if not token_text_clean:\n",
    "            continue\n",
    "        if not WORD_PATTERN.fullmatch(token_text_clean):\n",
    "            continue\n",
    "\n",
    "        base_word = token_text_clean\n",
    "\n",
    "        # Keep the first token we see for this base_word\n",
    "        if base_word not in word_to_token_id:\n",
    "            word_to_token_id[base_word] = token_id\n",
    "            token_id_to_word[token_id] = base_word\n",
    "            candidate_token_ids.append(token_id)\n",
    "\n",
    "    candidate_token_ids_array = np.array(candidate_token_ids, dtype=np.int32)\n",
    "    print(f\"Number of word-like tokens: {len(candidate_token_ids_array)}\")\n",
    "\n",
    "    return word_to_token_id, token_id_to_word, candidate_token_ids_array\n",
    "\n",
    "\n",
    "def rank_token_in_candidates(\n",
    "    logits: np.ndarray,\n",
    "    candidate_token_ids: np.ndarray,\n",
    "    target_token_id: int,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Rank of target_token_id among candidate_token_ids, sorted by logit descending.\n",
    "    Returns 1-based rank.\n",
    "    \"\"\"\n",
    "    candidate_logits = logits[candidate_token_ids]\n",
    "    sorted_indices = np.argsort(candidate_logits)[::-1]\n",
    "    sorted_candidate_ids = candidate_token_ids[sorted_indices]\n",
    "\n",
    "    matches = np.where(sorted_candidate_ids == target_token_id)[0]\n",
    "    if matches.size == 0:\n",
    "        raise ValueError(f\"Target token id {target_token_id} not in candidate set.\")\n",
    "    return int(matches[0]) + 1  # 1-based\n",
    "\n",
    "\n",
    "def get_ranks_for_words(\n",
    "    words: List[str],\n",
    "    model: Llama,\n",
    "    word_to_token_id: Dict[str, int],\n",
    "    candidate_token_ids: np.ndarray,\n",
    "    prompt_text: str = \"\",\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Given a sequence of words, compute the rank of each word's token\n",
    "    under the model, step by step, *after* feeding an optional prompt_text.\n",
    "\n",
    "    - prompt_text is the 'k'' or 'k' string used as context.\n",
    "    - words must all be in word_to_token_id (single-token words).\n",
    "    \"\"\"\n",
    "    model.reset()\n",
    "\n",
    "    if prompt_text:\n",
    "        # Tokenize prompt with BOS\n",
    "        prompt_ids = encode_text(prompt_text, add_bos=True)\n",
    "    else:\n",
    "        # Just BOS if there is no prompt\n",
    "        prompt_ids = [token_bos_id()]\n",
    "\n",
    "    model.eval(prompt_ids)\n",
    "\n",
    "    ranks: List[int] = []\n",
    "\n",
    "    for word in words:\n",
    "        if word not in word_to_token_id:\n",
    "            raise KeyError(f\"No token found for word '{word}' in word_to_token_id map.\")\n",
    "\n",
    "        token_id = word_to_token_id[word]\n",
    "\n",
    "        # Logits for next token given current context\n",
    "        logits = model.scores[model.n_tokens - 1]\n",
    "        rank = rank_token_in_candidates(logits, candidate_token_ids, token_id)\n",
    "        ranks.append(rank)\n",
    "\n",
    "        # Update context with the actual token\n",
    "        model.eval([token_id])\n",
    "\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def decode_words_from_ranks(\n",
    "    ranks: List[int],\n",
    "    model: Llama,\n",
    "    word_to_token_id: Dict[str, int],\n",
    "    token_id_to_word: Dict[int, str],\n",
    "    candidate_token_ids: np.ndarray,\n",
    "    prompt_text: str = \"\",\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Inverse of get_ranks_for_words: given ranks and an optional prompt_text,\n",
    "    generate the sequence of words that produce those ranks.\n",
    "\n",
    "    - prompt_text is the context (either k or k').\n",
    "    - Only words with single-token entries in candidate_token_ids are used.\n",
    "    \"\"\"\n",
    "    model.reset()\n",
    "\n",
    "    if prompt_text:\n",
    "        prompt_ids = encode_text(prompt_text, add_bos=True)\n",
    "    else:\n",
    "        prompt_ids = [token_bos_id()]\n",
    "\n",
    "    model.eval(prompt_ids)\n",
    "\n",
    "    generated_words: List[str] = []\n",
    "\n",
    "    for desired_rank in ranks:\n",
    "        logits = model.scores[model.n_tokens - 1]\n",
    "\n",
    "        candidate_logits = logits[candidate_token_ids]\n",
    "        sorted_indices = np.argsort(candidate_logits)[::-1]\n",
    "        sorted_candidate_ids = candidate_token_ids[sorted_indices]\n",
    "\n",
    "        index_in_sorted = desired_rank - 1\n",
    "        if index_in_sorted >= len(sorted_candidate_ids):\n",
    "            raise ValueError(\n",
    "                f\"Rank {desired_rank} is out of range \"\n",
    "                f\"(only {len(sorted_candidate_ids)} candidate tokens).\"\n",
    "            )\n",
    "\n",
    "        next_token_id = int(sorted_candidate_ids[index_in_sorted])\n",
    "        word = token_id_to_word[next_token_id]\n",
    "        generated_words.append(word)\n",
    "\n",
    "        # Feed the chosen token to advance the context\n",
    "        model.eval([next_token_id])\n",
    "\n",
    "    return generated_words\n",
    "\n",
    "\n",
    "# Build the word-level vocabulary once\n",
    "word_to_token_id, token_id_to_word, candidate_token_ids = build_word_token_maps(llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_words: ['the', 'current', 'system', 'has', 'repeatedly', 'failed']\n",
      "k_words: ['here', 'it', 'is', 'the', 'infamous', 'british', 'roasted', 'board', 'with', 'mint', 'sauce', 'how', 'to', 'make', 'it', 'perfect']\n",
      "\n",
      "Ranks for e under k':\n",
      "[3361, 97, 11, 6, 261, 1]\n",
      "\n",
      "Stegotext s from e using k:\n",
      "aven nero ve and happy cook\n",
      "\n",
      "Recovered ranks from s and k:\n",
      "[3361, 97, 11, 6, 261, 1]\n",
      "\n",
      "Recovered e under k':\n",
      "the current system has repeatedly failed\n"
     ]
    }
   ],
   "source": [
    "# Your original and key texts\n",
    "e = \"THE CURRENT SYSTEM HAS REPEATEDLY FAILED\"\n",
    "k = \"Here it is: the infamous British roasted board with mint sauce. How to make it perfect.\"\n",
    "\n",
    "# Optional secret prefix k' (can be empty or something like \"A text:\")\n",
    "k_prime = \"\"  # try \"A text:\" if you want to match the notebook's style\n",
    "\n",
    "# Word-level conversion (simple, no punctuation)\n",
    "e_words = simple_word_tokenize(e)\n",
    "k_words = simple_word_tokenize(k)\n",
    "\n",
    "print(\"e_words:\", e_words)\n",
    "print(\"k_words:\", k_words)\n",
    "\n",
    "# 1. ENCODE SIDE\n",
    "# 1a. Ranks of e under prefix k'\n",
    "ranks_e = get_ranks_for_words(\n",
    "    words=e_words,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k_prime,  # k'\n",
    ")\n",
    "print(\"\\nRanks for e under k':\")\n",
    "print(ranks_e)\n",
    "\n",
    "# 1b. Stegotext s from ranks_e under key k\n",
    "stego_words = decode_words_from_ranks(\n",
    "    ranks=ranks_e,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    token_id_to_word=token_id_to_word,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k,  # k\n",
    ")\n",
    "stego_text = \" \".join(stego_words)\n",
    "print(\"\\nStegotext s from e using k:\")\n",
    "print(stego_text)\n",
    "\n",
    "# 2. DECODE SIDE\n",
    "# 2a. Recover ranks from s under key k (same as encoding side)\n",
    "decoded_ranks = get_ranks_for_words(\n",
    "    words=stego_words,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k,  # k\n",
    ")\n",
    "print(\"\\nRecovered ranks from s and k:\")\n",
    "print(decoded_ranks)\n",
    "\n",
    "# 2b. Recover e under k'\n",
    "recovered_e_words = decode_words_from_ranks(\n",
    "    ranks=decoded_ranks,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    token_id_to_word=token_id_to_word,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k_prime,  # k'\n",
    ")\n",
    "recovered_e_text = \" \".join(recovered_e_words)\n",
    "print(\"\\nRecovered e under k':\")\n",
    "print(recovered_e_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Original secret text e:\n",
      "THE CURRENT SYSTEM HAS REPEATEDLY FAILED\n",
      "\n",
      "2) Secret key k (used to generate stegotext):\n",
      "Here it is: the infamous British roasted board with mint sauce. How to make it perfect.\n",
      "\n",
      "3) Optional prefix k' (used only for hiding/revealing e):\n",
      "''\n",
      "\n",
      "4) Word-level tokens of e:\n",
      "['the', 'current', 'system', 'has', 'repeatedly', 'failed']\n",
      "\n",
      "5) Word-level tokens of k:\n",
      "['here', 'it', 'is', 'the', 'infamous', 'british', 'roasted', 'board', 'with', 'mint', 'sauce', 'how', 'to', 'make', 'it', 'perfect']\n",
      "\n",
      "6) Ranks of e under k':\n",
      "[3361, 97, 11, 6, 261, 1]\n",
      "\n",
      "7) Stegotext s (this is what you would send):\n",
      "aven nero ve and happy cook\n",
      "\n",
      "8) Ranks recovered from s and k:\n",
      "[3361, 97, 11, 6, 261, 1]\n",
      "\n",
      "9) Recovered secret text e (lowercased, word-tokenized):\n",
      "the current system has repeatedly failed\n",
      "\n",
      "10) Do recovered words match e_words?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Demo: full encode/decode pipeline for e and k using ranks\n",
    "\n",
    "# 1. Define secret text e, key k, and optional prefix k'\n",
    "e = \"THE CURRENT SYSTEM HAS REPEATEDLY FAILED\"\n",
    "k = \"Here it is: the infamous British roasted board with mint sauce. How to make it perfect.\"\n",
    "k_prime = \"\"  # you can also try \"A text:\" here\n",
    "\n",
    "print(\"1) Original secret text e:\")\n",
    "print(e)\n",
    "print()\n",
    "\n",
    "print(\"2) Secret key k (used to generate stegotext):\")\n",
    "print(k)\n",
    "print()\n",
    "\n",
    "print(\"3) Optional prefix k' (used only for hiding/revealing e):\")\n",
    "print(repr(k_prime))\n",
    "print()\n",
    "\n",
    "# 2. Tokenize to simple word sequences\n",
    "e_words = simple_word_tokenize(e)\n",
    "k_words = simple_word_tokenize(k)\n",
    "\n",
    "print(\"4) Word-level tokens of e:\")\n",
    "print(e_words)\n",
    "print()\n",
    "\n",
    "print(\"5) Word-level tokens of k:\")\n",
    "print(k_words)\n",
    "print()\n",
    "\n",
    "# 3. ENCODE SIDE:\n",
    "# 3a. Compute ranks for e under prefix k' (this is the hidden payload as ranks)\n",
    "ranks_e = get_ranks_for_words(\n",
    "    words=e_words,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k_prime,  # k'\n",
    ")\n",
    "\n",
    "print(\"6) Ranks of e under k':\")\n",
    "print(ranks_e)\n",
    "print()\n",
    "\n",
    "# 3b. Generate stegotext s using k plus those ranks\n",
    "stego_words = decode_words_from_ranks(\n",
    "    ranks=ranks_e,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    token_id_to_word=token_id_to_word,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k,  # k\n",
    ")\n",
    "stego_text = \" \".join(stego_words)\n",
    "\n",
    "print(\"7) Stegotext s (this is what you would send):\")\n",
    "print(stego_text)\n",
    "print()\n",
    "\n",
    "# 4. DECODE SIDE (receiver knows k and k'):\n",
    "# 4a. Recompute ranks from s under k\n",
    "decoded_ranks = get_ranks_for_words(\n",
    "    words=stego_words,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k,  # k\n",
    ")\n",
    "\n",
    "print(\"8) Ranks recovered from s and k:\")\n",
    "print(decoded_ranks)\n",
    "print()\n",
    "\n",
    "# 4b. Reconstruct e from those ranks under k'\n",
    "recovered_e_words = decode_words_from_ranks(\n",
    "    ranks=decoded_ranks,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    token_id_to_word=token_id_to_word,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k_prime,  # k'\n",
    ")\n",
    "recovered_e_text = \" \".join(recovered_e_words)\n",
    "\n",
    "print(\"9) Recovered secret text e (lowercased, word-tokenized):\")\n",
    "print(recovered_e_text)\n",
    "print()\n",
    "\n",
    "# 5. Sanity check\n",
    "print(\"10) Do recovered words match e_words?\")\n",
    "print(recovered_e_words == e_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "def encode_secret(\n",
    "    e_text: str,\n",
    "    k_text: str,\n",
    "    k_prime_text: str = \"\",\n",
    ") -> Tuple[str, List[int], List[str]]:\n",
    "    \"\"\"\n",
    "    Encode a secret text e into a stegotext s using key k and optional prefix k'.\n",
    "\n",
    "    Returns:\n",
    "      stego_text (s),\n",
    "      ranks_for_e,\n",
    "      e_words (tokenized version of e)\n",
    "    \"\"\"\n",
    "    # Word-level tokens\n",
    "    e_words = simple_word_tokenize(e_text)\n",
    "\n",
    "    # 1. Ranks of e under k'\n",
    "    ranks_e = get_ranks_for_words(\n",
    "        words=e_words,\n",
    "        model=llm,\n",
    "        word_to_token_id=word_to_token_id,\n",
    "        candidate_token_ids=candidate_token_ids,\n",
    "        prompt_text=k_prime_text,  # k'\n",
    "    )\n",
    "\n",
    "    # 2. Stegotext words s under k\n",
    "    stego_words = decode_words_from_ranks(\n",
    "        ranks=ranks_e,\n",
    "        model=llm,\n",
    "        word_to_token_id=word_to_token_id,\n",
    "        token_id_to_word=token_id_to_word,\n",
    "        candidate_token_ids=candidate_token_ids,\n",
    "        prompt_text=k_text,  # k\n",
    "    )\n",
    "    stego_text = \" \".join(stego_words)\n",
    "\n",
    "    return stego_text, ranks_e, e_words\n",
    "\n",
    "\n",
    "def decode_secret(\n",
    "    stego_text: str,\n",
    "    k_text: str,\n",
    "    k_prime_text: str = \"\",\n",
    ") -> Tuple[str, List[int], List[str]]:\n",
    "    \"\"\"\n",
    "    Decode a stegotext s back to a secret text using key k and optional prefix k'.\n",
    "\n",
    "    Returns:\n",
    "      recovered_secret_text,\n",
    "      recovered_ranks,\n",
    "      stego_words\n",
    "    \"\"\"\n",
    "    stego_words = simple_word_tokenize(stego_text)\n",
    "\n",
    "    # 1. Recover ranks from s under k\n",
    "    recovered_ranks = get_ranks_for_words(\n",
    "        words=stego_words,\n",
    "        model=llm,\n",
    "        word_to_token_id=word_to_token_id,\n",
    "        candidate_token_ids=candidate_token_ids,\n",
    "        prompt_text=k_text,  # k\n",
    "    )\n",
    "\n",
    "    # 2. Recover e under k'\n",
    "    recovered_e_words = decode_words_from_ranks(\n",
    "        ranks=recovered_ranks,\n",
    "        model=llm,\n",
    "        word_to_token_id=word_to_token_id,\n",
    "        token_id_to_word=token_id_to_word,\n",
    "        candidate_token_ids=candidate_token_ids,\n",
    "        prompt_text=k_prime_text,  # k'\n",
    "    )\n",
    "    recovered_e_text = \" \".join(recovered_e_words)\n",
    "\n",
    "    return recovered_e_text, recovered_ranks, stego_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original secret text e:\n",
      "THE CURRENT SYSTEM HAS REPEATEDLY FAILED\n",
      "\n",
      "Secret key k:\n",
      "Here it is: the infamous British roasted board with mint sauce. How to make it perfect.\n",
      "\n",
      "Optional prefix k': ''\n",
      "\n",
      "Word-level e_words: ['the', 'current', 'system', 'has', 'repeatedly', 'failed']\n",
      "Ranks for e under k': [3361, 97, 11, 6, 261, 1]\n",
      "\n",
      "Stegotext s (to send):\n",
      "aven nero ve and happy cook\n",
      "\n",
      "Stegotext words: ['aven', 'nero', 've', 'and', 'happy', 'cook']\n",
      "Recovered ranks from s and k: [3361, 97, 11, 6, 261, 1]\n",
      "\n",
      "Recovered secret text e:\n",
      "the current system has repeatedly failed\n",
      "\n",
      "Do recovered ranks match original ranks? True\n",
      "Do recovered words match original e_words? True\n"
     ]
    }
   ],
   "source": [
    "# Define secret text e, key k, and optional prefix k'\n",
    "e = \"THE CURRENT SYSTEM HAS REPEATEDLY FAILED\"\n",
    "k = \"Here it is: the infamous British roasted board with mint sauce. How to make it perfect.\"\n",
    "k_prime = \"\"  # try \"A text:\" if you want\n",
    "\n",
    "print(\"Original secret text e:\")\n",
    "print(e)\n",
    "print()\n",
    "\n",
    "print(\"Secret key k:\")\n",
    "print(k)\n",
    "print()\n",
    "\n",
    "print(\"Optional prefix k':\", repr(k_prime))\n",
    "print()\n",
    "\n",
    "# ENCODE\n",
    "stego_text, ranks_e, e_words = encode_secret(e_text=e, k_text=k, k_prime_text=k_prime)\n",
    "\n",
    "print(\"Word-level e_words:\", e_words)\n",
    "print(\"Ranks for e under k':\", ranks_e)\n",
    "print()\n",
    "print(\"Stegotext s (to send):\")\n",
    "print(stego_text)\n",
    "print()\n",
    "\n",
    "# DECODE\n",
    "recovered_e_text, recovered_ranks, stego_words = decode_secret(\n",
    "    stego_text=stego_text,\n",
    "    k_text=k,\n",
    "    k_prime_text=k_prime,\n",
    ")\n",
    "\n",
    "print(\"Stegotext words:\", stego_words)\n",
    "print(\"Recovered ranks from s and k:\", recovered_ranks)\n",
    "print()\n",
    "print(\"Recovered secret text e:\")\n",
    "print(recovered_e_text)\n",
    "print()\n",
    "\n",
    "print(\"Do recovered ranks match original ranks?\", recovered_ranks == ranks_e)\n",
    "print(\"Do recovered words match original e_words?\",\n",
    "      simple_word_tokenize(recovered_e_text) == e_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
