{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: /home/meow/Documents/repos/LlmStenoExplore/models/phi3/Phi-3-mini-4k-instruct-q4.gguf exists: True\n",
      "BOS token id: 1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Sequence\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "#   notebook is in LlmStenoExplore/notebooks\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "MODEL_PATH = REPO_ROOT / \"models/phi3/Phi-3-mini-4k-instruct-q4.gguf\"\n",
    "print(\"Using model:\", MODEL_PATH, \"exists:\", MODEL_PATH.exists())\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=str(MODEL_PATH),\n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=0,      # CPU-only\n",
    "    logits_all=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "def token_bos_id() -> int:\n",
    "    return llm.token_bos()\n",
    "\n",
    "def encode_text(text: str, add_bos: bool = True) -> List[int]:\n",
    "    return llm.tokenize(text.encode(\"utf-8\"), add_bos=add_bos, special=False)\n",
    "\n",
    "def decode_tokens(tokens: List[int], prev_tokens: List[int] | None = None) -> str:\n",
    "    b = llm.detokenize(tokens, prev_tokens=prev_tokens, special=False)\n",
    "    return b.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "print(\"BOS token id:\", token_bos_id())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = \"THE CURRENT SYSTEM HAS REPEATEDLY FAILED\"\n",
    "k = \"Here it is: the infamous British roasted boar with mint sauce. How to make it perfect.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32064\n",
      "Number of word-like tokens: 15562\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Simple word tokenizer: lowercase, only a-z, no punctuation, no numbers\n",
    "WORD_PATTERN = re.compile(r\"[a-z]+\")\n",
    "\n",
    "def simple_word_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert a string to a list of simple word tokens.\n",
    "    - Lowercase\n",
    "    - Drop punctuation and numbers\n",
    "    - Only keep [a-z]+ spans\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    return WORD_PATTERN.findall(text_lower)\n",
    "\n",
    "\n",
    "def build_word_token_maps(model: Llama) -> Tuple[Dict[str, int], Dict[int, str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build a mapping between plain words and model token ids.\n",
    "\n",
    "    We only keep tokens that detokenize to something that, after stripping\n",
    "    whitespace and lowercasing, matches [a-z]+ exactly.\n",
    "    \"\"\"\n",
    "    word_to_token_id: Dict[str, int] = {}\n",
    "    token_id_to_word: Dict[int, str] = {}\n",
    "    candidate_token_ids: List[int] = []\n",
    "\n",
    "    vocabulary_size = model.n_vocab()\n",
    "    print(f\"Vocabulary size: {vocabulary_size}\")\n",
    "\n",
    "    for token_id in range(vocabulary_size):\n",
    "        token_text = decode_tokens([token_id])          # model -> text\n",
    "        token_text_clean = token_text.strip().lower()   # remove leading/trailing spaces\n",
    "\n",
    "        if not token_text_clean:\n",
    "            continue\n",
    "        if not WORD_PATTERN.fullmatch(token_text_clean):\n",
    "            continue\n",
    "\n",
    "        base_word = token_text_clean\n",
    "\n",
    "        # Keep the first token we see for this base_word\n",
    "        if base_word not in word_to_token_id:\n",
    "            word_to_token_id[base_word] = token_id\n",
    "            token_id_to_word[token_id] = base_word\n",
    "            candidate_token_ids.append(token_id)\n",
    "\n",
    "    candidate_token_ids_array = np.array(candidate_token_ids, dtype=np.int32)\n",
    "    print(f\"Number of word-like tokens: {len(candidate_token_ids_array)}\")\n",
    "\n",
    "    return word_to_token_id, token_id_to_word, candidate_token_ids_array\n",
    "\n",
    "\n",
    "def rank_token_in_candidates(\n",
    "    logits: np.ndarray,\n",
    "    candidate_token_ids: np.ndarray,\n",
    "    target_token_id: int,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Rank of target_token_id among candidate_token_ids, sorted by logit descending.\n",
    "    Returns 1-based rank.\n",
    "    \"\"\"\n",
    "    candidate_logits = logits[candidate_token_ids]\n",
    "    sorted_indices = np.argsort(candidate_logits)[::-1]\n",
    "    sorted_candidate_ids = candidate_token_ids[sorted_indices]\n",
    "\n",
    "    matches = np.where(sorted_candidate_ids == target_token_id)[0]\n",
    "    if matches.size == 0:\n",
    "        raise ValueError(f\"Target token id {target_token_id} not in candidate set.\")\n",
    "    return int(matches[0]) + 1  # 1-based\n",
    "\n",
    "\n",
    "def get_ranks_for_words(\n",
    "    words: List[str],\n",
    "    model: Llama,\n",
    "    word_to_token_id: Dict[str, int],\n",
    "    candidate_token_ids: np.ndarray,\n",
    "    prompt_text: str = \"\",\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Given a sequence of words, compute the rank of each word's token\n",
    "    under the model, step by step, *after* feeding an optional prompt_text.\n",
    "\n",
    "    - prompt_text is the 'k'' or 'k' string used as context.\n",
    "    - words must all be in word_to_token_id (single-token words).\n",
    "    \"\"\"\n",
    "    model.reset()\n",
    "\n",
    "    if prompt_text:\n",
    "        # Tokenize prompt with BOS\n",
    "        prompt_ids = encode_text(prompt_text, add_bos=True)\n",
    "    else:\n",
    "        # Just BOS if there is no prompt\n",
    "        prompt_ids = [token_bos_id()]\n",
    "\n",
    "    model.eval(prompt_ids)\n",
    "\n",
    "    ranks: List[int] = []\n",
    "\n",
    "    for word in words:\n",
    "        if word not in word_to_token_id:\n",
    "            raise KeyError(f\"No token found for word '{word}' in word_to_token_id map.\")\n",
    "\n",
    "        token_id = word_to_token_id[word]\n",
    "\n",
    "        # Logits for next token given current context\n",
    "        logits = model.scores[model.n_tokens - 1]\n",
    "        rank = rank_token_in_candidates(logits, candidate_token_ids, token_id)\n",
    "        ranks.append(rank)\n",
    "\n",
    "        # Update context with the actual token\n",
    "        model.eval([token_id])\n",
    "\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def decode_words_from_ranks(\n",
    "    ranks: List[int],\n",
    "    model: Llama,\n",
    "    word_to_token_id: Dict[str, int],\n",
    "    token_id_to_word: Dict[int, str],\n",
    "    candidate_token_ids: np.ndarray,\n",
    "    prompt_text: str = \"\",\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Inverse of get_ranks_for_words: given ranks and an optional prompt_text,\n",
    "    generate the sequence of words that produce those ranks.\n",
    "\n",
    "    - prompt_text is the context (either k or k').\n",
    "    - Only words with single-token entries in candidate_token_ids are used.\n",
    "    \"\"\"\n",
    "    model.reset()\n",
    "\n",
    "    if prompt_text:\n",
    "        prompt_ids = encode_text(prompt_text, add_bos=True)\n",
    "    else:\n",
    "        prompt_ids = [token_bos_id()]\n",
    "\n",
    "    model.eval(prompt_ids)\n",
    "\n",
    "    generated_words: List[str] = []\n",
    "\n",
    "    for desired_rank in ranks:\n",
    "        logits = model.scores[model.n_tokens - 1]\n",
    "\n",
    "        candidate_logits = logits[candidate_token_ids]\n",
    "        sorted_indices = np.argsort(candidate_logits)[::-1]\n",
    "        sorted_candidate_ids = candidate_token_ids[sorted_indices]\n",
    "\n",
    "        index_in_sorted = desired_rank - 1\n",
    "        if index_in_sorted >= len(sorted_candidate_ids):\n",
    "            raise ValueError(\n",
    "                f\"Rank {desired_rank} is out of range \"\n",
    "                f\"(only {len(sorted_candidate_ids)} candidate tokens).\"\n",
    "            )\n",
    "\n",
    "        next_token_id = int(sorted_candidate_ids[index_in_sorted])\n",
    "        word = token_id_to_word[next_token_id]\n",
    "        generated_words.append(word)\n",
    "\n",
    "        # Feed the chosen token to advance the context\n",
    "        model.eval([next_token_id])\n",
    "\n",
    "    return generated_words\n",
    "\n",
    "\n",
    "# Build the word-level vocabulary once\n",
    "word_to_token_id, token_id_to_word, candidate_token_ids = build_word_token_maps(llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = \"THE SYSTEM HAS REPEATEDLY FAILED\"\n",
    "k = \"hi hi hi hi hi\" #\"Here it is: the infamous British roasted boar with mint sauce. How to make it perfect.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_words: ['the', 'system', 'has', 'repeatedly', 'failed']\n",
      "k_words: ['hi', 'hi', 'hi', 'hi', 'hi']\n",
      "\n",
      "Ranks for e under k':\n",
      "[3361, 11, 5, 1142, 1]\n",
      "\n",
      "Stegotext s from e using k:\n",
      "rust les the general store\n",
      "\n",
      "Recovered ranks from s and k:\n",
      "[3361, 11, 5, 1142, 1]\n",
      "\n",
      "Recovered e under k':\n",
      "the system has repeatedly failed\n"
     ]
    }
   ],
   "source": [
    "# Your original and key texts\n",
    "# e = \"THE CURRENT SYSTEM HAS REPEATEDLY FAILED\"\n",
    "# k = \"Here it is: the infamous British roasted board with mint sauce. How to make it perfect.\"\n",
    "\n",
    "# Optional secret prefix k' (can be empty or something like \"A text:\")\n",
    "k_prime = \"\"  # try \"A text:\" if you want to match the notebook's style\n",
    "\n",
    "# Word-level conversion (simple, no punctuation)\n",
    "e_words = simple_word_tokenize(e)\n",
    "k_words = simple_word_tokenize(k)\n",
    "\n",
    "print(\"e_words:\", e_words)\n",
    "print(\"k_words:\", k_words)\n",
    "\n",
    "# 1. ENCODE SIDE\n",
    "# 1a. Ranks of e under prefix k'\n",
    "ranks_e = get_ranks_for_words(\n",
    "    words=e_words,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k_prime,  # k'\n",
    ")\n",
    "print(\"\\nRanks for e under k':\")\n",
    "print(ranks_e)\n",
    "\n",
    "# 1b. Stegotext s from ranks_e under key k\n",
    "stego_words = decode_words_from_ranks(\n",
    "    ranks=ranks_e,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    token_id_to_word=token_id_to_word,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k,  # k\n",
    ")\n",
    "stego_text = \" \".join(stego_words)\n",
    "print(\"\\nStegotext s from e using k:\")\n",
    "print(stego_text)\n",
    "\n",
    "# 2. DECODE SIDE\n",
    "# 2a. Recover ranks from s under key k (same as encoding side)\n",
    "decoded_ranks = get_ranks_for_words(\n",
    "    words=stego_words,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k,  # k\n",
    ")\n",
    "print(\"\\nRecovered ranks from s and k:\")\n",
    "print(decoded_ranks)\n",
    "\n",
    "# 2b. Recover e under k'\n",
    "recovered_e_words = decode_words_from_ranks(\n",
    "    ranks=decoded_ranks,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    token_id_to_word=token_id_to_word,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k_prime,  # k'\n",
    ")\n",
    "recovered_e_text = \" \".join(recovered_e_words)\n",
    "print(\"\\nRecovered e under k':\")\n",
    "print(recovered_e_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Original secret text e:\n",
      "OUR CURRENT SYSTEM HAS REPEATEDLY FAILED TO PREPARE THE EMPLOYEES TODAY\n",
      "\n",
      "2) Secret key k (used to generate stegotext):\n",
      "system operators are currently hard at work\n",
      "\n",
      "3) Optional prefix k' (used only for hiding/revealing e):\n",
      "''\n",
      "\n",
      "4) Word-level tokens of e:\n",
      "['our', 'current', 'system', 'has', 'repeatedly', 'failed', 'to', 'prepare', 'the', 'employees', 'today']\n",
      "\n",
      "5) Word-level tokens of k:\n",
      "['system', 'operators', 'are', 'currently', 'hard', 'at', 'work']\n",
      "\n",
      "6) Ranks of e under k':\n",
      "[14564, 2125, 11, 6, 356, 1, 1, 125, 1, 199, 247]\n",
      "\n",
      "7) Stegotext s (this is what you would send):\n",
      "izations rapidly deploy and executing their plans collabor atively simultaneously appears\n",
      "\n",
      "8) Ranks recovered from s and k:\n",
      "[14564, 2125, 11, 6, 356, 1, 1, 125, 1, 199, 247]\n",
      "\n",
      "9) Recovered secret text e (lowercased, word-tokenized):\n",
      "our current system has repeatedly failed to prepare the employees today\n",
      "\n",
      "10) Do recovered words match e_words?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Demo: full encode/decode pipeline for e and k using ranks\n",
    "\n",
    "# 1. Define secret text e, key k, and optional prefix k'\n",
    "#e = \"THE CURRENT SYSTEM HAS REPEATEDLY FAILED\"\n",
    "#k = \"Here it is: the infamous British roasted board with mint sauce. How to make it perfect.\"\n",
    "k_prime = \"\"  # you can also try \"A text:\" here\n",
    "\n",
    "print(\"1) Original secret text e:\")\n",
    "print(e)\n",
    "print()\n",
    "\n",
    "print(\"2) Secret key k (used to generate stegotext):\")\n",
    "print(k)\n",
    "print()\n",
    "\n",
    "print(\"3) Optional prefix k' (used only for hiding/revealing e):\")\n",
    "print(repr(k_prime))\n",
    "print()\n",
    "\n",
    "# 2. Tokenize to simple word sequences\n",
    "e_words = simple_word_tokenize(e)\n",
    "k_words = simple_word_tokenize(k)\n",
    "\n",
    "print(\"4) Word-level tokens of e:\")\n",
    "print(e_words)\n",
    "print()\n",
    "\n",
    "print(\"5) Word-level tokens of k:\")\n",
    "print(k_words)\n",
    "print()\n",
    "\n",
    "# 3. ENCODE SIDE:\n",
    "# 3a. Compute ranks for e under prefix k' (this is the hidden payload as ranks)\n",
    "ranks_e = get_ranks_for_words(\n",
    "    words=e_words,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k_prime,  # k'\n",
    ")\n",
    "\n",
    "print(\"6) Ranks of e under k':\")\n",
    "print(ranks_e)\n",
    "print()\n",
    "\n",
    "# 3b. Generate stegotext s using k plus those ranks\n",
    "stego_words = decode_words_from_ranks(\n",
    "    ranks=ranks_e,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    token_id_to_word=token_id_to_word,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k,  # k\n",
    ")\n",
    "stego_text = \" \".join(stego_words)\n",
    "\n",
    "print(\"7) Stegotext s (this is what you would send):\")\n",
    "print(stego_text)\n",
    "print()\n",
    "\n",
    "# 4. DECODE SIDE (receiver knows k and k'):\n",
    "# 4a. Recompute ranks from s under k\n",
    "decoded_ranks = get_ranks_for_words(\n",
    "    words=stego_words,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k,  # k\n",
    ")\n",
    "\n",
    "print(\"8) Ranks recovered from s and k:\")\n",
    "print(decoded_ranks)\n",
    "print()\n",
    "\n",
    "# 4b. Reconstruct e from those ranks under k'\n",
    "recovered_e_words = decode_words_from_ranks(\n",
    "    ranks=decoded_ranks,\n",
    "    model=llm,\n",
    "    word_to_token_id=word_to_token_id,\n",
    "    token_id_to_word=token_id_to_word,\n",
    "    candidate_token_ids=candidate_token_ids,\n",
    "    prompt_text=k_prime,  # k'\n",
    ")\n",
    "recovered_e_text = \" \".join(recovered_e_words)\n",
    "\n",
    "print(\"9) Recovered secret text e (lowercased, word-tokenized):\")\n",
    "print(recovered_e_text)\n",
    "print()\n",
    "\n",
    "# 5. Sanity check\n",
    "print(\"10) Do recovered words match e_words?\")\n",
    "print(recovered_e_words == e_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "def encode_secret(\n",
    "    e_text: str,\n",
    "    k_text: str,\n",
    "    k_prime_text: str = \"\",\n",
    ") -> Tuple[str, List[int], List[str]]:\n",
    "    \"\"\"\n",
    "    Encode a secret text e into a stegotext s using key k and optional prefix k'.\n",
    "\n",
    "    Returns:\n",
    "      stego_text (s),\n",
    "      ranks_for_e,\n",
    "      e_words (tokenized version of e)\n",
    "    \"\"\"\n",
    "    # Word-level tokens\n",
    "    e_words = simple_word_tokenize(e_text)\n",
    "\n",
    "    # 1. Ranks of e under k'\n",
    "    ranks_e = get_ranks_for_words(\n",
    "        words=e_words,\n",
    "        model=llm,\n",
    "        word_to_token_id=word_to_token_id,\n",
    "        candidate_token_ids=candidate_token_ids,\n",
    "        prompt_text=k_prime_text,  # k'\n",
    "    )\n",
    "\n",
    "    # 2. Stegotext words s under k\n",
    "    stego_words = decode_words_from_ranks(\n",
    "        ranks=ranks_e,\n",
    "        model=llm,\n",
    "        word_to_token_id=word_to_token_id,\n",
    "        token_id_to_word=token_id_to_word,\n",
    "        candidate_token_ids=candidate_token_ids,\n",
    "        prompt_text=k_text,  # k\n",
    "    )\n",
    "    stego_text = \" \".join(stego_words)\n",
    "\n",
    "    return stego_text, ranks_e, e_words\n",
    "\n",
    "\n",
    "def decode_secret(\n",
    "    stego_text: str,\n",
    "    k_text: str,\n",
    "    k_prime_text: str = \"\",\n",
    ") -> Tuple[str, List[int], List[str]]:\n",
    "    \"\"\"\n",
    "    Decode a stegotext s back to a secret text using key k and optional prefix k'.\n",
    "\n",
    "    Returns:\n",
    "      recovered_secret_text,\n",
    "      recovered_ranks,\n",
    "      stego_words\n",
    "    \"\"\"\n",
    "    stego_words = simple_word_tokenize(stego_text)\n",
    "\n",
    "    # 1. Recover ranks from s under k\n",
    "    recovered_ranks = get_ranks_for_words(\n",
    "        words=stego_words,\n",
    "        model=llm,\n",
    "        word_to_token_id=word_to_token_id,\n",
    "        candidate_token_ids=candidate_token_ids,\n",
    "        prompt_text=k_text,  # k\n",
    "    )\n",
    "\n",
    "    # 2. Recover e under k'\n",
    "    recovered_e_words = decode_words_from_ranks(\n",
    "        ranks=recovered_ranks,\n",
    "        model=llm,\n",
    "        word_to_token_id=word_to_token_id,\n",
    "        token_id_to_word=token_id_to_word,\n",
    "        candidate_token_ids=candidate_token_ids,\n",
    "        prompt_text=k_prime_text,  # k'\n",
    "    )\n",
    "    recovered_e_text = \" \".join(recovered_e_words)\n",
    "\n",
    "    return recovered_e_text, recovered_ranks, stego_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original secret text e:\n",
      "THE CURRENT SYSTEM HAS REPEATEDLY FAILED\n",
      "\n",
      "Secret key k:\n",
      "Here it is: the infamous British roasted board with mint sauce. How to make it perfect.\n",
      "\n",
      "Optional prefix k': ''\n",
      "\n",
      "Word-level e_words: ['the', 'current', 'system', 'has', 'repeatedly', 'failed']\n",
      "Ranks for e under k': [3361, 97, 11, 6, 261, 1]\n",
      "\n",
      "Stegotext s (to send):\n",
      "aven nero ve and happy cook\n",
      "\n",
      "Stegotext words: ['aven', 'nero', 've', 'and', 'happy', 'cook']\n",
      "Recovered ranks from s and k: [3361, 97, 11, 6, 261, 1]\n",
      "\n",
      "Recovered secret text e:\n",
      "the current system has repeatedly failed\n",
      "\n",
      "Do recovered ranks match original ranks? True\n",
      "Do recovered words match original e_words? True\n"
     ]
    }
   ],
   "source": [
    "# Define secret text e, key k, and optional prefix k'\n",
    "e = \"THE CURRENT SYSTEM HAS REPEATEDLY FAILED\"\n",
    "k = \"Here it is: the infamous British roasted board with mint sauce. How to make it perfect.\"\n",
    "k_prime = \"\"  # try \"A text:\" if you want\n",
    "\n",
    "print(\"Original secret text e:\")\n",
    "print(e)\n",
    "print()\n",
    "\n",
    "print(\"Secret key k:\")\n",
    "print(k)\n",
    "print()\n",
    "\n",
    "print(\"Optional prefix k':\", repr(k_prime))\n",
    "print()\n",
    "\n",
    "# ENCODE\n",
    "stego_text, ranks_e, e_words = encode_secret(e_text=e, k_text=k, k_prime_text=k_prime)\n",
    "\n",
    "print(\"Word-level e_words:\", e_words)\n",
    "print(\"Ranks for e under k':\", ranks_e)\n",
    "print()\n",
    "print(\"Stegotext s (to send):\")\n",
    "print(stego_text)\n",
    "print()\n",
    "\n",
    "# DECODE\n",
    "recovered_e_text, recovered_ranks, stego_words = decode_secret(\n",
    "    stego_text=stego_text,\n",
    "    k_text=k,\n",
    "    k_prime_text=k_prime,\n",
    ")\n",
    "\n",
    "print(\"Stegotext words:\", stego_words)\n",
    "print(\"Recovered ranks from s and k:\", recovered_ranks)\n",
    "print()\n",
    "print(\"Recovered secret text e:\")\n",
    "print(recovered_e_text)\n",
    "print()\n",
    "\n",
    "print(\"Do recovered ranks match original ranks?\", recovered_ranks == ranks_e)\n",
    "print(\"Do recovered words match original e_words?\",\n",
    "      simple_word_tokenize(recovered_e_text) == e_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# difference between 'e' and 'stego_text' measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical similarity e vs stego: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lexical_similarity(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Super simple similarity: Jaccard overlap of word sets.\n",
    "    Returns a number in [0, 1].\n",
    "    \"\"\"\n",
    "    a_words = set(simple_word_tokenize(a))\n",
    "    b_words = set(simple_word_tokenize(b))\n",
    "    if not a_words and not b_words:\n",
    "        return 1.0\n",
    "    intersection = len(a_words & b_words)\n",
    "    union = len(a_words | b_words)\n",
    "    return intersection / union\n",
    "\n",
    "# Example\n",
    "#e = \"THE CURRENT SYSTEM HAS REPEATEDLY FAILED\"\n",
    "#stego_text = \"aven nero ve and happy cook\"\n",
    "\n",
    "print(\"Lexical similarity e vs stego:\", lexical_similarity(e, stego_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse similarity score from: ''\n",
      "LLM similarity e vs stego: nan\n"
     ]
    }
   ],
   "source": [
    "#levereage llm\n",
    "def llm_similarity_score(text_a: str, text_b: str) -> float:\n",
    "    \"\"\"\n",
    "    Ask the LLM: on a scale from 0 to 1, how similar are these texts in meaning?\n",
    "    Uses your existing `llm` (Phi-3 via llama_cpp).\n",
    "\n",
    "    Returns a float in [0, 1], or np.nan if parsing fails.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a strict similarity rater.\n",
    "\n",
    "On a scale from 0 to 1:\n",
    "\n",
    "- 0 means \"completely unrelated in meaning\".\n",
    "- 1 means \"identical in meaning\".\n",
    "- Values in between reflect partial similarity.\n",
    "\n",
    "Rate the semantic similarity between these two texts:\n",
    "\n",
    "TEXT A: {text_a}\n",
    "\n",
    "TEXT B: {text_b}\n",
    "\n",
    "Answer with only a single number between 0 and 1, using up to 3 decimal places, and nothing else.\n",
    "\"\"\".strip()\n",
    "\n",
    "    # Generate with llama_cpp\n",
    "    result = llm(\n",
    "        prompt,\n",
    "        max_tokens=8,\n",
    "        temperature=0.0,\n",
    "        stop=[\"\\n\"],\n",
    "    )\n",
    "    # llama_cpp returns dict with \"choices\"\n",
    "    raw = result[\"choices\"][0][\"text\"].strip()\n",
    "    try:\n",
    "        score = float(raw)\n",
    "        # clamp just in case\n",
    "        score = max(0.0, min(1.0, score))\n",
    "        return score\n",
    "    except ValueError:\n",
    "        print(\"Could not parse similarity score from:\", repr(raw))\n",
    "        return float(\"nan\")\n",
    "\n",
    "# Example\n",
    "print(\"LLM similarity e vs stego:\", llm_similarity_score(e, stego_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: OUR CURRENT SYSTEM HAS REPEATEDLY FAILED TO PREPARE THE EMPLOYEES TODAY\n",
      "stego_text: izations rapidly deploy and executing their plans collabor atively simultaneously appears\n",
      "Embedding similarity e vs stego_text: 0.5037416815757751\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. Embedding-only model instance\n",
    "# ----------------------------------------\n",
    "# Reuse the same MODEL_PATH you already used for llm\n",
    "# (adjust this variable name/path if needed).\n",
    "embed_llm = Llama(\n",
    "    model_path=str(MODEL_PATH),\n",
    "    n_ctx=0,          # context length not important for embeddings\n",
    "    n_gpu_layers=0,   # CPU-only (match your current setup)\n",
    "    embedding=True,   # IMPORTANT: enable embedding mode\n",
    "    logits_all=False,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. Cosine similarity\n",
    "# ----------------------------------------\n",
    "\n",
    "def cosine_similarity(vec_a: np.ndarray, vec_b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    a = np.asarray(vec_a, dtype=np.float32)\n",
    "    b = np.asarray(vec_b, dtype=np.float32)\n",
    "\n",
    "    a_norm = np.linalg.norm(a)\n",
    "    b_norm = np.linalg.norm(b)\n",
    "    if a_norm == 0.0 or b_norm == 0.0:\n",
    "        raise ValueError(\"One of the vectors has zero norm.\")\n",
    "\n",
    "    a = a / a_norm\n",
    "    b = b / b_norm\n",
    "    return float(np.dot(a, b))\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. get_embedding using embed_llm\n",
    "# ----------------------------------------\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return a 1D numpy array embedding for the given text using embed_llm.\n",
    "\n",
    "    llama-cpp-python's .embed() can return either:\n",
    "      - {\"data\":[{\"embedding\":[...]}]}\n",
    "      - a raw list/array (possibly 2D: n_tokens x dim)\n",
    "    \"\"\"\n",
    "    result = embed_llm.embed(text)\n",
    "\n",
    "    # Case 1: dict-like output\n",
    "    if isinstance(result, dict) and \"data\" in result:\n",
    "        emb = np.asarray(result[\"data\"][0][\"embedding\"], dtype=np.float32)\n",
    "    else:\n",
    "        # Case 2: raw list/array\n",
    "        emb = np.asarray(result, dtype=np.float32)\n",
    "\n",
    "    # If we get per-token embeddings (2D), average over tokens\n",
    "    if emb.ndim == 2:\n",
    "        emb = emb.mean(axis=0)\n",
    "\n",
    "    if emb.ndim != 1:\n",
    "        raise RuntimeError(f\"Expected 1D embedding, got shape {emb.shape}\")\n",
    "\n",
    "    return emb\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. embedding_similarity wrapper\n",
    "# ----------------------------------------\n",
    "\n",
    "def embedding_similarity(text_a: str, text_b: str) -> float:\n",
    "    \"\"\"\n",
    "    Cosine similarity between embeddings of the two texts.\n",
    "    \"\"\"\n",
    "    emb_a = get_embedding(text_a)\n",
    "    emb_b = get_embedding(text_b)\n",
    "    return cosine_similarity(emb_a, emb_b)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. Example usage with your e and stego_text\n",
    "# ----------------------------------------\n",
    "\n",
    "print(\"e:\", e)\n",
    "print(\"stego_text:\", stego_text)\n",
    "\n",
    "sim = embedding_similarity(e, stego_text)\n",
    "print(\"Embedding similarity e vs stego_text:\", sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
