{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Sequence, Dict\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "#   notebook is in LlmStenoExplore/notebooks\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "MODEL_REGISTRY = {\n",
    "    \"phi3_mini_q4\": REPO_ROOT / \"models/phi3/Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "    \"llama3_8b_q4_k_m\": REPO_ROOT / \"models/llama3_8b/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\",\n",
    "}\n",
    "\n",
    "def load_language_model(model_key: str) -> Llama:\n",
    "    model_path = MODEL_REGISTRY[model_key]\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "    maximum_context_tokens = 8192 if \"llama3\" in model_key else 4096\n",
    "\n",
    "    return Llama(\n",
    "        model_path=str(model_path),\n",
    "        n_ctx=maximum_context_tokens,\n",
    "        n_gpu_layers=0,\n",
    "        n_threads=os.cpu_count() or 4,\n",
    "        n_batch=256,\n",
    "        logits_all=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "llm = load_language_model(\"llama3_8b_q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_e = \"The bus is late again this week\"\n",
    "msg_k = \"Here it is: the infamous British roasted boar with mint sauce. How to make it perfect.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msg_e: 'The bus is late again this week'\n",
      "e_words: ['the', 'bus', 'is', 'late', 'again', 'this', 'week']\n",
      "\n",
      "msg_k: 'Here it is: the infamous British roasted boar with mint sauce. How to make it perfect.'\n",
      "k_words: ['here', 'it', 'is:', 'the', 'infamous', 'british', 'roasted', 'boar', 'with', 'mint', 'sauce.', 'how', 'to', 'make', 'it', 'perfect.']\n"
     ]
    }
   ],
   "source": [
    "def simple_space_tokenize(text: str) -> List[str]:\n",
    "    #return text.split()\n",
    "    if not text:\n",
    "        return []\n",
    "    return [word.lower() for word in text.strip().split() if word]\n",
    "\n",
    "e_words = simple_space_tokenize(msg_e)\n",
    "k_words = simple_space_tokenize(msg_k)\n",
    "\n",
    "print(\"msg_e:\", repr(msg_e))\n",
    "print(\"e_words:\", e_words)\n",
    "print()\n",
    "print(\"msg_k:\", repr(msg_k))\n",
    "print(\"k_words:\", k_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         The -> 41941\n",
      "         bus -> 1232\n",
      "          is -> 1189\n",
      "        late -> 752\n",
      "       again -> 5\n",
      "        this -> 18\n",
      "        week -> 2\n"
     ]
    }
   ],
   "source": [
    "WORD_PATTERN = re.compile(r\"[a-z]+\")\n",
    "\n",
    "def build_word_token_maps(model: Llama):\n",
    "    word_to_token_id = {}\n",
    "    token_id_to_word = {}\n",
    "    candidate_token_ids = []\n",
    "\n",
    "    vocab_size = model.n_vocab()\n",
    "    for token_id in range(vocab_size):\n",
    "        token_text = model.detokenize([token_id]).decode(\"utf-8\", errors=\"ignore\")\n",
    "        token_clean = token_text.strip().lower()\n",
    "\n",
    "        if not token_clean:\n",
    "            continue\n",
    "        if not WORD_PATTERN.fullmatch(token_clean):\n",
    "            continue\n",
    "\n",
    "        if token_clean not in word_to_token_id:\n",
    "            word_to_token_id[token_clean] = token_id\n",
    "            token_id_to_word[token_id] = token_clean\n",
    "            candidate_token_ids.append(token_id)\n",
    "\n",
    "    return word_to_token_id, token_id_to_word, np.array(candidate_token_ids, dtype=np.int32)\n",
    "\n",
    "def get_word_ranks_with_vocab_map(\n",
    "    words: List[str],\n",
    "    model: Llama,\n",
    "    word_to_token_id: dict,\n",
    "    prefix_text: str = \"\",\n",
    ") -> List[int]:\n",
    "    model.reset()\n",
    "\n",
    "    if prefix_text:\n",
    "        prefix_ids = model.tokenize(prefix_text.encode(\"utf-8\"), add_bos=True, special=False)\n",
    "    else:\n",
    "        prefix_ids = [model.token_bos()]\n",
    "\n",
    "    model.eval(prefix_ids)\n",
    "    ranks: List[int] = []\n",
    "\n",
    "    for word in words:\n",
    "        base = word.lower()\n",
    "        if base not in word_to_token_id:\n",
    "            raise KeyError(f\"Word {word!r} not in word_to_token_id\")\n",
    "\n",
    "        target_token_id = word_to_token_id[base]\n",
    "\n",
    "        logits = np.array(model.scores[model.n_tokens - 1], dtype=np.float32)\n",
    "        sorted_indices = np.argsort(logits)[::-1]\n",
    "        positions = np.where(sorted_indices == target_token_id)[0]\n",
    "        rank = int(positions[0]) + 1\n",
    "        ranks.append(rank)\n",
    "\n",
    "        model.eval([target_token_id])\n",
    "\n",
    "    return ranks\n",
    "\n",
    "\n",
    "word_to_token_id, token_id_to_word, candidate_token_ids = build_word_token_maps(llm)\n",
    "e_words = msg_e.strip().split()\n",
    "ranks_e = get_word_ranks_with_vocab_map(e_words, llm, word_to_token_id, prefix_text=\"\")\n",
    "for w, r in zip(e_words, ranks_e):\n",
    "    print(f\"{w:>12} -> {r}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Word-level rank utilities (global maps + simple API) ---\n",
    "\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from llama_cpp import Llama  # only needed for type hints\n",
    "\n",
    "# Match \"plain\" words like: government, over, spent\n",
    "WORD_PATTERN = re.compile(r\"[a-z]+\")\n",
    "\n",
    "# Global caches so we only scan the vocabulary once\n",
    "WORD_TO_TOKEN_ID: Dict[str, int] = {}\n",
    "TOKEN_ID_TO_WORD: Dict[int, str] = {}\n",
    "CANDIDATE_TOKEN_IDS: np.ndarray | None = None\n",
    "\n",
    "\n",
    "def build_word_token_maps(model: Llama) -> Tuple[Dict[str, int], Dict[int, str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build (once) a mapping between lowercase words and model token ids.\n",
    "\n",
    "    We keep only tokens whose detokenized form, after stripping whitespace\n",
    "    and lowercasing, matches [a-z]+ exactly.\n",
    "\n",
    "    Returns the three globals so you can grab them if you want, but the\n",
    "    results are also cached in WORD_TO_TOKEN_ID, TOKEN_ID_TO_WORD, and\n",
    "    CANDIDATE_TOKEN_IDS for reuse.\n",
    "    \"\"\"\n",
    "    global WORD_TO_TOKEN_ID, TOKEN_ID_TO_WORD, CANDIDATE_TOKEN_IDS\n",
    "\n",
    "    # Already built: just return cached values\n",
    "    if WORD_TO_TOKEN_ID and CANDIDATE_TOKEN_IDS is not None:\n",
    "        return WORD_TO_TOKEN_ID, TOKEN_ID_TO_WORD, CANDIDATE_TOKEN_IDS\n",
    "\n",
    "    word_to_token_id: Dict[str, int] = {}\n",
    "    token_id_to_word: Dict[int, str] = {}\n",
    "    candidate_token_ids: List[int] = []\n",
    "\n",
    "    vocabulary_size = model.n_vocab()\n",
    "    print(f\"Scanning vocabulary of size {vocabulary_size}...\")\n",
    "\n",
    "    for token_id in range(vocabulary_size):\n",
    "        token_text = model.detokenize([token_id]).decode(\"utf-8\", errors=\"ignore\")\n",
    "        token_clean = token_text.strip().lower()\n",
    "\n",
    "        if not token_clean:\n",
    "            continue\n",
    "        if not WORD_PATTERN.fullmatch(token_clean):\n",
    "            continue\n",
    "\n",
    "        if token_clean not in word_to_token_id:\n",
    "            word_to_token_id[token_clean] = token_id\n",
    "            token_id_to_word[token_id] = token_clean\n",
    "            candidate_token_ids.append(token_id)\n",
    "\n",
    "    candidate_token_ids_array = np.array(candidate_token_ids, dtype=np.int32)\n",
    "\n",
    "    WORD_TO_TOKEN_ID = word_to_token_id\n",
    "    TOKEN_ID_TO_WORD = token_id_to_word\n",
    "    CANDIDATE_TOKEN_IDS = candidate_token_ids_array\n",
    "\n",
    "    print(f\"Kept {len(candidate_token_ids_array)} word-like tokens.\")\n",
    "\n",
    "    return WORD_TO_TOKEN_ID, TOKEN_ID_TO_WORD, CANDIDATE_TOKEN_IDS\n",
    "\n",
    "\n",
    "def get_word_ranks(\n",
    "    words: List[str],\n",
    "    prefix_text: str = \"\",\n",
    "    model: Llama | None = None,\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Given a list of space-delimited words, compute the rank of each word's\n",
    "    single-token representation under the model, feeding words sequentially.\n",
    "\n",
    "    - Uses the cached WORD_TO_TOKEN_ID map (builds it on first call).\n",
    "    - prefix_text is the k' context in the paper (can be empty).\n",
    "\n",
    "    Returns:\n",
    "        A list of integer ranks, one per word (1 = most probable).\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        # fall back to the global llm defined earlier in the notebook\n",
    "        global llm\n",
    "        model = llm\n",
    "\n",
    "    # Ensure vocabulary maps are ready\n",
    "    build_word_token_maps(model)\n",
    "\n",
    "    # Reset model state and prime with BOS or BOS+prefix\n",
    "    model.reset()\n",
    "    if prefix_text:\n",
    "        prefix_ids = model.tokenize(\n",
    "            prefix_text.encode(\"utf-8\"),\n",
    "            add_bos=True,\n",
    "            special=False,\n",
    "        )\n",
    "    else:\n",
    "        prefix_ids = [model.token_bos()]\n",
    "\n",
    "    model.eval(prefix_ids)\n",
    "\n",
    "    ranks: List[int] = []\n",
    "\n",
    "    for word in words:\n",
    "        base = word.lower()\n",
    "        if base not in WORD_TO_TOKEN_ID:\n",
    "            raise KeyError(f\"Word {word!r} not in WORD_TO_TOKEN_ID map\")\n",
    "\n",
    "        target_token_id = WORD_TO_TOKEN_ID[base]\n",
    "\n",
    "        # Logits for the next token given current context\n",
    "        logits = np.array(model.scores[model.n_tokens - 1], dtype=np.float32)\n",
    "\n",
    "        # Rank = position of target_token_id when sorting logits descending\n",
    "        sorted_indices = np.argsort(logits)[::-1]\n",
    "        positions = np.where(sorted_indices == target_token_id)[0]\n",
    "        if positions.size == 0:\n",
    "            raise RuntimeError(\n",
    "                f\"Token id {target_token_id} for word {word!r} not found in logits\"\n",
    "            )\n",
    "\n",
    "        rank = int(positions[0]) + 1  # 1-based\n",
    "        ranks.append(rank)\n",
    "\n",
    "        # Advance the context by feeding this word's token\n",
    "        model.eval([target_token_id])\n",
    "\n",
    "    return ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning vocabulary of size 128256...\n",
      "Kept 42661 word-like tokens.\n",
      "         The -> 41941\n",
      "         bus -> 1232\n",
      "          is -> 1189\n",
      "        late -> 752\n",
      "       again -> 5\n",
      "        this -> 18\n",
      "        week -> 2\n"
     ]
    }
   ],
   "source": [
    "word_to_token_id, token_id_to_word, candidate_token_ids = build_word_token_maps(llm)\n",
    "e_words = msg_e.strip().split()\n",
    "ranks_e = get_word_ranks_with_vocab_map(e_words, llm, word_to_token_id, prefix_text=\"\")\n",
    "for w, r in zip(e_words, ranks_e):\n",
    "    print(f\"{w:>12} -> {r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
