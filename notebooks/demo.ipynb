{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Sequence, Dict\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "#   notebook is in LlmStenoExplore/notebooks\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "MODEL_REGISTRY = {\n",
    "    \"phi3_mini_q4\": REPO_ROOT / \"models/phi3/Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "    \"llama3_8b_q4_k_m\": REPO_ROOT / \"models/llama3_8b/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\",\n",
    "}\n",
    "\n",
    "def load_language_model(model_key: str) -> Llama:\n",
    "    model_path = MODEL_REGISTRY[model_key]\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "    maximum_context_tokens = 8192 if \"llama3\" in model_key else 4096\n",
    "\n",
    "    return Llama(\n",
    "        model_path=str(model_path),\n",
    "        n_ctx=maximum_context_tokens,\n",
    "        n_gpu_layers=0,\n",
    "        n_threads=os.cpu_count() or 4,\n",
    "        n_batch=256,\n",
    "        logits_all=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "llm = load_language_model(\"llama3_8b_q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_token_ranks_like_paper(\n",
    "    text: str,\n",
    "    model: Llama,\n",
    "    prefix: str = \"A text:\",\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Token-level rank computation following the paper's recipe:\n",
    "\n",
    "      1. Tokenize e and k' with the LLM tokenizer.\n",
    "      2. For each token e_i, compute its rank among ALL vocab tokens\n",
    "         under p(· | k', e_1,...,e_{i-1}).\n",
    "\n",
    "    This mirrors the authors' get_token_ranks_llama_cpp, but is robust\n",
    "    to empty prefix by injecting BOS when prefix == \"\".\n",
    "    \"\"\"\n",
    "    # Tokenize prefix and drop BOS (to match notebook style when prefix != \"\")\n",
    "    if prefix:\n",
    "        prefix_ids = model.tokenize(prefix.encode(\"utf-8\"))[1:]\n",
    "    else:\n",
    "        prefix_ids = []\n",
    "\n",
    "    # Ensure text is valid UTF-8 and tokenize with leading space, drop BOS\n",
    "    text = text.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
    "    text_ids = model.tokenize((\" \" + text).encode(\"utf-8\"))[1:]\n",
    "\n",
    "    model.reset()\n",
    "\n",
    "    # If there is no prefix at all, use BOS as minimal context\n",
    "    if prefix_ids:\n",
    "        model.eval(prefix_ids)\n",
    "    else:\n",
    "        bos_id = model.token_bos()\n",
    "        model.eval([bos_id])\n",
    "        prefix_ids = [bos_id]\n",
    "\n",
    "    ranks: List[int] = []\n",
    "\n",
    "    # One rank per token in text_ids\n",
    "    for token_id in text_ids:\n",
    "        # logits for next token given current context\n",
    "        logits = np.array(model.scores[model.n_tokens - 1], dtype=np.float32)\n",
    "\n",
    "        # rank of token_id among all vocab entries (1-based)\n",
    "        sorted_indices = np.argsort(logits)[::-1]\n",
    "        positions = np.where(sorted_indices == token_id)[0]\n",
    "        if positions.size == 0:\n",
    "            raise RuntimeError(f\"Token id {token_id} not found in logits\")\n",
    "        rank = int(positions[0]) + 1\n",
    "        ranks.append(rank)\n",
    "\n",
    "        # extend context with this token\n",
    "        model.eval([token_id])\n",
    "\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def decode_from_ranks_like_paper(\n",
    "    prompt: str,\n",
    "    ranks: List[int],\n",
    "    model: Llama,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Token-level decoder matching the paper's scheme:\n",
    "\n",
    "      - Tokenize prompt k or k' with the model tokenizer.\n",
    "      - Reset the model and eval the prompt tokens.\n",
    "      - For each rank r_i:\n",
    "          * get logits for next token given current context\n",
    "          * pick the r_i-th most probable token\n",
    "          * feed it and append to the sequence\n",
    "      - Detokenize and strip the prompt text from the front if present.\n",
    "    \"\"\"\n",
    "    # Tokenize prompt and drop BOS (same pattern as authors' code)\n",
    "    if prompt:\n",
    "        prompt_ids = model.tokenize(prompt.encode(\"utf-8\"))[1:]\n",
    "    else:\n",
    "        prompt_ids = []\n",
    "        # For robustness, inject BOS when prompt is empty\n",
    "        bos_id = model.token_bos()\n",
    "        prompt_ids = [bos_id]\n",
    "\n",
    "    model.reset()\n",
    "    model.eval(prompt_ids)\n",
    "\n",
    "    generated_ids = list(prompt_ids)\n",
    "\n",
    "    for rank in ranks:\n",
    "        # logits for next token given current context\n",
    "        logits = np.array(model.scores[model.n_tokens - 1], dtype=np.float32)\n",
    "\n",
    "        sorted_indices = np.argsort(logits)[::-1]\n",
    "        if rank < 1 or rank > len(sorted_indices):\n",
    "            raise ValueError(\n",
    "                f\"Rank {rank} out of range for vocabulary size {len(sorted_indices)}\"\n",
    "            )\n",
    "\n",
    "        next_token_id = int(sorted_indices[rank - 1])\n",
    "        generated_ids.append(next_token_id)\n",
    "\n",
    "        # advance context\n",
    "        model.eval([next_token_id])\n",
    "\n",
    "    decoded_bytes = model.detokenize(generated_ids)\n",
    "    decoded_text = decoded_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # Remove the prompt prefix from the visible text if present\n",
    "    if prompt and decoded_text.startswith(prompt):\n",
    "        decoded_text = decoded_text[len(prompt):].lstrip()\n",
    "\n",
    "    return decoded_text\n",
    "\n",
    "\n",
    "def hide_text_token_level(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    secret_key: str,\n",
    "    model: Llama = llm,\n",
    ") -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Encode pipeline (e -> ranks -> stegotext):\n",
    "\n",
    "      1. Compute ranks for secret_text e after prefix k'.\n",
    "      2. Generate stegotext s from key k by following those ranks.\n",
    "    \"\"\"\n",
    "    ranks = get_token_ranks_like_paper(\n",
    "        text=secret_text,\n",
    "        model=model,\n",
    "        prefix=secret_prefix,\n",
    "    )\n",
    "    stegotext = decode_from_ranks_like_paper(\n",
    "        prompt=secret_key,\n",
    "        ranks=ranks,\n",
    "        model=model,\n",
    "    )\n",
    "    return stegotext, ranks\n",
    "\n",
    "\n",
    "def reveal_text_token_level(\n",
    "    stegotext: str,\n",
    "    secret_prefix: str,\n",
    "    secret_key: str,\n",
    "    model: Llama = llm,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Decode pipeline (s -> ranks -> e):\n",
    "\n",
    "      1. From stegotext s and key k, recover the same ranks.\n",
    "      2. From those ranks and prefix k', reconstruct e.\n",
    "    \"\"\"\n",
    "    recovered_ranks = get_token_ranks_like_paper(\n",
    "        text=stegotext,\n",
    "        model=model,\n",
    "        prefix=secret_key,\n",
    "    )\n",
    "    recovered_text = decode_from_ranks_like_paper(\n",
    "        prompt=secret_prefix,\n",
    "        ranks=recovered_ranks,\n",
    "        model=model,\n",
    "    )\n",
    "    return recovered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(secret_text: str, secret_prefix: str, secret_key: str, model: Llama = llm) -> None:\n",
    "    \"\"\"\n",
    "    Run one full encode/decode example and log everything consistently:\n",
    "      - secret text\n",
    "      - ranks_e and their length\n",
    "      - stegotext\n",
    "      - token counts for secret and stego (same tokenization as get_token_ranks_like_paper)\n",
    "      - recovered text and equality check\n",
    "    \"\"\"\n",
    "    print(\"Secret text e:\")\n",
    "    print(secret_text)\n",
    "    print()\n",
    "\n",
    "    # Encode: e -> (ranks_e) -> stegotext\n",
    "    stegotext, ranks_e = hide_text_token_level(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        secret_key=secret_key,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    print(\"ranks_e (len = {}):\".format(len(ranks_e)))\n",
    "    print(ranks_e)\n",
    "    print()\n",
    "\n",
    "    print(\"Stegotext s:\")\n",
    "    print(stegotext)\n",
    "    print()\n",
    "\n",
    "    # Use the *same* tokenization scheme as get_token_ranks_like_paper:\n",
    "    # tokenize((\" \" + text).encode(\"utf-8\"))[1:]\n",
    "    secret_token_ids = model.tokenize((\" \" + secret_text).encode(\"utf-8\"))[1:]\n",
    "    stego_token_ids  = model.tokenize((\" \" + stegotext).encode(\"utf-8\"))[1:]\n",
    "\n",
    "    print(\"Secret tokens:\", len(secret_token_ids))\n",
    "    print(\"Stego tokens :\", len(stego_token_ids))\n",
    "    print(\"len(ranks_e) :\", len(ranks_e))\n",
    "    print()\n",
    "\n",
    "    # Sanity checks (will raise if something is inconsistent)\n",
    "    assert len(secret_token_ids) == len(ranks_e), \"Token count for e does not match len(ranks_e)\"\n",
    "    assert len(stego_token_ids)  == len(ranks_e), \"Token count for s does not match len(ranks_e)\"\n",
    "\n",
    "    # Decode: s -> (ranks) -> e\n",
    "    recovered_text = reveal_text_token_level(\n",
    "        stegotext=stegotext,\n",
    "        secret_prefix=secret_prefix,\n",
    "        secret_key=secret_key,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    print(\"Recovered e:\")\n",
    "    print(recovered_text)\n",
    "    print(\"Recovered == secret_text:\", recovered_text == secret_text)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret text e:\n",
      "THE CURRENT SYSTEM HAS REPEATEDLY FAILED\n",
      "\n",
      "ranks_e (len = 9):\n",
      "[164, 639, 21, 10, 10, 17, 1, 1, 1]\n",
      "\n",
      "Stegotext s:\n",
      "Get sufficient roas tting time. The\n",
      "\n",
      "Secret tokens: 9\n",
      "Stego tokens : 9\n",
      "len(ranks_e) : 9\n",
      "\n",
      "Recovered e:\n",
      "THE CURRENT SYSTEM HAS REPEATEDLY FAILED\n",
      "Recovered == secret_text: True\n",
      "------------------------------------------------------------\n",
      "Secret text e:\n",
      "The cats like to meow all the time. It is annoying.\n",
      "\n",
      "ranks_e (len = 14):\n",
      "[2, 16879, 67, 1, 81, 1, 25, 1, 1, 1, 3, 2, 6, 1]\n",
      "\n",
      "Stegotext s:\n",
      "Thepurpose…\n",
      "ThereOnceWasASmallJungleInTheAmazonRiverBas\n",
      "\n",
      "Secret tokens: 14\n",
      "Stego tokens : 14\n",
      "len(ranks_e) : 14\n",
      "\n",
      "Recovered e:\n",
      "The cats like to meow all the time. It is annoying.\n",
      "Recovered == secret_text: True\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "secret_text_1  = \"THE CURRENT SYSTEM HAS REPEATEDLY FAILED\"\n",
    "secret_prefix_1 = \"A text:\"   # k'\n",
    "secret_key_1    = \"Here it is: the infamous British roasted boar with mint sauce. How to make it perfect.\"\n",
    "\n",
    "run_example(secret_text_1, secret_prefix_1, secret_key_1, model=llm)\n",
    "\n",
    "# Example 2\n",
    "secret_text_2  = \"The cats like to meow all the time. It is annoying.\"\n",
    "secret_prefix_2 = \"Text:\"  # k'\n",
    "secret_key_2    = \"There is a big jungle in Brazil.\"  # k\n",
    "\n",
    "run_example(secret_text_2, secret_prefix_2, secret_key_2, model=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
