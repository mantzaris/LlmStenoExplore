{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Sequence, Dict, Any\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import string\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "#   notebook is in LlmStenoExplore/notebooks\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "MODEL_REGISTRY = {\n",
    "    \"phi3_mini_q4\": REPO_ROOT / \"models/phi3/Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "    \"llama3_8b_q4_k_m\": REPO_ROOT / \"models/llama3_8b/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\",\n",
    "}\n",
    "\n",
    "def load_language_model(model_key: str) -> Llama:\n",
    "    model_path = MODEL_REGISTRY[model_key]\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "    maximum_context_tokens = 8192 if \"llama3\" in model_key else 4096\n",
    "\n",
    "    return Llama(\n",
    "        model_path=str(model_path),\n",
    "        n_ctx=maximum_context_tokens,\n",
    "        n_gpu_layers=0,\n",
    "        n_threads=os.cpu_count() or 4,\n",
    "        n_batch=256,\n",
    "        logits_all=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "llm = load_language_model(\"llama3_8b_q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _make_prefix_ids(prefix: str, model: Llama) -> List[int]:\n",
    "    \"\"\"\n",
    "    Turn a textual prefix (k or k') into initial context token ids.\n",
    "\n",
    "    - If prefix is non-empty: tokenize it and drop the BOS token\n",
    "      (this matches the authors' implementation).\n",
    "    - If prefix is empty: use a single BOS token.\n",
    "\n",
    "    This is used both in encoding (get_token_ranks_like_paper)\n",
    "    and decoding (decode_from_ranks_like_paper), so empty/non-empty\n",
    "    keys are treated consistently everywhere.\n",
    "    \"\"\"\n",
    "    if prefix:\n",
    "        # Tokenize with BOS, then drop BOS (index 0)\n",
    "        token_ids = model.tokenize(prefix.encode(\"utf-8\"), add_bos=True)\n",
    "        return token_ids[1:]\n",
    "    else:\n",
    "        # No textual prefix: start context from BOS\n",
    "        return [model.token_bos()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_token_ranks_like_paper(\n",
    "    text: str,\n",
    "    model: Llama,\n",
    "    prefix: str = \"A text:\",\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Token-level rank computation following the paper's recipe:\n",
    "\n",
    "      1. Tokenize e and k' with the LLM tokenizer.\n",
    "      2. For each token e_i, compute its rank among ALL vocab tokens\n",
    "         under p(· | k', e_1,...,e_{i-1}).\n",
    "\n",
    "    This mirrors the authors' get_token_ranks_llama_cpp, but uses\n",
    "    _make_prefix_ids so it behaves sensibly even when prefix == \"\".\n",
    "    \"\"\"\n",
    "    # Initial context tokens from k' (or BOS if prefix == \"\")\n",
    "    prefix_ids = _make_prefix_ids(prefix, model)\n",
    "\n",
    "    # Ensure text is valid UTF-8 and tokenize with leading space, drop BOS\n",
    "    text = text.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
    "    text_ids = model.tokenize((\" \" + text).encode(\"utf-8\"), add_bos=True)[1:]\n",
    "\n",
    "    model.reset()\n",
    "    model.eval(prefix_ids)\n",
    "\n",
    "    ranks: List[int] = []\n",
    "\n",
    "    # One rank per token in text_ids\n",
    "    for token_id in text_ids:\n",
    "        # logits for next token given current context\n",
    "        logits = np.array(model.scores[model.n_tokens - 1], dtype=np.float32)\n",
    "\n",
    "        # rank of token_id among all vocab entries (1-based)\n",
    "        sorted_indices = np.argsort(logits)[::-1]\n",
    "        positions = np.where(sorted_indices == token_id)[0]\n",
    "        if positions.size == 0:\n",
    "            raise RuntimeError(f\"Token id {token_id} not found in logits\")\n",
    "        rank = int(positions[0]) + 1\n",
    "        ranks.append(rank)\n",
    "\n",
    "        # extend context with this token\n",
    "        model.eval([token_id])\n",
    "\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_from_ranks_like_paper(\n",
    "    prompt: str,\n",
    "    ranks: List[int],\n",
    "    model: Llama,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Token-level decoder matching the paper's scheme:\n",
    "\n",
    "      - Turn prompt k or k' into initial context via _make_prefix_ids.\n",
    "      - For each rank r_i:\n",
    "          * get logits for next token given current context\n",
    "          * pick the r_i-th most probable token\n",
    "          * feed it and append to the sequence\n",
    "      - Detokenize and, if prompt is non-empty, strip it from the front.\n",
    "    \"\"\"\n",
    "    prompt_ids = _make_prefix_ids(prompt, model)\n",
    "\n",
    "    model.reset()\n",
    "    model.eval(prompt_ids)\n",
    "\n",
    "    generated_ids = list(prompt_ids)\n",
    "\n",
    "    for rank in ranks:\n",
    "        logits = np.array(model.scores[model.n_tokens - 1], dtype=np.float32)\n",
    "\n",
    "        sorted_indices = np.argsort(logits)[::-1]\n",
    "        if rank < 1 or rank > len(sorted_indices):\n",
    "            raise ValueError(\n",
    "                f\"Rank {rank} out of range for vocabulary size {len(sorted_indices)}\"\n",
    "            )\n",
    "\n",
    "        next_token_id = int(sorted_indices[rank - 1])\n",
    "        generated_ids.append(next_token_id)\n",
    "\n",
    "        model.eval([next_token_id])\n",
    "\n",
    "    decoded_bytes = model.detokenize(generated_ids)\n",
    "    decoded_text = decoded_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # If we had a textual prompt, strip it; for empty prompt we only had BOS,\n",
    "    # which normally does not render as visible text.\n",
    "    if prompt and decoded_text.startswith(prompt):\n",
    "        decoded_text = decoded_text[len(prompt):].lstrip()\n",
    "\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hide_text_token_level(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    secret_key: str,\n",
    "    model: Llama = llm,\n",
    ") -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Encode pipeline (e -> ranks -> stegotext):\n",
    "\n",
    "      1. Compute ranks for secret_text e after prefix k'.\n",
    "      2. Generate stegotext s from key k by following those ranks.\n",
    "    \"\"\"\n",
    "    ranks = get_token_ranks_like_paper(\n",
    "        text=secret_text,\n",
    "        model=model,\n",
    "        prefix=secret_prefix,\n",
    "    )\n",
    "    stegotext = decode_from_ranks_like_paper(\n",
    "        prompt=secret_key,\n",
    "        ranks=ranks,\n",
    "        model=model,\n",
    "    )\n",
    "    return stegotext, ranks\n",
    "\n",
    "\n",
    "def reveal_text_token_level(\n",
    "    stegotext: str,\n",
    "    secret_prefix: str,\n",
    "    secret_key: str,\n",
    "    model: Llama = llm,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Decode pipeline (s -> ranks -> e):\n",
    "\n",
    "      1. From stegotext s and key k, recover the same ranks.\n",
    "      2. From those ranks and prefix k', reconstruct e.\n",
    "    \"\"\"\n",
    "    recovered_ranks = get_token_ranks_like_paper(\n",
    "        text=stegotext,\n",
    "        model=model,\n",
    "        prefix=secret_key,\n",
    "    )\n",
    "    recovered_text = decode_from_ranks_like_paper(\n",
    "        prompt=secret_prefix,\n",
    "        ranks=recovered_ranks,\n",
    "        model=model,\n",
    "    )\n",
    "    return recovered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    secret_key: str,\n",
    "    model: Llama = llm,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run one full encode/decode example and log everything consistently:\n",
    "      - secret text\n",
    "      - ranks_e and their length\n",
    "      - stegotext\n",
    "      - token counts for secret and stego (same tokenization as get_token_ranks_like_paper)\n",
    "      - recovered text and equality check\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Secret text e:\")\n",
    "    print(secret_text)\n",
    "    print()\n",
    "\n",
    "    # Encode: e -> (ranks_e) -> stegotext\n",
    "    stegotext, ranks_e = hide_text_token_level(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        secret_key=secret_key,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    print(\"ranks_e (len = {}):\".format(len(ranks_e)))\n",
    "    print(ranks_e)\n",
    "    print()\n",
    "\n",
    "    print(\"Stegotext s:\")\n",
    "    print(stegotext)\n",
    "    print()\n",
    "\n",
    "    # Same tokenization scheme as get_token_ranks_like_paper\n",
    "    secret_token_ids = model.tokenize((\" \" + secret_text).encode(\"utf-8\"), add_bos=True)[1:]\n",
    "    stego_token_ids  = model.tokenize((\" \" + stegotext).encode(\"utf-8\"), add_bos=True)[1:]\n",
    "\n",
    "    print(\"Secret tokens :\", len(secret_token_ids))\n",
    "    print(\"Stego tokens  :\", len(stego_token_ids))\n",
    "    print(\"len(ranks_e)  :\", len(ranks_e))\n",
    "    print()\n",
    "\n",
    "    # Sanity checks\n",
    "    assert len(secret_token_ids) == len(ranks_e), \"Token count for e does not match len(ranks_e)\"\n",
    "    assert len(stego_token_ids)  == len(ranks_e), \"Token count for s does not match len(ranks_e)\"\n",
    "\n",
    "    # Decode: s -> (ranks) -> e\n",
    "    recovered_text = reveal_text_token_level(\n",
    "        stegotext=stegotext,\n",
    "        secret_prefix=secret_prefix,\n",
    "        secret_key=secret_key,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    print(\"Recovered e:\")\n",
    "    print(recovered_text)\n",
    "    print(\"Recovered == secret_text:\", recovered_text == secret_text)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Secret text e:\n",
      "THE CURRENT SYSTEM HAS REPEATEDLY FAILED\n",
      "\n",
      "ranks_e (len = 9):\n",
      "[164, 639, 21, 10, 10, 17, 1, 1, 1]\n",
      "\n",
      "Stegotext s:\n",
      "Get sufficient roas tting time. The\n",
      "\n",
      "Secret tokens : 9\n",
      "Stego tokens  : 9\n",
      "len(ranks_e)  : 9\n",
      "\n",
      "Recovered e:\n",
      "THE CURRENT SYSTEM HAS REPEATEDLY FAILED\n",
      "Recovered == secret_text: True\n",
      "------------------------------------------------------------\n",
      "================================================================================\n",
      "Secret text e:\n",
      "The cats like to meow all the time. It is annoying.\n",
      "\n",
      "ranks_e (len = 14):\n",
      "[89803, 14969, 58, 1, 106, 1, 26, 1, 1, 1, 6, 2, 6, 1]\n",
      "\n",
      "Stegotext s:\n",
      "önemlidir Ringvorstellung Schriftgutachten. They have sharp claws\n",
      "\n",
      "Secret tokens : 14\n",
      "Stego tokens  : 14\n",
      "len(ranks_e)  : 14\n",
      "\n",
      "Recovered e:\n",
      "The cats like to meow all the time. It is annoying.\n",
      "Recovered == secret_text: True\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "secret_text_1  = \"THE CURRENT SYSTEM HAS REPEATEDLY FAILED\"\n",
    "secret_prefix_1 = \"A text:\"   # k'\n",
    "secret_key_1    = \"Here it is: the infamous British roasted boar with mint sauce. How to make it perfect.\"\n",
    "\n",
    "run_example(secret_text_1, secret_prefix_1, secret_key_1, model=llm)\n",
    "\n",
    "# Example 2\n",
    "secret_text_2  = \"The cats like to meow all the time. It is annoying.\"\n",
    "secret_prefix_2 = \"\"  # k'\n",
    "secret_key_2    = \"The cat is a feline member just like lions and tigers but much smaller.\"  # k\n",
    "\n",
    "run_example(secret_text_2, secret_prefix_2, secret_key_2, model=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# goal context\n",
    "\n",
    "conduct research on the 'sensitivity of the key in respect to the stenography text produced, the stego text'. For instance if I have a key eg, 'I like cats' and then another 'I like kittens' to calculate the distance between the keys but then to find the equivalent distance between the stego text to see how much different is the keys are and the mapping to the distance between the stego texts. To do this for a few examples to see how that can work out. so effectively some simple examples of the (d_k, d_s| e) for the same message 'e' and some key and produce the stego text 's' and the distance 'd_k' and 'd_s'.\n",
    "\n",
    "studying here is the map\n",
    "\n",
    "k -> s(k;e)\n",
    "\n",
    "fixed hidden message e, e: how much does changing the key k change the stegotext s? we want empirical pairs\n",
    "\n",
    "(dk(k1,k2), ds(s1,s2)∣e), with  si=s(ki;e)\n",
    "\n",
    "\n",
    "## Distances for keys and stegotexts\n",
    "\n",
    "1. Character level edit distance, Normalized Levenshtein:\n",
    "\n",
    "d_k^{char}(k1,k2) = edit_distance(k1,k2) / max⁡(∣k1∣,∣k2∣,1)\n",
    "\n",
    "says how much you had to literally edit the string.\n",
    "\n",
    "(%pip install python-Levenshtein)\n",
    "\n",
    "2. Token level distance using the same tokenizer as the LLM. Since the protocol is token based, it is natural to look at key distance in token space.\n",
    "\n",
    "Let key_token_ids(k) be the tokenization you already use for prompts (via _make_prefix_ids, but without the BOS heuristic). For two keys with token sequences of possibly different lengths we can use a normalized edit distance in token space.\n",
    "\n",
    "3. Embedding distance\n",
    "\n",
    "In a sentence embedding model (for example a small sentence transformer) we can also measure cosine distance between embeddings of keys:\n",
    "\n",
    "dkemb(k1,k2)=1−cos⁡(emb(k1),emb(k2))\n",
    "\n",
    "says you how far apart the prompts are semantically, not just lexically.\n",
    "\n",
    "4. Token level Hamming distance under the Llama tokenizer.\n",
    "\n",
    "For fixed e, all stegotexts have exactly the same number of tokens (always use the same rank sequence), very clean:\n",
    "\n",
    "dstok(s1,s2)= 1/n \\sum_i^n  \\delta[t_i^(1) \\neq t_i^(2)]\n",
    "\n",
    "where t_i^(j) is the i-th token of stegotext sj\tin the Llama tokenizer and n is the common length. This is a per position token mismatch rate.\n",
    "\n",
    "\n",
    "# plan\n",
    "\n",
    "scatter of key distance vs stego distance\n",
    "\n",
    "Fix a secret text e (something like 10-ish words).\n",
    "Fix a prefix k' (or \"\").\n",
    "Generate many keys of similar length (for example, 5 words).\n",
    "For many key pairs (k1, k2):\n",
    "compute d_k (Levenshtein between keys),\n",
    "compute d_s (Levenshtein between corresponding stegotexts).\n",
    "\n",
    "Plot d_k on the x axis, d_s on the y axis, and save to results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def levenshtein_raw(a: str, b: str) -> int:\n",
    "    \"\"\"\n",
    "    Raw Levenshtein edit distance between two strings.\n",
    "    \"\"\"\n",
    "    return Levenshtein.distance(a, b)\n",
    "\n",
    "\n",
    "def levenshtein_normalized(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Normalized Levenshtein distance in [0, 1], using max length\n",
    "    as the normalization factor.\n",
    "\n",
    "    0.0 means identical strings, values closer to 1.0 mean more different.\n",
    "    \"\"\"\n",
    "    raw_distance = Levenshtein.distance(a, b)\n",
    "    maximum_length = max(len(a), len(b))\n",
    "    if maximum_length == 0:\n",
    "        return 0.0\n",
    "    return raw_distance / maximum_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_key_and_stego_distances(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    secret_key_one: str,\n",
    "    secret_key_two: str,\n",
    "    model: Llama = llm,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    For a fixed secret message e and prefix k',\n",
    "    generate stegotexts for two keys and compute:\n",
    "\n",
    "      - Levenshtein distance between the keys\n",
    "      - Levenshtein distance between the stegotexts\n",
    "\n",
    "    Returns a dictionary with:\n",
    "      - secret_key_one, secret_key_two\n",
    "      - stegotext_one, stegotext_two\n",
    "      - d_k_raw, d_k_norm\n",
    "      - d_s_raw, d_s_norm\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate stegotext for key 1\n",
    "    stegotext_one, _ = hide_text_token_level(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        secret_key=secret_key_one,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # Generate stegotext for key 2\n",
    "    stegotext_two, _ = hide_text_token_level(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        secret_key=secret_key_two,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # Distances between keys\n",
    "    d_k_raw = levenshtein_raw(secret_key_one, secret_key_two)\n",
    "    d_k_norm = levenshtein_normalized(secret_key_one, secret_key_two)\n",
    "\n",
    "    # Distances between stegotexts\n",
    "    d_s_raw = levenshtein_raw(stegotext_one, stegotext_two)\n",
    "    d_s_norm = levenshtein_normalized(stegotext_one, stegotext_two)\n",
    "\n",
    "    return {\n",
    "        \"secret_key_one\": secret_key_one,\n",
    "        \"secret_key_two\": secret_key_two,\n",
    "        \"stegotext_one\": stegotext_one,\n",
    "        \"stegotext_two\": stegotext_two,\n",
    "        \"d_k_raw\": d_k_raw,\n",
    "        \"d_k_norm\": d_k_norm,\n",
    "        \"d_s_raw\": d_s_raw,\n",
    "        \"d_s_norm\": d_s_norm,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret_key_one: I like cats\n",
      "secret_key_two: I like kittens\n",
      "stegotext_one: asticsearch slack-github**\n",
      "\n",
      "I need to find the common interests that are\n",
      "stegotext_two: uyếnTek và các chuyến bay an toàn. I have been following your\n",
      "d_k_raw: 5\n",
      "d_k_norm: 0.35714285714285715\n",
      "d_s_raw: 58\n",
      "d_s_norm: 0.8055555555555556\n"
     ]
    }
   ],
   "source": [
    "secret_text = \"The cats like to meow all the time. It is annoying.\"\n",
    "secret_prefix = \"\"  # or \"A text:\" if you want to use a prefix\n",
    "\n",
    "secret_key_one = \"I like cats\"\n",
    "secret_key_two = \"I like kittens\"\n",
    "\n",
    "result = compute_key_and_stego_distances(\n",
    "    secret_text=secret_text,\n",
    "    secret_prefix=secret_prefix,\n",
    "    secret_key_one=secret_key_one,\n",
    "    secret_key_two=secret_key_two,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precompute_ranks_for_secret(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    model: Llama = llm,\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Compute the rank sequence for a fixed secret text e and prefix k'\n",
    "    once, to be reused for many different keys k.\n",
    "    \"\"\"\n",
    "    ranks = get_token_ranks_like_paper(\n",
    "        text=secret_text,\n",
    "        model=model,\n",
    "        prefix=secret_prefix,\n",
    "    )\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def generate_stegotext_from_ranks(\n",
    "    ranks: List[int],\n",
    "    secret_key: str,\n",
    "    model: Llama = llm,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Given a rank sequence and a key k, generate the corresponding\n",
    "    stegotext s(k; e) by following those ranks under prompt=k.\n",
    "    \"\"\"\n",
    "    stegotext = decode_from_ranks_like_paper(\n",
    "        prompt=secret_key,\n",
    "        ranks=ranks,\n",
    "        model=model,\n",
    "    )\n",
    "    return stegotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_KEY_VOCABULARY = [\n",
    "    \"cats\", \"kittens\", \"dogs\", \"puppies\",\n",
    "    \"music\", \"books\", \"coffee\", \"travel\",\n",
    "    \"coding\", \"movies\", \"reading\", \"walking\",\n",
    "    \"running\", \"summer\", \"winter\", \"sunny\",\n",
    "    \"rainy\", \"happy\", \"sad\", \"quiet\",\n",
    "]\n",
    "\n",
    "\n",
    "def generate_random_keys(\n",
    "    number_of_keys: int,\n",
    "    number_of_words: int,\n",
    "    random_seed: int = 0,\n",
    "    vocabulary: Sequence[str] = DEFAULT_KEY_VOCABULARY,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate simple natural-language-like keys, each with the same\n",
    "    number of words (approx same length). For example:\n",
    "\n",
    "        \"Cats love quiet music.\"\n",
    "    \"\"\"\n",
    "    random_generator = random.Random(random_seed)\n",
    "    keys: List[str] = []\n",
    "\n",
    "    for _ in range(number_of_keys):\n",
    "        words = [random_generator.choice(vocabulary) for _ in range(number_of_words)]\n",
    "        sentence = \" \".join(words).capitalize() + \".\"\n",
    "        keys.append(sentence)\n",
    "\n",
    "    return keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stegotexts_for_keys(\n",
    "    ranks_for_secret: List[int],\n",
    "    keys: Sequence[str],\n",
    "    model: Llama = llm,\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    For a fixed secret (encoded by ranks_for_secret), generate stegotext\n",
    "    for each key.\n",
    "    \"\"\"\n",
    "    stegotext_by_key: Dict[str, str] = {}\n",
    "    for key in keys:\n",
    "        stegotext_by_key[key] = generate_stegotext_from_ranks(\n",
    "            ranks=ranks_for_secret,\n",
    "            secret_key=key,\n",
    "            model=model,\n",
    "        )\n",
    "    return stegotext_by_key\n",
    "\n",
    "\n",
    "def compute_pairwise_key_and_stego_distances(\n",
    "    keys: Sequence[str],\n",
    "    stegotext_by_key: Dict[str, str],\n",
    ") -> List[Dict[str, Any]]:\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for key_one, key_two in combinations(keys, 2):\n",
    "        stego_one = stegotext_by_key[key_one]\n",
    "        stego_two = stegotext_by_key[key_two]\n",
    "\n",
    "        # Distances between keys\n",
    "        d_k_raw = Levenshtein.distance(key_one, key_two)\n",
    "        max_key_length = max(len(key_one), len(key_two), 1)\n",
    "        d_k_norm = d_k_raw / max_key_length\n",
    "\n",
    "        # Distances between stegotexts\n",
    "        d_s_raw = Levenshtein.distance(stego_one, stego_two)\n",
    "        max_stego_length = max(len(stego_one), len(stego_two), 1)\n",
    "        d_s_norm = d_s_raw / max_stego_length\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"key_one\": key_one,\n",
    "                \"key_two\": key_two,\n",
    "                \"stego_one\": stego_one,\n",
    "                \"stego_two\": stego_two,\n",
    "                \"d_k_raw\": d_k_raw,\n",
    "                \"d_k_norm\": d_k_norm,\n",
    "                \"d_s_raw\": d_s_raw,\n",
    "                \"d_s_norm\": d_s_norm,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_key_vs_stego_levenshtein(\n",
    "    pairwise_records: Sequence[Dict[str, Any]],\n",
    "    use_normalized: bool = True,\n",
    "    output_directory: Path = REPO_ROOT / \"results\",\n",
    "    output_filename: str = \"key_vs_stego_levenshtein_scatter.png\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Create a scatter plot with key distance on the x axis and\n",
    "    stegotext distance on the y axis. Save it to output_directory\n",
    "    and return the path.\n",
    "    \"\"\"\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if use_normalized:\n",
    "        x_values = [record[\"d_k_norm\"] for record in pairwise_records]\n",
    "        y_values = [record[\"d_s_norm\"] for record in pairwise_records]\n",
    "        x_label = \"Key Levenshtein distance (normalized)\"\n",
    "        y_label = \"Stegotext Levenshtein distance (normalized)\"\n",
    "    else:\n",
    "        x_values = [record[\"d_k_raw\"] for record in pairwise_records]\n",
    "        y_values = [record[\"d_s_raw\"] for record in pairwise_records]\n",
    "        x_label = \"Key Levenshtein distance (raw)\"\n",
    "        y_label = \"Stegotext Levenshtein distance (raw)\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(x_values, y_values, alpha=0.7)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(\"Sensitivity of stegotext to key (Levenshtein distance)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    output_path = output_directory / output_filename\n",
    "    plt.savefig(output_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved scatter plot to: {output_path}\")\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "key_one: Puppies cats coding travel travel.\n",
      "key_two: Music puppies happy dogs sad.\n",
      "d_k_norm: 0.7941176470588235\n",
      "d_s_norm: 0.9074074074074074\n",
      "===\n",
      "key_one: Puppies cats coding travel travel.\n",
      "key_two: Summer kittens cats dogs coffee.\n",
      "d_k_norm: 0.7647058823529411\n",
      "d_s_norm: 0.8703703703703703\n",
      "===\n",
      "key_one: Puppies cats coding travel travel.\n",
      "key_two: Travel rainy quiet cats happy.\n",
      "d_k_norm: 0.7352941176470589\n",
      "d_s_norm: 0.9464285714285714\n",
      "Saved scatter plot to: /home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_levenshtein_scatter.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_levenshtein_scatter.png')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose secret text and prefix (k')\n",
    "secret_text = \"Cats like to meow all the time, it is annoying.\"\n",
    "secret_prefix = \"\"  # or \"A text:\" if you want to condition e\n",
    "\n",
    "# Precompute ranks for the secret once\n",
    "ranks_for_secret = precompute_ranks_for_secret(\n",
    "    secret_text=secret_text,\n",
    "    secret_prefix=secret_prefix,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# Generate a bunch of keys of equal word length\n",
    "number_of_keys = 30\n",
    "number_of_words_per_key = 5\n",
    "\n",
    "keys = generate_random_keys(\n",
    "    number_of_keys=number_of_keys,\n",
    "    number_of_words=number_of_words_per_key,\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "# Stegotexts for each key\n",
    "stegotext_by_key = generate_stegotexts_for_keys(\n",
    "    ranks_for_secret=ranks_for_secret,\n",
    "    keys=keys,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# All pairwise distances (d_k, d_s | e)\n",
    "pairwise_records = compute_pairwise_key_and_stego_distances(\n",
    "    keys=keys,\n",
    "    stegotext_by_key=stegotext_by_key,\n",
    ")\n",
    "\n",
    "# inspect a couple of records\n",
    "for record in pairwise_records[:3]:\n",
    "    print(\"===\")\n",
    "    print(\"key_one:\", record[\"key_one\"])\n",
    "    print(\"key_two:\", record[\"key_two\"])\n",
    "    print(\"d_k_norm:\", record[\"d_k_norm\"])\n",
    "    print(\"d_s_norm:\", record[\"d_s_norm\"])\n",
    "\n",
    "# Plot and save to ../results/\n",
    "plot_key_vs_stego_levenshtein(\n",
    "    pairwise_records=pairwise_records,\n",
    "    use_normalized=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import Levenshtein\n",
    "\n",
    "\n",
    "def make_key_with_substitution_edits(\n",
    "    base_key: str,\n",
    "    number_of_edits: int,\n",
    "    random_seed: int = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a variant of base_key by applying exactly `number_of_edits`\n",
    "    character substitutions (no insertions or deletions).\n",
    "    This ensures the raw Levenshtein distance is exactly `number_of_edits`\n",
    "    as long as we only perform substitutions.\n",
    "\n",
    "    We only edit alphabetic characters to keep the key readable and\n",
    "    leave spaces and punctuation intact.\n",
    "    \"\"\"\n",
    "    if number_of_edits <= 0:\n",
    "        return base_key\n",
    "\n",
    "    random_generator = random.Random(random_seed)\n",
    "\n",
    "    characters = list(base_key)\n",
    "    editable_positions = [index for index, character in enumerate(characters) if character.isalpha()]\n",
    "\n",
    "    if not editable_positions:\n",
    "        # Fallback: nothing alphabetic to edit\n",
    "        return base_key\n",
    "\n",
    "    # Clamp edits to available positions\n",
    "    edits_to_apply = min(number_of_edits, len(editable_positions))\n",
    "\n",
    "    positions_to_edit = random_generator.sample(editable_positions, edits_to_apply)\n",
    "\n",
    "    alphabet = string.ascii_letters\n",
    "\n",
    "    for index in positions_to_edit:\n",
    "        original_character = characters[index]\n",
    "        possible_replacements = [ch for ch in alphabet if ch != original_character]\n",
    "        characters[index] = random_generator.choice(possible_replacements)\n",
    "\n",
    "    mutated_key = \"\".join(characters)\n",
    "\n",
    "    # Optional assertion: Levenshtein distance should match the number of edits we applied\n",
    "    raw_distance = Levenshtein.distance(base_key, mutated_key)\n",
    "    assert raw_distance == edits_to_apply, f\"Expected distance {edits_to_apply}, got {raw_distance}\"\n",
    "\n",
    "    return mutated_key\n",
    "\n",
    "\n",
    "from typing import List, Dict, Any, Sequence\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def key_distance_sweep_against_base(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    base_key: str,\n",
    "    edit_counts: Sequence[int],\n",
    "    samples_per_edit: int,\n",
    "    model: Llama = llm,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For a fixed secret text e, prefix k' and base key k_base, create variants of k_base\n",
    "    at various character-level edit distances and measure:\n",
    "\n",
    "      - d_k_raw, d_k_norm between k_base and k_variant\n",
    "      - d_s_raw, d_s_norm between their corresponding stegotexts\n",
    "\n",
    "    Returns a list of records, one per (edit_count, sample) pair.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Precompute ranks for the secret once\n",
    "    ranks_for_secret = precompute_ranks_for_secret(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # 2. Stegotext for the base key\n",
    "    stego_base = generate_stegotext_from_ranks(\n",
    "        ranks=ranks_for_secret,\n",
    "        secret_key=base_key,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for edit_count in edit_counts:\n",
    "        for sample_index in range(samples_per_edit):\n",
    "            mutated_key = make_key_with_substitution_edits(\n",
    "                base_key=base_key,\n",
    "                number_of_edits=edit_count,\n",
    "                random_seed=1000 + edit_count * 100 + sample_index,\n",
    "            )\n",
    "\n",
    "            stego_mutated = generate_stegotext_from_ranks(\n",
    "                ranks=ranks_for_secret,\n",
    "                secret_key=mutated_key,\n",
    "                model=model,\n",
    "            )\n",
    "\n",
    "            # Key distances\n",
    "            d_k_raw = Levenshtein.distance(base_key, mutated_key)\n",
    "            max_key_length = max(len(base_key), len(mutated_key), 1)\n",
    "            d_k_norm = d_k_raw / max_key_length\n",
    "\n",
    "            # Stegotext distances\n",
    "            d_s_raw = Levenshtein.distance(stego_base, stego_mutated)\n",
    "            max_stego_length = max(len(stego_base), len(stego_mutated), 1)\n",
    "            d_s_norm = d_s_raw / max_stego_length\n",
    "\n",
    "            records.append(\n",
    "                {\n",
    "                    \"base_key\": base_key,\n",
    "                    \"mutated_key\": mutated_key,\n",
    "                    \"edit_count\": edit_count,\n",
    "                    \"stego_base\": stego_base,\n",
    "                    \"stego_mutated\": stego_mutated,\n",
    "                    \"d_k_raw\": d_k_raw,\n",
    "                    \"d_k_norm\": d_k_norm,\n",
    "                    \"d_s_raw\": d_s_raw,\n",
    "                    \"d_s_norm\": d_s_norm,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def build_uniform_edit_counts(base_key: str, number_of_levels: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Build a set of edit_counts that give (approximately) uniformly spaced\n",
    "    normalized distances d_k_norm in [0, 1], relative to the number of\n",
    "    alphabetic characters in base_key.\n",
    "    \"\"\"\n",
    "    number_of_alphabetic_characters = sum(character.isalpha() for character in base_key)\n",
    "    if number_of_alphabetic_characters == 0:\n",
    "        return [0]\n",
    "\n",
    "    raw_counts: List[int] = []\n",
    "    for level_index in range(number_of_levels):\n",
    "        fraction = level_index / max(number_of_levels - 1, 1)\n",
    "        count = int(round(fraction * number_of_alphabetic_characters))\n",
    "        raw_counts.append(count)\n",
    "\n",
    "    # Remove duplicates and sort\n",
    "    edit_counts = sorted(set(raw_counts))\n",
    "    return edit_counts\n",
    "\n",
    "def plot_sweep_key_vs_stego_levenshtein(\n",
    "    records: Sequence[Dict[str, Any]],\n",
    "    output_directory: Path = REPO_ROOT / \"results\",\n",
    "    output_filename: str = \"key_vs_stego_levenshtein_sweep.png\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Scatter plot of d_k_norm vs d_s_norm for the base-vs-variant experiment.\n",
    "    \"\"\"\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    x_values = [record[\"d_k_norm\"] for record in records]\n",
    "    y_values = [record[\"d_s_norm\"] for record in records]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(x_values, y_values, alpha=0.7)\n",
    "    plt.xlabel(\"Key Levenshtein distance (normalized, base vs variant)\")\n",
    "    plt.ylabel(\"Stegotext Levenshtein distance (normalized)\")\n",
    "    plt.title(\"Sensitivity of stegotext to key edits (base key sweep)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    output_path = output_directory / output_filename\n",
    "    plt.savefig(output_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved sweep scatter plot to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sweep scatter plot to: /home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_levenshtein_sweep.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_levenshtein_sweep.png')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secret_text = \"The cats like to meow all the time. It is annoying.\"\n",
    "secret_prefix = \"\"  # or \"A text:\" if you prefer\n",
    "base_key = \"Puppies cats coding travel travel.\"\n",
    "\n",
    "# Build many edit levels, roughly uniformly spanning [0, 1] in d_k_norm\n",
    "edit_counts = build_uniform_edit_counts(base_key=base_key, number_of_levels=30)\n",
    "\n",
    "samples_per_edit = 5  # same as before\n",
    "\n",
    "sweep_records = key_distance_sweep_against_base(\n",
    "    secret_text=secret_text,\n",
    "    secret_prefix=secret_prefix,\n",
    "    base_key=base_key,\n",
    "    edit_counts=edit_counts,\n",
    "    samples_per_edit=samples_per_edit,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "plot_sweep_key_vs_stego_levenshtein(sweep_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " new plot *does* show a very strong \"avalanche like\" sensitivity, but it is not literally a cryptographic hash. It is more like:\n",
    "\n",
    "> For almost any nonzero character level change to the key, the resulting stegotext is almost as different as it can be (by Levenshtein), given the fixed length and language constraints.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1.  plot is really showing\n",
    "\n",
    "- fix:\n",
    "  - secret text `e`\n",
    "  - prefix `k'`\n",
    "  - base key `k_base`\n",
    "- For each mutated key `k_variant` :\n",
    "  - change some number of characters in `k_base` (no insertions, only substitutions),\n",
    "  - generate stegotext `s_base = s(k_base; e)`,\n",
    "  - generate `s_variant = s(k_variant; e)`,\n",
    "  - compute `d_k_norm(k_base, k_variant)` and `d_s_norm(s_base, s_variant)`.\n",
    "\n",
    "The plot shows:\n",
    "\n",
    "- one point at `(0, 0)` (same key -> same stegotext), and  \n",
    "- for any nonzero `d_k_norm`, almost all `d_s_norm` values are in roughly `[0.8, 0.95]`.\n",
    "\n",
    "\n",
    "> Once the key differs at all in characters, the stegotext looks almost maximally different at the character level.\n",
    "\n",
    "That is exactly the flat band\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why a tiny change in the key can produce a huge change in the stegotext\n",
    "\n",
    "Two effects combine here.\n",
    "\n",
    "### 2.1 Character edits are huge from the model's perspective\n",
    "\n",
    "Your `make_key_with_substitution_edits` changes individual characters inside words. To Levenshtein this is a tiny change. But to the Llama tokenizer and model it can be catastrophic:\n",
    "\n",
    "- `\"coding\"` -> `\"coxing\"` or `\"codxng\"` often changes the tokenization completely.\n",
    "- Many mutated words become out of distribution or extremely rare subword tokens.\n",
    "\n",
    "So from the LLM's point of view, even one or two character substitutions often mean:\n",
    "\n",
    "- \"clean normal prompt\" vs \"weird noisy prompt\".\n",
    "\n",
    "The model's internal representation of the context (and therefore its probability ordering over the next token) can change dramatically, even though Levenshtein says \"distance 1 or 2\".\n",
    "\n",
    "sweep is really probing something like:\n",
    "\n",
    "> \"How different are stegotexts when I move the key from a natural English sentence to various corrupted versions of that sentence?\"\n",
    "\n",
    "Given the paper's protocol, the key `k` only enters through the conditional distribution `p(. | k, s_<i>)`. If the context embedding changes a lot, the sorted rank order of tokens at each step changes a lot, and with a fixed rank sequence `r_i` you get almost entirely different tokens. :contentReference[oaicite:0]{index=0}  \n",
    "\n",
    "### 2.2 Fixed rank sequence + different contexts is like random relabeling\n",
    "\n",
    "For each token position `i`, the protocol does:\n",
    "\n",
    "- For key `k_1`: choose the token at rank `r_i` under `p(. | k_1, s^{(1)}_<i>)`.\n",
    "- For key `k_2`: choose the token at the same rank `r_i` but under `p(. | k_2, s^{(2)}_<i>)`.\n",
    "\n",
    "If you roughly model \"sorted vocab by probability under context 1\" and \"sorted vocab under context 2\" as two different permutations of the vocabulary, then \"rank `r_i` under context 1\" and \"rank `r_i` under context 2\" will almost never be the same token. That means at each position:\n",
    "\n",
    "- Probability that the two stegotexts share the same token is very small.\n",
    "- So the expected fraction of matching tokens is tiny, and the normalized Levenshtein distance is close to 1.\n",
    "\n",
    "Because your stegotexts are of moderate length and use natural language tokens see roughly `0.8-0.95` rather than literally `1.0`, but it is clearly very high.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Is this \"like a hash function\"?\n",
    "\n",
    "In spirit, yes in one important sense, but no in several others.\n",
    "\n",
    "### 3.1 How it is similar to a hash\n",
    "\n",
    "seeing an avalanche like effect:\n",
    "\n",
    "> Any nonzero small character change to `k` almost always produces a stegotext that is \"maximally scrambled\" compared to `s(k_base; e)` under your distance measure.\n",
    "\n",
    "This is exactly the kind of behavior we qualitatively expect from a good hash: output looks essentially unrelated even for tiny input changes.\n",
    "\n",
    "Given the protocol in the paper, that is not surprising: the key's job is to set the entire probability landscape from which we pick tokens by pre fixed ranks. A slightly different landscape generally yields a completely different path. :contentReference[oaicite:1]{index=1}  \n",
    "\n",
    "So, for character level perturbations of this type, our empirical plots are telling us:\n",
    "\n",
    "> The map `k -> s(k; e)` behaves almost like a chaotic function: once you depart from exactly the same key, the resulting stegotexts are very far apart.\n",
    "\n",
    "### 3.2 How it differs from a cryptographic hash\n",
    "\n",
    "However, it is not a cryptographic hash:\n",
    "\n",
    "1. **Not designed for uniformity or bit level independence**\n",
    "\n",
    "   In a hash, changing one input bit flips each output bit with probability 0.5 independently. Here:\n",
    "\n",
    "   - Output is constrained to be natural language.\n",
    "   - There are correlations between tokens due to grammar, semantics, and the fixed rank sequence.\n",
    "   - Distances saturate around `0.8-0.95`, not `1.0`, and in token space there may be more structure than Levenshtein exposes.\n",
    "\n",
    "2. **Metric mismatch**\n",
    "\n",
    "   - You measure distance between keys by character Levenshtein.\n",
    "   - The model \"feels\" keys in token or embedding space. Two keys that are very close in Levenshtein can be very far to the model (your current experiment), and two keys that are quite far in Levenshtein but semantically similar (word level edits) might have much more similar stegotexts.\n",
    "   - So the apparent avalanche is partly an artifact of using a metric that damages words, not just changes their semantics.\n",
    "\n",
    "3. **No formal collision resistance or preimage resistance**\n",
    "\n",
    "   - Many different keys will lead to broadly similar stegotexts, and the protocol is not designed to minimize such collisions.\n",
    "   - An attacker who knows `e`, `k'`, and the model can trivially generate infinitely many different keys that produce stegotexts for the same `e` (just change `k`).\n",
    "   - Security in the paper relies on the secrecy of the key and the need to match both the model and the key, not on hash like one wayness. :contentReference[oaicite:2]{index=2}  \n",
    "\n",
    "4. **Local versus global behavior**\n",
    "\n",
    "   - A hash is equally scrambling everywhere in its input space.\n",
    "   - Here, behavior might depend on the region of prompt space you are in. Your current experiment mutates a single base key with fairly aggressive character noise; if you instead moved between semantically close, clean prompts (word substitutions, added detail, style tweaks), you might see more structure and less perfect scrambling.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. How to sharpen this picture with further experiments\n",
    "\n",
    "If we want to make the \"hash like\" statement more precise, there are a few natural next experiments:\n",
    "\n",
    "1. **Word level edits instead of character noise**\n",
    "\n",
    "   - Replace the base key's words with other words from your vocabulary (keeping grammar and structure intact).\n",
    "   - Measure `(d_k, d_s)` again.\n",
    "   - If stegotext distances are still high even when keys remain grammatical and similar in meaning, that is stronger evidence of intrinsic sensitivity rather than just \"the model hates corrupted strings\".\n",
    "\n",
    "2. **Token level Hamming distance**\n",
    "\n",
    "   - Compute the fraction of token positions where two stegotexts disagree.\n",
    "   - Compare that to character level Levenshtein. Your `0.8-0.95` may simply reflect morphological and subword similarities.\n",
    "\n",
    "3. **Compare local vs global**\n",
    "\n",
    "   - Put the base key sweep points and the random key pair points on the same scatter (different colors).\n",
    "   - If both clouds sit in the same high `d_s` band, that supports the \"almost any difference in key -> huge difference in stegotext\" story.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Bottom line\n",
    "\n",
    "- Our latest plot does show that, under your current notion of \"small change in key\" (character substitutions), the mapping `k -> s(k; e)` is extremely sensitive: once `d_k > 0`, `d_s` is already very large and does not grow much further.\n",
    "- This is qualitatively hash like in the avalanche sense: tiny key changes yield almost maximally different stegotexts.\n",
    "- But it is not a cryptographic hash: the output space is constrained, the metric is not bit level, and there is no formal security guarantee or uniformity.\n",
    "\n",
    "Conceptually, we can say:\n",
    "\n",
    "> For this protocol and this model, the stegotext behaves approximately like a chaotic function of the key - more like a hash than like a smooth function - especially when you look at character level perturbations.\n",
    "\n",
    "If we repeat this with word level or prompt style variations, you will get a more nuanced picture of how \"hashy\" it really is when the key changes are ones the model perceives as small rather than corrupted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import Levenshtein\n",
    "\n",
    "\n",
    "def make_key_with_word_replacements(\n",
    "    base_key: str,\n",
    "    number_of_word_replacements: int,\n",
    "    replacement_vocabulary: list[str],\n",
    "    random_seed: int | None = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Replace `number_of_word_replacements` word tokens in base_key with words\n",
    "    drawn from replacement_vocabulary.\n",
    "\n",
    "    This keeps the key grammatical and avoids introducing corrupted tokens,\n",
    "    so the model is more likely to see these changes as \"small\" than raw\n",
    "    character noise.\n",
    "    \"\"\"\n",
    "    if number_of_word_replacements <= 0:\n",
    "        return base_key\n",
    "\n",
    "    random_generator = random.Random(random_seed)\n",
    "\n",
    "    tokens = base_key.split()\n",
    "\n",
    "    # Only consider tokens that contain at least one alphabetic character\n",
    "    editable_positions = [\n",
    "        index\n",
    "        for index, token in enumerate(tokens)\n",
    "        if any(character.isalpha() for character in token)\n",
    "    ]\n",
    "\n",
    "    if not editable_positions:\n",
    "        return base_key\n",
    "\n",
    "    replacements_to_apply = min(number_of_word_replacements, len(editable_positions))\n",
    "    positions_to_replace = random_generator.sample(\n",
    "        editable_positions,\n",
    "        replacements_to_apply,\n",
    "    )\n",
    "\n",
    "    mutated_tokens = list(tokens)\n",
    "\n",
    "    for index in positions_to_replace:\n",
    "        original_token = mutated_tokens[index]\n",
    "\n",
    "        # Separate core word from trailing punctuation, e.g. \"cats.\" -> (\"cats\", \".\")\n",
    "        match = re.match(r\"^([A-Za-z']+)([^A-Za-z']*)$\", original_token)\n",
    "        if match is None:\n",
    "            # If we cannot parse it nicely, just skip this token\n",
    "            continue\n",
    "\n",
    "        original_word = match.group(1)\n",
    "        trailing_punctuation = match.group(2)\n",
    "\n",
    "        # Choose a replacement word distinct from the original (case insensitive)\n",
    "        candidate_words = [\n",
    "            word for word in replacement_vocabulary\n",
    "            if word.lower() != original_word.lower()\n",
    "        ]\n",
    "        if not candidate_words:\n",
    "            continue\n",
    "\n",
    "        replacement_word = random_generator.choice(candidate_words)\n",
    "\n",
    "        # Preserve capitalization pattern of the original word\n",
    "        if original_word.istitle():\n",
    "            replacement_word = replacement_word.capitalize()\n",
    "        elif original_word.isupper():\n",
    "            replacement_word = replacement_word.upper()\n",
    "\n",
    "        mutated_tokens[index] = replacement_word + trailing_punctuation\n",
    "\n",
    "    mutated_key = \" \".join(mutated_tokens)\n",
    "    return mutated_key\n",
    "\n",
    "\n",
    "REPLACEMENT_VOCABULARY = [\n",
    "    \"cats\", \"kittens\", \"dogs\", \"puppies\",\n",
    "    \"music\", \"books\", \"coffee\", \"tea\",\n",
    "    \"travel\", \"coding\", \"movies\", \"reading\",\n",
    "    \"walking\", \"running\", \"summer\", \"winter\",\n",
    "    \"sunny\", \"rainy\", \"happy\", \"quiet\",\n",
    "    \"busy\", \"calm\", \"evening\", \"morning\",\n",
    "]\n",
    "\n",
    "STYLE_PREFIXES = [\n",
    "    \"\",\n",
    "    \"In my opinion,\",\n",
    "    \"Honestly,\",\n",
    "    \"From my perspective,\",\n",
    "    \"To be honest,\",\n",
    "]\n",
    "\n",
    "STYLE_SUFFIXES = [\n",
    "    \"\",\n",
    "    \"and that is just how I see it.\",\n",
    "    \"most of the time.\",\n",
    "    \"when I have some free time.\",\n",
    "    \"especially on weekends.\",\n",
    "]\n",
    "\n",
    "\n",
    "def make_key_with_style_variation(\n",
    "    base_key: str,\n",
    "    random_seed: int | None = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a stylistic variant of base_key by adding a soft prefix and/or suffix.\n",
    "\n",
    "    This changes tone and length, but keeps the sentence clean and grammatical.\n",
    "    \"\"\"\n",
    "    random_generator = random.Random(random_seed)\n",
    "\n",
    "    prefix = random_generator.choice(STYLE_PREFIXES)\n",
    "    suffix = random_generator.choice(STYLE_SUFFIXES)\n",
    "\n",
    "    mutated_key = base_key\n",
    "    if prefix:\n",
    "        mutated_key = prefix + \" \" + mutated_key\n",
    "    if suffix:\n",
    "        mutated_key = mutated_key + \" \" + suffix\n",
    "\n",
    "    return mutated_key\n",
    "\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def build_uniform_word_edit_counts(base_key: str, number_of_levels: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Build a list of distinct word replacement counts that roughly span\n",
    "    from 0 to the maximum possible number of word replacements.\n",
    "    \"\"\"\n",
    "    tokens = base_key.split()\n",
    "    number_of_tokens = len(tokens)\n",
    "    if number_of_tokens == 0:\n",
    "        return [0]\n",
    "\n",
    "    raw_counts: List[int] = []\n",
    "    for level_index in range(number_of_levels):\n",
    "        fraction = level_index / max(number_of_levels - 1, 1)\n",
    "        count = int(round(fraction * number_of_tokens))\n",
    "        raw_counts.append(count)\n",
    "\n",
    "    edit_counts = sorted(set(raw_counts))\n",
    "    return edit_counts\n",
    "\n",
    "\n",
    "from typing import Dict, Any, Sequence\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def key_distance_sweep_word_level(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    base_key: str,\n",
    "    edit_counts: Sequence[int],\n",
    "    samples_per_edit: int,\n",
    "    replacement_vocabulary: list[str],\n",
    "    model: Llama = llm,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For a fixed secret text e, prefix k' and base key k_base, create variants of k_base\n",
    "    using WORD-LEVEL replacements and measure:\n",
    "\n",
    "      - d_k_raw, d_k_norm between k_base and k_variant\n",
    "      - d_s_raw, d_s_norm between their corresponding stegotexts\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Precompute ranks for the secret once\n",
    "    ranks_for_secret = precompute_ranks_for_secret(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # 2. Stegotext for the base key\n",
    "    stegotext_base = generate_stegotext_from_ranks(\n",
    "        ranks=ranks_for_secret,\n",
    "        secret_key=base_key,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for edit_count in edit_counts:\n",
    "        for sample_index in range(samples_per_edit):\n",
    "            mutated_key = make_key_with_word_replacements(\n",
    "                base_key=base_key,\n",
    "                number_of_word_replacements=edit_count,\n",
    "                replacement_vocabulary=replacement_vocabulary,\n",
    "                random_seed=2000 + edit_count * 100 + sample_index,\n",
    "            )\n",
    "\n",
    "            stegotext_mutated = generate_stegotext_from_ranks(\n",
    "                ranks=ranks_for_secret,\n",
    "                secret_key=mutated_key,\n",
    "                model=model,\n",
    "            )\n",
    "\n",
    "            # Key distances\n",
    "            d_k_raw = Levenshtein.distance(base_key, mutated_key)\n",
    "            max_key_length = max(len(base_key), len(mutated_key), 1)\n",
    "            d_k_norm = d_k_raw / max_key_length\n",
    "\n",
    "            # Stegotext distances\n",
    "            d_s_raw = Levenshtein.distance(stegotext_base, stegotext_mutated)\n",
    "            max_stego_length = max(len(stegotext_base), len(stegotext_mutated), 1)\n",
    "            d_s_norm = d_s_raw / max_stego_length\n",
    "\n",
    "            records.append(\n",
    "                {\n",
    "                    \"base_key\": base_key,\n",
    "                    \"mutated_key\": mutated_key,\n",
    "                    \"edit_count\": edit_count,\n",
    "                    \"stegotext_base\": stegotext_base,\n",
    "                    \"stegotext_mutated\": stegotext_mutated,\n",
    "                    \"d_k_raw\": d_k_raw,\n",
    "                    \"d_k_norm\": d_k_norm,\n",
    "                    \"d_s_raw\": d_s_raw,\n",
    "                    \"d_s_norm\": d_s_norm,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return records\n",
    "\n",
    "def key_distance_sweep_style_variations(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    base_key: str,\n",
    "    number_of_variants: int,\n",
    "    model: Llama = llm,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For a fixed secret text e, prefix k' and base key k_base, create stylistic\n",
    "    variants of k_base and measure key and stegotext distances.\n",
    "    \"\"\"\n",
    "\n",
    "    ranks_for_secret = precompute_ranks_for_secret(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    stegotext_base = generate_stegotext_from_ranks(\n",
    "        ranks=ranks_for_secret,\n",
    "        secret_key=base_key,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for variant_index in range(number_of_variants):\n",
    "        mutated_key = make_key_with_style_variation(\n",
    "            base_key=base_key,\n",
    "            random_seed=3000 + variant_index,\n",
    "        )\n",
    "\n",
    "        stegotext_mutated = generate_stegotext_from_ranks(\n",
    "            ranks=ranks_for_secret,\n",
    "            secret_key=mutated_key,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "        d_k_raw = Levenshtein.distance(base_key, mutated_key)\n",
    "        max_key_length = max(len(base_key), len(mutated_key), 1)\n",
    "        d_k_norm = d_k_raw / max_key_length\n",
    "\n",
    "        d_s_raw = Levenshtein.distance(stegotext_base, stegotext_mutated)\n",
    "        max_stego_length = max(len(stegotext_base), len(stegotext_mutated), 1)\n",
    "        d_s_norm = d_s_raw / max_stego_length\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"base_key\": base_key,\n",
    "                \"mutated_key\": mutated_key,\n",
    "                \"variant_index\": variant_index,\n",
    "                \"stegotext_base\": stegotext_base,\n",
    "                \"stegotext_mutated\": stegotext_mutated,\n",
    "                \"d_k_raw\": d_k_raw,\n",
    "                \"d_k_norm\": d_k_norm,\n",
    "                \"d_s_raw\": d_s_raw,\n",
    "                \"d_s_norm\": d_s_norm,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sweep scatter plot to: /home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_word_level_sweep.png\n",
      "Saved sweep scatter plot to: /home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_style_variation_sweep.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_style_variation_sweep.png')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secret_text = \"The cats like to meow all the time. It is annoying.\"\n",
    "secret_prefix = \"\"  # or \"A text:\" if you want a k' prefix\n",
    "base_key = \"Puppies cats coding travel travel.\"\n",
    "\n",
    "# Word-level sweep\n",
    "word_edit_counts = build_uniform_word_edit_counts(\n",
    "    base_key=base_key,\n",
    "    number_of_levels=10,\n",
    ")\n",
    "\n",
    "samples_per_edit = 5\n",
    "\n",
    "word_sweep_records = key_distance_sweep_word_level(\n",
    "    secret_text=secret_text,\n",
    "    secret_prefix=secret_prefix,\n",
    "    base_key=base_key,\n",
    "    edit_counts=word_edit_counts,\n",
    "    samples_per_edit=samples_per_edit,\n",
    "    replacement_vocabulary=REPLACEMENT_VOCABULARY,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "plot_sweep_key_vs_stego_levenshtein(\n",
    "    records=word_sweep_records,\n",
    "    output_filename=\"key_vs_stego_word_level_sweep.png\",\n",
    ")\n",
    "\n",
    "# Style-variation sweep\n",
    "style_sweep_records = key_distance_sweep_style_variations(\n",
    "    secret_text=secret_text,\n",
    "    secret_prefix=secret_prefix,\n",
    "    base_key=base_key,\n",
    "    number_of_variants=40,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "plot_sweep_key_vs_stego_levenshtein(\n",
    "    records=style_sweep_records,\n",
    "    output_filename=\"key_vs_stego_style_variation_sweep.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
