{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Sequence, Dict, Any\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import string\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "#   notebook is in LlmStenoExplore/notebooks\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "MODEL_REGISTRY = {\n",
    "    \"phi3_mini_q4\": REPO_ROOT / \"models/phi3/Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "    \"llama3_8b_q4_k_m\": REPO_ROOT / \"models/llama3_8b/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\",\n",
    "}\n",
    "\n",
    "def load_language_model(model_key: str) -> Llama:\n",
    "    model_path = MODEL_REGISTRY[model_key]\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "    maximum_context_tokens = 8192 if \"llama3\" in model_key else 4096\n",
    "\n",
    "    return Llama(\n",
    "        model_path=str(model_path),\n",
    "        n_ctx=maximum_context_tokens,\n",
    "        n_gpu_layers=0,\n",
    "        n_threads=os.cpu_count() or 4,\n",
    "        n_batch=256,\n",
    "        logits_all=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "llm = load_language_model(\"llama3_8b_q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _make_prefix_ids(prefix: str, model: Llama) -> List[int]:\n",
    "    \"\"\"\n",
    "    Turn a textual prefix (k or k') into initial context token ids.\n",
    "\n",
    "    - If prefix is non-empty: tokenize it and drop the BOS token\n",
    "      (this matches the authors' implementation).\n",
    "    - If prefix is empty: use a single BOS token.\n",
    "\n",
    "    This is used both in encoding (get_token_ranks_like_paper)\n",
    "    and decoding (decode_from_ranks_like_paper), so empty/non-empty\n",
    "    keys are treated consistently everywhere.\n",
    "    \"\"\"\n",
    "    if prefix:\n",
    "        # Tokenize with BOS, then drop BOS (index 0)\n",
    "        token_ids = model.tokenize(prefix.encode(\"utf-8\"), add_bos=True)\n",
    "        return token_ids[1:]\n",
    "    else:\n",
    "        # No textual prefix: start context from BOS\n",
    "        return [model.token_bos()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_token_ranks_like_paper(\n",
    "    text: str,\n",
    "    model: Llama,\n",
    "    prefix: str = \"A text:\",\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Token-level rank computation following the paper's recipe:\n",
    "\n",
    "      1. Tokenize e and k' with the LLM tokenizer.\n",
    "      2. For each token e_i, compute its rank among ALL vocab tokens\n",
    "         under p(· | k', e_1,...,e_{i-1}).\n",
    "\n",
    "    This mirrors the authors' get_token_ranks_llama_cpp, but uses\n",
    "    _make_prefix_ids so it behaves sensibly even when prefix == \"\".\n",
    "    \"\"\"\n",
    "    # Initial context tokens from k' (or BOS if prefix == \"\")\n",
    "    prefix_ids = _make_prefix_ids(prefix, model)\n",
    "\n",
    "    # Ensure text is valid UTF-8 and tokenize with leading space, drop BOS\n",
    "    text = text.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
    "    text_ids = model.tokenize((\" \" + text).encode(\"utf-8\"), add_bos=True)[1:]\n",
    "\n",
    "    model.reset()\n",
    "    model.eval(prefix_ids)\n",
    "\n",
    "    ranks: List[int] = []\n",
    "\n",
    "    # One rank per token in text_ids\n",
    "    for token_id in text_ids:\n",
    "        # logits for next token given current context\n",
    "        logits = np.array(model.scores[model.n_tokens - 1], dtype=np.float32)\n",
    "\n",
    "        # rank of token_id among all vocab entries (1-based)\n",
    "        sorted_indices = np.argsort(logits)[::-1]\n",
    "        positions = np.where(sorted_indices == token_id)[0]\n",
    "        if positions.size == 0:\n",
    "            raise RuntimeError(f\"Token id {token_id} not found in logits\")\n",
    "        rank = int(positions[0]) + 1\n",
    "        ranks.append(rank)\n",
    "\n",
    "        # extend context with this token\n",
    "        model.eval([token_id])\n",
    "\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_from_ranks_like_paper(\n",
    "    prompt: str,\n",
    "    ranks: List[int],\n",
    "    model: Llama,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Token-level decoder matching the paper's scheme:\n",
    "\n",
    "      - Turn prompt k or k' into initial context via _make_prefix_ids.\n",
    "      - For each rank r_i:\n",
    "          * get logits for next token given current context\n",
    "          * pick the r_i-th most probable token\n",
    "          * feed it and append to the sequence\n",
    "      - Detokenize and, if prompt is non-empty, strip it from the front.\n",
    "    \"\"\"\n",
    "    prompt_ids = _make_prefix_ids(prompt, model)\n",
    "\n",
    "    model.reset()\n",
    "    model.eval(prompt_ids)\n",
    "\n",
    "    generated_ids = list(prompt_ids)\n",
    "\n",
    "    for rank in ranks:\n",
    "        logits = np.array(model.scores[model.n_tokens - 1], dtype=np.float32)\n",
    "\n",
    "        sorted_indices = np.argsort(logits)[::-1]\n",
    "        if rank < 1 or rank > len(sorted_indices):\n",
    "            raise ValueError(\n",
    "                f\"Rank {rank} out of range for vocabulary size {len(sorted_indices)}\"\n",
    "            )\n",
    "\n",
    "        next_token_id = int(sorted_indices[rank - 1])\n",
    "        generated_ids.append(next_token_id)\n",
    "\n",
    "        model.eval([next_token_id])\n",
    "\n",
    "    decoded_bytes = model.detokenize(generated_ids)\n",
    "    decoded_text = decoded_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # If we had a textual prompt, strip it; for empty prompt we only had BOS,\n",
    "    # which normally does not render as visible text.\n",
    "    if prompt and decoded_text.startswith(prompt):\n",
    "        decoded_text = decoded_text[len(prompt):].lstrip()\n",
    "\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hide_text_token_level(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    secret_key: str,\n",
    "    model: Llama = llm,\n",
    ") -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Encode pipeline (e -> ranks -> stegotext):\n",
    "\n",
    "      1. Compute ranks for secret_text e after prefix k'.\n",
    "      2. Generate stegotext s from key k by following those ranks.\n",
    "    \"\"\"\n",
    "    ranks = get_token_ranks_like_paper(\n",
    "        text=secret_text,\n",
    "        model=model,\n",
    "        prefix=secret_prefix,\n",
    "    )\n",
    "    stegotext = decode_from_ranks_like_paper(\n",
    "        prompt=secret_key,\n",
    "        ranks=ranks,\n",
    "        model=model,\n",
    "    )\n",
    "    return stegotext, ranks\n",
    "\n",
    "\n",
    "def reveal_text_token_level(\n",
    "    stegotext: str,\n",
    "    secret_prefix: str,\n",
    "    secret_key: str,\n",
    "    model: Llama = llm,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Decode pipeline (s -> ranks -> e):\n",
    "\n",
    "      1. From stegotext s and key k, recover the same ranks.\n",
    "      2. From those ranks and prefix k', reconstruct e.\n",
    "    \"\"\"\n",
    "    recovered_ranks = get_token_ranks_like_paper(\n",
    "        text=stegotext,\n",
    "        model=model,\n",
    "        prefix=secret_key,\n",
    "    )\n",
    "    recovered_text = decode_from_ranks_like_paper(\n",
    "        prompt=secret_prefix,\n",
    "        ranks=recovered_ranks,\n",
    "        model=model,\n",
    "    )\n",
    "    return recovered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    secret_key: str,\n",
    "    model: Llama = llm,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run one full encode/decode example and log everything consistently:\n",
    "      - secret text\n",
    "      - ranks_e and their length\n",
    "      - stegotext\n",
    "      - token counts for secret and stego (same tokenization as get_token_ranks_like_paper)\n",
    "      - recovered text and equality check\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Secret text e:\")\n",
    "    print(secret_text)\n",
    "    print()\n",
    "\n",
    "    # Encode: e -> (ranks_e) -> stegotext\n",
    "    stegotext, ranks_e = hide_text_token_level(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        secret_key=secret_key,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    print(\"ranks_e (len = {}):\".format(len(ranks_e)))\n",
    "    print(ranks_e)\n",
    "    print()\n",
    "\n",
    "    print(\"Stegotext s:\")\n",
    "    print(stegotext)\n",
    "    print()\n",
    "\n",
    "    # Same tokenization scheme as get_token_ranks_like_paper\n",
    "    secret_token_ids = model.tokenize((\" \" + secret_text).encode(\"utf-8\"), add_bos=True)[1:]\n",
    "    stego_token_ids  = model.tokenize((\" \" + stegotext).encode(\"utf-8\"), add_bos=True)[1:]\n",
    "\n",
    "    print(\"Secret tokens :\", len(secret_token_ids))\n",
    "    print(\"Stego tokens  :\", len(stego_token_ids))\n",
    "    print(\"len(ranks_e)  :\", len(ranks_e))\n",
    "    print()\n",
    "\n",
    "    # Sanity checks\n",
    "    assert len(secret_token_ids) == len(ranks_e), \"Token count for e does not match len(ranks_e)\"\n",
    "    assert len(stego_token_ids)  == len(ranks_e), \"Token count for s does not match len(ranks_e)\"\n",
    "\n",
    "    # Decode: s -> (ranks) -> e\n",
    "    recovered_text = reveal_text_token_level(\n",
    "        stegotext=stegotext,\n",
    "        secret_prefix=secret_prefix,\n",
    "        secret_key=secret_key,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    print(\"Recovered e:\")\n",
    "    print(recovered_text)\n",
    "    print(\"Recovered == secret_text:\", recovered_text == secret_text)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Secret text e:\n",
      "THE CURRENT SYSTEM HAS REPEATEDLY FAILED\n",
      "\n",
      "ranks_e (len = 9):\n",
      "[164, 639, 21, 10, 10, 17, 1, 1, 1]\n",
      "\n",
      "Stegotext s:\n",
      "Get sufficient roas tting time. The\n",
      "\n",
      "Secret tokens : 9\n",
      "Stego tokens  : 9\n",
      "len(ranks_e)  : 9\n",
      "\n",
      "Recovered e:\n",
      "THE CURRENT SYSTEM HAS REPEATEDLY FAILED\n",
      "Recovered == secret_text: True\n",
      "------------------------------------------------------------\n",
      "================================================================================\n",
      "Secret text e:\n",
      "The cats like to meow all the time. It is annoying.\n",
      "\n",
      "ranks_e (len = 14):\n",
      "[89803, 14969, 58, 1, 106, 1, 26, 1, 1, 1, 6, 2, 6, 1]\n",
      "\n",
      "Stegotext s:\n",
      "önemlidir Ringvorstellung Schriftgutachten. They have sharp claws\n",
      "\n",
      "Secret tokens : 14\n",
      "Stego tokens  : 14\n",
      "len(ranks_e)  : 14\n",
      "\n",
      "Recovered e:\n",
      "The cats like to meow all the time. It is annoying.\n",
      "Recovered == secret_text: True\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "secret_text_1  = \"THE CURRENT SYSTEM HAS REPEATEDLY FAILED\"\n",
    "secret_prefix_1 = \"A text:\"   # k'\n",
    "secret_key_1    = \"Here it is: the infamous British roasted boar with mint sauce. How to make it perfect.\"\n",
    "\n",
    "run_example(secret_text_1, secret_prefix_1, secret_key_1, model=llm)\n",
    "\n",
    "# Example 2\n",
    "secret_text_2  = \"The cats like to meow all the time. It is annoying.\"\n",
    "secret_prefix_2 = \"\"  # k'\n",
    "secret_key_2    = \"The cat is a feline member just like lions and tigers but much smaller.\"  # k\n",
    "\n",
    "run_example(secret_text_2, secret_prefix_2, secret_key_2, model=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# goal context\n",
    "\n",
    "conduct research on the 'sensitivity of the key in respect to the stenography text produced, the stego text'. For instance if I have a key eg, 'I like cats' and then another 'I like kittens' to calculate the distance between the keys but then to find the equivalent distance between the stego text to see how much different is the keys are and the mapping to the distance between the stego texts. To do this for a few examples to see how that can work out. so effectively some simple examples of the (d_k, d_s| e) for the same message 'e' and some key and produce the stego text 's' and the distance 'd_k' and 'd_s'.\n",
    "\n",
    "studying here is the map\n",
    "\n",
    "k -> s(k;e)\n",
    "\n",
    "fixed hidden message e, e: how much does changing the key k change the stegotext s? we want empirical pairs\n",
    "\n",
    "(dk(k1,k2), ds(s1,s2)∣e), with  si=s(ki;e)\n",
    "\n",
    "\n",
    "## Distances for keys and stegotexts\n",
    "\n",
    "1. Character level edit distance, Normalized Levenshtein:\n",
    "\n",
    "d_k^{char}(k1,k2) = edit_distance(k1,k2) / max⁡(∣k1∣,∣k2∣,1)\n",
    "\n",
    "says how much you had to literally edit the string.\n",
    "\n",
    "(%pip install python-Levenshtein)\n",
    "\n",
    "2. Token level distance using the same tokenizer as the LLM. Since the protocol is token based, it is natural to look at key distance in token space.\n",
    "\n",
    "Let key_token_ids(k) be the tokenization you already use for prompts (via _make_prefix_ids, but without the BOS heuristic). For two keys with token sequences of possibly different lengths we can use a normalized edit distance in token space.\n",
    "\n",
    "3. Embedding distance\n",
    "\n",
    "In a sentence embedding model (for example a small sentence transformer) we can also measure cosine distance between embeddings of keys:\n",
    "\n",
    "dkemb(k1,k2)=1−cos⁡(emb(k1),emb(k2))\n",
    "\n",
    "says you how far apart the prompts are semantically, not just lexically.\n",
    "\n",
    "4. Token level Hamming distance under the Llama tokenizer.\n",
    "\n",
    "For fixed e, all stegotexts have exactly the same number of tokens (always use the same rank sequence), very clean:\n",
    "\n",
    "dstok(s1,s2)= 1/n \\sum_i^n  \\delta[t_i^(1) \\neq t_i^(2)]\n",
    "\n",
    "where t_i^(j) is the i-th token of stegotext sj\tin the Llama tokenizer and n is the common length. This is a per position token mismatch rate.\n",
    "\n",
    "\n",
    "# plan\n",
    "\n",
    "scatter of key distance vs stego distance\n",
    "\n",
    "Fix a secret text e (something like 10-ish words).\n",
    "Fix a prefix k' (or \"\").\n",
    "Generate many keys of similar length (for example, 5 words).\n",
    "For many key pairs (k1, k2):\n",
    "compute d_k (Levenshtein between keys),\n",
    "compute d_s (Levenshtein between corresponding stegotexts).\n",
    "\n",
    "Plot d_k on the x axis, d_s on the y axis, and save to results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def levenshtein_raw(a: str, b: str) -> int:\n",
    "    \"\"\"\n",
    "    Raw Levenshtein edit distance between two strings.\n",
    "    \"\"\"\n",
    "    return Levenshtein.distance(a, b)\n",
    "\n",
    "\n",
    "def levenshtein_normalized(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Normalized Levenshtein distance in [0, 1], using max length\n",
    "    as the normalization factor.\n",
    "\n",
    "    0.0 means identical strings, values closer to 1.0 mean more different.\n",
    "    \"\"\"\n",
    "    raw_distance = Levenshtein.distance(a, b)\n",
    "    maximum_length = max(len(a), len(b))\n",
    "    if maximum_length == 0:\n",
    "        return 0.0\n",
    "    return raw_distance / maximum_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_key_and_stego_distances(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    secret_key_one: str,\n",
    "    secret_key_two: str,\n",
    "    model: Llama = llm,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    For a fixed secret message e and prefix k',\n",
    "    generate stegotexts for two keys and compute:\n",
    "\n",
    "      - Levenshtein distance between the keys\n",
    "      - Levenshtein distance between the stegotexts\n",
    "\n",
    "    Returns a dictionary with:\n",
    "      - secret_key_one, secret_key_two\n",
    "      - stegotext_one, stegotext_two\n",
    "      - d_k_raw, d_k_norm\n",
    "      - d_s_raw, d_s_norm\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate stegotext for key 1\n",
    "    stegotext_one, _ = hide_text_token_level(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        secret_key=secret_key_one,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # Generate stegotext for key 2\n",
    "    stegotext_two, _ = hide_text_token_level(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        secret_key=secret_key_two,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # Distances between keys\n",
    "    d_k_raw = levenshtein_raw(secret_key_one, secret_key_two)\n",
    "    d_k_norm = levenshtein_normalized(secret_key_one, secret_key_two)\n",
    "\n",
    "    # Distances between stegotexts\n",
    "    d_s_raw = levenshtein_raw(stegotext_one, stegotext_two)\n",
    "    d_s_norm = levenshtein_normalized(stegotext_one, stegotext_two)\n",
    "\n",
    "    return {\n",
    "        \"secret_key_one\": secret_key_one,\n",
    "        \"secret_key_two\": secret_key_two,\n",
    "        \"stegotext_one\": stegotext_one,\n",
    "        \"stegotext_two\": stegotext_two,\n",
    "        \"d_k_raw\": d_k_raw,\n",
    "        \"d_k_norm\": d_k_norm,\n",
    "        \"d_s_raw\": d_s_raw,\n",
    "        \"d_s_norm\": d_s_norm,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret_key_one: I like cats\n",
      "secret_key_two: I like kittens\n",
      "stegotext_one: asticsearch slack-github**\n",
      "\n",
      "I need to find the common interests that are\n",
      "stegotext_two: uyếnTek và các chuyến bay an toàn. I have been following your\n",
      "d_k_raw: 5\n",
      "d_k_norm: 0.35714285714285715\n",
      "d_s_raw: 58\n",
      "d_s_norm: 0.8055555555555556\n"
     ]
    }
   ],
   "source": [
    "secret_text = \"The cats like to meow all the time. It is annoying.\"\n",
    "secret_prefix = \"\"  # or \"A text:\" if you want to use a prefix\n",
    "\n",
    "secret_key_one = \"I like cats\"\n",
    "secret_key_two = \"I like kittens\"\n",
    "\n",
    "result = compute_key_and_stego_distances(\n",
    "    secret_text=secret_text,\n",
    "    secret_prefix=secret_prefix,\n",
    "    secret_key_one=secret_key_one,\n",
    "    secret_key_two=secret_key_two,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precompute_ranks_for_secret(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    model: Llama = llm,\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Compute the rank sequence for a fixed secret text e and prefix k'\n",
    "    once, to be reused for many different keys k.\n",
    "    \"\"\"\n",
    "    ranks = get_token_ranks_like_paper(\n",
    "        text=secret_text,\n",
    "        model=model,\n",
    "        prefix=secret_prefix,\n",
    "    )\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def generate_stegotext_from_ranks(\n",
    "    ranks: List[int],\n",
    "    secret_key: str,\n",
    "    model: Llama = llm,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Given a rank sequence and a key k, generate the corresponding\n",
    "    stegotext s(k; e) by following those ranks under prompt=k.\n",
    "    \"\"\"\n",
    "    stegotext = decode_from_ranks_like_paper(\n",
    "        prompt=secret_key,\n",
    "        ranks=ranks,\n",
    "        model=model,\n",
    "    )\n",
    "    return stegotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_KEY_VOCABULARY = [\n",
    "    \"cats\", \"kittens\", \"dogs\", \"puppies\",\n",
    "    \"music\", \"books\", \"coffee\", \"travel\",\n",
    "    \"coding\", \"movies\", \"reading\", \"walking\",\n",
    "    \"running\", \"summer\", \"winter\", \"sunny\",\n",
    "    \"rainy\", \"happy\", \"sad\", \"quiet\",\n",
    "]\n",
    "\n",
    "\n",
    "def generate_random_keys(\n",
    "    number_of_keys: int,\n",
    "    number_of_words: int,\n",
    "    random_seed: int = 0,\n",
    "    vocabulary: Sequence[str] = DEFAULT_KEY_VOCABULARY,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate simple natural-language-like keys, each with the same\n",
    "    number of words (approx same length). For example:\n",
    "\n",
    "        \"Cats love quiet music.\"\n",
    "    \"\"\"\n",
    "    random_generator = random.Random(random_seed)\n",
    "    keys: List[str] = []\n",
    "\n",
    "    for _ in range(number_of_keys):\n",
    "        words = [random_generator.choice(vocabulary) for _ in range(number_of_words)]\n",
    "        sentence = \" \".join(words).capitalize() + \".\"\n",
    "        keys.append(sentence)\n",
    "\n",
    "    return keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stegotexts_for_keys(\n",
    "    ranks_for_secret: List[int],\n",
    "    keys: Sequence[str],\n",
    "    model: Llama = llm,\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    For a fixed secret (encoded by ranks_for_secret), generate stegotext\n",
    "    for each key.\n",
    "    \"\"\"\n",
    "    stegotext_by_key: Dict[str, str] = {}\n",
    "    for key in keys:\n",
    "        stegotext_by_key[key] = generate_stegotext_from_ranks(\n",
    "            ranks=ranks_for_secret,\n",
    "            secret_key=key,\n",
    "            model=model,\n",
    "        )\n",
    "    return stegotext_by_key\n",
    "\n",
    "\n",
    "def compute_pairwise_key_and_stego_distances(\n",
    "    keys: Sequence[str],\n",
    "    stegotext_by_key: Dict[str, str],\n",
    ") -> List[Dict[str, Any]]:\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for key_one, key_two in combinations(keys, 2):\n",
    "        stego_one = stegotext_by_key[key_one]\n",
    "        stego_two = stegotext_by_key[key_two]\n",
    "\n",
    "        # Distances between keys\n",
    "        d_k_raw = Levenshtein.distance(key_one, key_two)\n",
    "        max_key_length = max(len(key_one), len(key_two), 1)\n",
    "        d_k_norm = d_k_raw / max_key_length\n",
    "\n",
    "        # Distances between stegotexts\n",
    "        d_s_raw = Levenshtein.distance(stego_one, stego_two)\n",
    "        max_stego_length = max(len(stego_one), len(stego_two), 1)\n",
    "        d_s_norm = d_s_raw / max_stego_length\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"key_one\": key_one,\n",
    "                \"key_two\": key_two,\n",
    "                \"stego_one\": stego_one,\n",
    "                \"stego_two\": stego_two,\n",
    "                \"d_k_raw\": d_k_raw,\n",
    "                \"d_k_norm\": d_k_norm,\n",
    "                \"d_s_raw\": d_s_raw,\n",
    "                \"d_s_norm\": d_s_norm,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_key_vs_stego_levenshtein(\n",
    "    pairwise_records: Sequence[Dict[str, Any]],\n",
    "    use_normalized: bool = True,\n",
    "    output_directory: Path = REPO_ROOT / \"results\",\n",
    "    output_filename: str = \"key_vs_stego_levenshtein_scatter.png\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Create a scatter plot with key distance on the x axis and\n",
    "    stegotext distance on the y axis. Save it to output_directory\n",
    "    and return the path.\n",
    "    \"\"\"\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if use_normalized:\n",
    "        x_values = [record[\"d_k_norm\"] for record in pairwise_records]\n",
    "        y_values = [record[\"d_s_norm\"] for record in pairwise_records]\n",
    "        x_label = \"Key Levenshtein distance (normalized)\"\n",
    "        y_label = \"Stegotext Levenshtein distance (normalized)\"\n",
    "    else:\n",
    "        x_values = [record[\"d_k_raw\"] for record in pairwise_records]\n",
    "        y_values = [record[\"d_s_raw\"] for record in pairwise_records]\n",
    "        x_label = \"Key Levenshtein distance (raw)\"\n",
    "        y_label = \"Stegotext Levenshtein distance (raw)\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(x_values, y_values, alpha=0.7)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(\"Sensitivity of stegotext to key (Levenshtein distance)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    output_path = output_directory / output_filename\n",
    "    plt.savefig(output_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved scatter plot to: {output_path}\")\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "key_one: Puppies cats coding travel travel.\n",
      "key_two: Music puppies happy dogs sad.\n",
      "d_k_norm: 0.7941176470588235\n",
      "d_s_norm: 0.9074074074074074\n",
      "===\n",
      "key_one: Puppies cats coding travel travel.\n",
      "key_two: Summer kittens cats dogs coffee.\n",
      "d_k_norm: 0.7647058823529411\n",
      "d_s_norm: 0.8703703703703703\n",
      "===\n",
      "key_one: Puppies cats coding travel travel.\n",
      "key_two: Travel rainy quiet cats happy.\n",
      "d_k_norm: 0.7352941176470589\n",
      "d_s_norm: 0.9464285714285714\n",
      "Saved scatter plot to: /home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_levenshtein_scatter.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_levenshtein_scatter.png')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose secret text and prefix (k')\n",
    "secret_text = \"Cats like to meow all the time, it is annoying.\"\n",
    "secret_prefix = \"\"  # or \"A text:\" if you want to condition e\n",
    "\n",
    "# Precompute ranks for the secret once\n",
    "ranks_for_secret = precompute_ranks_for_secret(\n",
    "    secret_text=secret_text,\n",
    "    secret_prefix=secret_prefix,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# Generate a bunch of keys of equal word length\n",
    "number_of_keys = 30\n",
    "number_of_words_per_key = 5\n",
    "\n",
    "keys = generate_random_keys(\n",
    "    number_of_keys=number_of_keys,\n",
    "    number_of_words=number_of_words_per_key,\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "# Stegotexts for each key\n",
    "stegotext_by_key = generate_stegotexts_for_keys(\n",
    "    ranks_for_secret=ranks_for_secret,\n",
    "    keys=keys,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# All pairwise distances (d_k, d_s | e)\n",
    "pairwise_records = compute_pairwise_key_and_stego_distances(\n",
    "    keys=keys,\n",
    "    stegotext_by_key=stegotext_by_key,\n",
    ")\n",
    "\n",
    "# inspect a couple of records\n",
    "for record in pairwise_records[:3]:\n",
    "    print(\"===\")\n",
    "    print(\"key_one:\", record[\"key_one\"])\n",
    "    print(\"key_two:\", record[\"key_two\"])\n",
    "    print(\"d_k_norm:\", record[\"d_k_norm\"])\n",
    "    print(\"d_s_norm:\", record[\"d_s_norm\"])\n",
    "\n",
    "# Plot and save to ../results/\n",
    "plot_key_vs_stego_levenshtein(\n",
    "    pairwise_records=pairwise_records,\n",
    "    use_normalized=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import Levenshtein\n",
    "\n",
    "\n",
    "def make_key_with_substitution_edits(\n",
    "    base_key: str,\n",
    "    number_of_edits: int,\n",
    "    random_seed: int = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a variant of base_key by applying exactly `number_of_edits`\n",
    "    character substitutions (no insertions or deletions).\n",
    "    This ensures the raw Levenshtein distance is exactly `number_of_edits`\n",
    "    as long as we only perform substitutions.\n",
    "\n",
    "    We only edit alphabetic characters to keep the key readable and\n",
    "    leave spaces and punctuation intact.\n",
    "    \"\"\"\n",
    "    if number_of_edits <= 0:\n",
    "        return base_key\n",
    "\n",
    "    random_generator = random.Random(random_seed)\n",
    "\n",
    "    characters = list(base_key)\n",
    "    editable_positions = [index for index, character in enumerate(characters) if character.isalpha()]\n",
    "\n",
    "    if not editable_positions:\n",
    "        # Fallback: nothing alphabetic to edit\n",
    "        return base_key\n",
    "\n",
    "    # Clamp edits to available positions\n",
    "    edits_to_apply = min(number_of_edits, len(editable_positions))\n",
    "\n",
    "    positions_to_edit = random_generator.sample(editable_positions, edits_to_apply)\n",
    "\n",
    "    alphabet = string.ascii_letters\n",
    "\n",
    "    for index in positions_to_edit:\n",
    "        original_character = characters[index]\n",
    "        possible_replacements = [ch for ch in alphabet if ch != original_character]\n",
    "        characters[index] = random_generator.choice(possible_replacements)\n",
    "\n",
    "    mutated_key = \"\".join(characters)\n",
    "\n",
    "    # Optional assertion: Levenshtein distance should match the number of edits we applied\n",
    "    raw_distance = Levenshtein.distance(base_key, mutated_key)\n",
    "    assert raw_distance == edits_to_apply, f\"Expected distance {edits_to_apply}, got {raw_distance}\"\n",
    "\n",
    "    return mutated_key\n",
    "\n",
    "\n",
    "from typing import List, Dict, Any, Sequence\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def key_distance_sweep_against_base(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    base_key: str,\n",
    "    edit_counts: Sequence[int],\n",
    "    samples_per_edit: int,\n",
    "    model: Llama = llm,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For a fixed secret text e, prefix k' and base key k_base, create variants of k_base\n",
    "    at various character-level edit distances and measure:\n",
    "\n",
    "      - d_k_raw, d_k_norm between k_base and k_variant\n",
    "      - d_s_raw, d_s_norm between their corresponding stegotexts\n",
    "\n",
    "    Returns a list of records, one per (edit_count, sample) pair.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Precompute ranks for the secret once\n",
    "    ranks_for_secret = precompute_ranks_for_secret(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # 2. Stegotext for the base key\n",
    "    stego_base = generate_stegotext_from_ranks(\n",
    "        ranks=ranks_for_secret,\n",
    "        secret_key=base_key,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for edit_count in edit_counts:\n",
    "        for sample_index in range(samples_per_edit):\n",
    "            mutated_key = make_key_with_substitution_edits(\n",
    "                base_key=base_key,\n",
    "                number_of_edits=edit_count,\n",
    "                random_seed=1000 + edit_count * 100 + sample_index,\n",
    "            )\n",
    "\n",
    "            stego_mutated = generate_stegotext_from_ranks(\n",
    "                ranks=ranks_for_secret,\n",
    "                secret_key=mutated_key,\n",
    "                model=model,\n",
    "            )\n",
    "\n",
    "            # Key distances\n",
    "            d_k_raw = Levenshtein.distance(base_key, mutated_key)\n",
    "            max_key_length = max(len(base_key), len(mutated_key), 1)\n",
    "            d_k_norm = d_k_raw / max_key_length\n",
    "\n",
    "            # Stegotext distances\n",
    "            d_s_raw = Levenshtein.distance(stego_base, stego_mutated)\n",
    "            max_stego_length = max(len(stego_base), len(stego_mutated), 1)\n",
    "            d_s_norm = d_s_raw / max_stego_length\n",
    "\n",
    "            records.append(\n",
    "                {\n",
    "                    \"base_key\": base_key,\n",
    "                    \"mutated_key\": mutated_key,\n",
    "                    \"edit_count\": edit_count,\n",
    "                    \"stego_base\": stego_base,\n",
    "                    \"stego_mutated\": stego_mutated,\n",
    "                    \"d_k_raw\": d_k_raw,\n",
    "                    \"d_k_norm\": d_k_norm,\n",
    "                    \"d_s_raw\": d_s_raw,\n",
    "                    \"d_s_norm\": d_s_norm,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def build_uniform_edit_counts(base_key: str, number_of_levels: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Build a set of edit_counts that give (approximately) uniformly spaced\n",
    "    normalized distances d_k_norm in [0, 1], relative to the number of\n",
    "    alphabetic characters in base_key.\n",
    "    \"\"\"\n",
    "    number_of_alphabetic_characters = sum(character.isalpha() for character in base_key)\n",
    "    if number_of_alphabetic_characters == 0:\n",
    "        return [0]\n",
    "\n",
    "    raw_counts: List[int] = []\n",
    "    for level_index in range(number_of_levels):\n",
    "        fraction = level_index / max(number_of_levels - 1, 1)\n",
    "        count = int(round(fraction * number_of_alphabetic_characters))\n",
    "        raw_counts.append(count)\n",
    "\n",
    "    # Remove duplicates and sort\n",
    "    edit_counts = sorted(set(raw_counts))\n",
    "    return edit_counts\n",
    "\n",
    "def plot_sweep_key_vs_stego_levenshtein(\n",
    "    records: Sequence[Dict[str, Any]],\n",
    "    output_directory: Path = REPO_ROOT / \"results\",\n",
    "    output_filename: str = \"key_vs_stego_levenshtein_sweep.png\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Scatter plot of d_k_norm vs d_s_norm for the base-vs-variant experiment.\n",
    "    \"\"\"\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    x_values = [record[\"d_k_norm\"] for record in records]\n",
    "    y_values = [record[\"d_s_norm\"] for record in records]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(x_values, y_values, alpha=0.7)\n",
    "    plt.xlabel(\"Key Levenshtein distance (normalized, base vs variant)\")\n",
    "    plt.ylabel(\"Stegotext Levenshtein distance (normalized)\")\n",
    "    plt.title(\"Sensitivity of stegotext to key edits (base key sweep)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    output_path = output_directory / output_filename\n",
    "    plt.savefig(output_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved sweep scatter plot to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sweep scatter plot to: /home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_levenshtein_sweep.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_levenshtein_sweep.png')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secret_text = \"The cats like to meow all the time. It is annoying.\"\n",
    "secret_prefix = \"\"  # or \"A text:\" if you prefer\n",
    "base_key = \"Puppies cats coding travel travel.\"\n",
    "\n",
    "# Build many edit levels, roughly uniformly spanning [0, 1] in d_k_norm\n",
    "edit_counts = build_uniform_edit_counts(base_key=base_key, number_of_levels=30)\n",
    "\n",
    "samples_per_edit = 5  # same as before\n",
    "\n",
    "sweep_records = key_distance_sweep_against_base(\n",
    "    secret_text=secret_text,\n",
    "    secret_prefix=secret_prefix,\n",
    "    base_key=base_key,\n",
    "    edit_counts=edit_counts,\n",
    "    samples_per_edit=samples_per_edit,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "plot_sweep_key_vs_stego_levenshtein(sweep_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " new plot *does* show a very strong \"avalanche like\" sensitivity, but it is not literally a cryptographic hash. It is more like:\n",
    "\n",
    "> For almost any nonzero character level change to the key, the resulting stegotext is almost as different as it can be (by Levenshtein), given the fixed length and language constraints.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1.  plot is really showing\n",
    "\n",
    "- fix:\n",
    "  - secret text `e`\n",
    "  - prefix `k'`\n",
    "  - base key `k_base`\n",
    "- For each mutated key `k_variant` :\n",
    "  - change some number of characters in `k_base` (no insertions, only substitutions),\n",
    "  - generate stegotext `s_base = s(k_base; e)`,\n",
    "  - generate `s_variant = s(k_variant; e)`,\n",
    "  - compute `d_k_norm(k_base, k_variant)` and `d_s_norm(s_base, s_variant)`.\n",
    "\n",
    "The plot shows:\n",
    "\n",
    "- one point at `(0, 0)` (same key -> same stegotext), and  \n",
    "- for any nonzero `d_k_norm`, almost all `d_s_norm` values are in roughly `[0.8, 0.95]`.\n",
    "\n",
    "\n",
    "> Once the key differs at all in characters, the stegotext looks almost maximally different at the character level.\n",
    "\n",
    "That is exactly the flat band\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why a tiny change in the key can produce a huge change in the stegotext\n",
    "\n",
    "Two effects combine here.\n",
    "\n",
    "### 2.1 Character edits are huge from the model's perspective\n",
    "\n",
    "Your `make_key_with_substitution_edits` changes individual characters inside words. To Levenshtein this is a tiny change. But to the Llama tokenizer and model it can be catastrophic:\n",
    "\n",
    "- `\"coding\"` -> `\"coxing\"` or `\"codxng\"` often changes the tokenization completely.\n",
    "- Many mutated words become out of distribution or extremely rare subword tokens.\n",
    "\n",
    "So from the LLM's point of view, even one or two character substitutions often mean:\n",
    "\n",
    "- \"clean normal prompt\" vs \"weird noisy prompt\".\n",
    "\n",
    "The model's internal representation of the context (and therefore its probability ordering over the next token) can change dramatically, even though Levenshtein says \"distance 1 or 2\".\n",
    "\n",
    "sweep is really probing something like:\n",
    "\n",
    "> \"How different are stegotexts when I move the key from a natural English sentence to various corrupted versions of that sentence?\"\n",
    "\n",
    "Given the paper's protocol, the key `k` only enters through the conditional distribution `p(. | k, s_<i>)`. If the context embedding changes a lot, the sorted rank order of tokens at each step changes a lot, and with a fixed rank sequence `r_i` you get almost entirely different tokens. :contentReference[oaicite:0]{index=0}  \n",
    "\n",
    "### 2.2 Fixed rank sequence + different contexts is like random relabeling\n",
    "\n",
    "For each token position `i`, the protocol does:\n",
    "\n",
    "- For key `k_1`: choose the token at rank `r_i` under `p(. | k_1, s^{(1)}_<i>)`.\n",
    "- For key `k_2`: choose the token at the same rank `r_i` but under `p(. | k_2, s^{(2)}_<i>)`.\n",
    "\n",
    "If you roughly model \"sorted vocab by probability under context 1\" and \"sorted vocab under context 2\" as two different permutations of the vocabulary, then \"rank `r_i` under context 1\" and \"rank `r_i` under context 2\" will almost never be the same token. That means at each position:\n",
    "\n",
    "- Probability that the two stegotexts share the same token is very small.\n",
    "- So the expected fraction of matching tokens is tiny, and the normalized Levenshtein distance is close to 1.\n",
    "\n",
    "Because your stegotexts are of moderate length and use natural language tokens see roughly `0.8-0.95` rather than literally `1.0`, but it is clearly very high.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Is this \"like a hash function\"?\n",
    "\n",
    "In spirit, yes in one important sense, but no in several others.\n",
    "\n",
    "### 3.1 How it is similar to a hash\n",
    "\n",
    "seeing an avalanche like effect:\n",
    "\n",
    "> Any nonzero small character change to `k` almost always produces a stegotext that is \"maximally scrambled\" compared to `s(k_base; e)` under your distance measure.\n",
    "\n",
    "This is exactly the kind of behavior we qualitatively expect from a good hash: output looks essentially unrelated even for tiny input changes.\n",
    "\n",
    "Given the protocol in the paper, that is not surprising: the key's job is to set the entire probability landscape from which we pick tokens by pre fixed ranks. A slightly different landscape generally yields a completely different path. :contentReference[oaicite:1]{index=1}  \n",
    "\n",
    "So, for character level perturbations of this type, our empirical plots are telling us:\n",
    "\n",
    "> The map `k -> s(k; e)` behaves almost like a chaotic function: once you depart from exactly the same key, the resulting stegotexts are very far apart.\n",
    "\n",
    "### 3.2 How it differs from a cryptographic hash\n",
    "\n",
    "However, it is not a cryptographic hash:\n",
    "\n",
    "1. **Not designed for uniformity or bit level independence**\n",
    "\n",
    "   In a hash, changing one input bit flips each output bit with probability 0.5 independently. Here:\n",
    "\n",
    "   - Output is constrained to be natural language.\n",
    "   - There are correlations between tokens due to grammar, semantics, and the fixed rank sequence.\n",
    "   - Distances saturate around `0.8-0.95`, not `1.0`, and in token space there may be more structure than Levenshtein exposes.\n",
    "\n",
    "2. **Metric mismatch**\n",
    "\n",
    "   - You measure distance between keys by character Levenshtein.\n",
    "   - The model \"feels\" keys in token or embedding space. Two keys that are very close in Levenshtein can be very far to the model (your current experiment), and two keys that are quite far in Levenshtein but semantically similar (word level edits) might have much more similar stegotexts.\n",
    "   - So the apparent avalanche is partly an artifact of using a metric that damages words, not just changes their semantics.\n",
    "\n",
    "3. **No formal collision resistance or preimage resistance**\n",
    "\n",
    "   - Many different keys will lead to broadly similar stegotexts, and the protocol is not designed to minimize such collisions.\n",
    "   - An attacker who knows `e`, `k'`, and the model can trivially generate infinitely many different keys that produce stegotexts for the same `e` (just change `k`).\n",
    "   - Security in the paper relies on the secrecy of the key and the need to match both the model and the key, not on hash like one wayness. :contentReference[oaicite:2]{index=2}  \n",
    "\n",
    "4. **Local versus global behavior**\n",
    "\n",
    "   - A hash is equally scrambling everywhere in its input space.\n",
    "   - Here, behavior might depend on the region of prompt space you are in. Your current experiment mutates a single base key with fairly aggressive character noise; if you instead moved between semantically close, clean prompts (word substitutions, added detail, style tweaks), you might see more structure and less perfect scrambling.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. How to sharpen this picture with further experiments\n",
    "\n",
    "If we want to make the \"hash like\" statement more precise, there are a few natural next experiments:\n",
    "\n",
    "1. **Word level edits instead of character noise**\n",
    "\n",
    "   - Replace the base key's words with other words from your vocabulary (keeping grammar and structure intact).\n",
    "   - Measure `(d_k, d_s)` again.\n",
    "   - If stegotext distances are still high even when keys remain grammatical and similar in meaning, that is stronger evidence of intrinsic sensitivity rather than just \"the model hates corrupted strings\".\n",
    "\n",
    "2. **Token level Hamming distance**\n",
    "\n",
    "   - Compute the fraction of token positions where two stegotexts disagree.\n",
    "   - Compare that to character level Levenshtein. Your `0.8-0.95` may simply reflect morphological and subword similarities.\n",
    "\n",
    "3. **Compare local vs global**\n",
    "\n",
    "   - Put the base key sweep points and the random key pair points on the same scatter (different colors).\n",
    "   - If both clouds sit in the same high `d_s` band, that supports the \"almost any difference in key -> huge difference in stegotext\" story.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Bottom line\n",
    "\n",
    "- Our latest plot does show that, under your current notion of \"small change in key\" (character substitutions), the mapping `k -> s(k; e)` is extremely sensitive: once `d_k > 0`, `d_s` is already very large and does not grow much further.\n",
    "- This is qualitatively hash like in the avalanche sense: tiny key changes yield almost maximally different stegotexts.\n",
    "- But it is not a cryptographic hash: the output space is constrained, the metric is not bit level, and there is no formal security guarantee or uniformity.\n",
    "\n",
    "Conceptually, we can say:\n",
    "\n",
    "> For this protocol and this model, the stegotext behaves approximately like a chaotic function of the key - more like a hash than like a smooth function - especially when you look at character level perturbations.\n",
    "\n",
    "If we repeat this with word level or prompt style variations, you will get a more nuanced picture of how \"hashy\" it really is when the key changes are ones the model perceives as small rather than corrupted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import Levenshtein\n",
    "\n",
    "\n",
    "def make_key_with_word_replacements(\n",
    "    base_key: str,\n",
    "    number_of_word_replacements: int,\n",
    "    replacement_vocabulary: list[str],\n",
    "    random_seed: int | None = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Replace `number_of_word_replacements` word tokens in base_key with words\n",
    "    drawn from replacement_vocabulary.\n",
    "\n",
    "    This keeps the key grammatical and avoids introducing corrupted tokens,\n",
    "    so the model is more likely to see these changes as \"small\" than raw\n",
    "    character noise.\n",
    "    \"\"\"\n",
    "    if number_of_word_replacements <= 0:\n",
    "        return base_key\n",
    "\n",
    "    random_generator = random.Random(random_seed)\n",
    "\n",
    "    tokens = base_key.split()\n",
    "\n",
    "    # Only consider tokens that contain at least one alphabetic character\n",
    "    editable_positions = [\n",
    "        index\n",
    "        for index, token in enumerate(tokens)\n",
    "        if any(character.isalpha() for character in token)\n",
    "    ]\n",
    "\n",
    "    if not editable_positions:\n",
    "        return base_key\n",
    "\n",
    "    replacements_to_apply = min(number_of_word_replacements, len(editable_positions))\n",
    "    positions_to_replace = random_generator.sample(\n",
    "        editable_positions,\n",
    "        replacements_to_apply,\n",
    "    )\n",
    "\n",
    "    mutated_tokens = list(tokens)\n",
    "\n",
    "    for index in positions_to_replace:\n",
    "        original_token = mutated_tokens[index]\n",
    "\n",
    "        # Separate core word from trailing punctuation, e.g. \"cats.\" -> (\"cats\", \".\")\n",
    "        match = re.match(r\"^([A-Za-z']+)([^A-Za-z']*)$\", original_token)\n",
    "        if match is None:\n",
    "            # If we cannot parse it nicely, just skip this token\n",
    "            continue\n",
    "\n",
    "        original_word = match.group(1)\n",
    "        trailing_punctuation = match.group(2)\n",
    "\n",
    "        # Choose a replacement word distinct from the original (case insensitive)\n",
    "        candidate_words = [\n",
    "            word for word in replacement_vocabulary\n",
    "            if word.lower() != original_word.lower()\n",
    "        ]\n",
    "        if not candidate_words:\n",
    "            continue\n",
    "\n",
    "        replacement_word = random_generator.choice(candidate_words)\n",
    "\n",
    "        # Preserve capitalization pattern of the original word\n",
    "        if original_word.istitle():\n",
    "            replacement_word = replacement_word.capitalize()\n",
    "        elif original_word.isupper():\n",
    "            replacement_word = replacement_word.upper()\n",
    "\n",
    "        mutated_tokens[index] = replacement_word + trailing_punctuation\n",
    "\n",
    "    mutated_key = \" \".join(mutated_tokens)\n",
    "    return mutated_key\n",
    "\n",
    "\n",
    "REPLACEMENT_VOCABULARY = [\n",
    "    \"cats\", \"kittens\", \"dogs\", \"puppies\",\n",
    "    \"music\", \"books\", \"coffee\", \"tea\",\n",
    "    \"travel\", \"coding\", \"movies\", \"reading\",\n",
    "    \"walking\", \"running\", \"summer\", \"winter\",\n",
    "    \"sunny\", \"rainy\", \"happy\", \"quiet\",\n",
    "    \"busy\", \"calm\", \"evening\", \"morning\",\n",
    "]\n",
    "\n",
    "STYLE_PREFIXES = [\n",
    "    \"\",\n",
    "    \"In my opinion,\",\n",
    "    \"Honestly,\",\n",
    "    \"From my perspective,\",\n",
    "    \"To be honest,\",\n",
    "]\n",
    "\n",
    "STYLE_SUFFIXES = [\n",
    "    \"\",\n",
    "    \"and that is just how I see it.\",\n",
    "    \"most of the time.\",\n",
    "    \"when I have some free time.\",\n",
    "    \"especially on weekends.\",\n",
    "]\n",
    "\n",
    "\n",
    "def make_key_with_style_variation(\n",
    "    base_key: str,\n",
    "    random_seed: int | None = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a stylistic variant of base_key by adding a soft prefix and/or suffix.\n",
    "\n",
    "    This changes tone and length, but keeps the sentence clean and grammatical.\n",
    "    \"\"\"\n",
    "    random_generator = random.Random(random_seed)\n",
    "\n",
    "    prefix = random_generator.choice(STYLE_PREFIXES)\n",
    "    suffix = random_generator.choice(STYLE_SUFFIXES)\n",
    "\n",
    "    mutated_key = base_key\n",
    "    if prefix:\n",
    "        mutated_key = prefix + \" \" + mutated_key\n",
    "    if suffix:\n",
    "        mutated_key = mutated_key + \" \" + suffix\n",
    "\n",
    "    return mutated_key\n",
    "\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def build_uniform_word_edit_counts(base_key: str, number_of_levels: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Build a list of distinct word replacement counts that roughly span\n",
    "    from 0 to the maximum possible number of word replacements.\n",
    "    \"\"\"\n",
    "    tokens = base_key.split()\n",
    "    number_of_tokens = len(tokens)\n",
    "    if number_of_tokens == 0:\n",
    "        return [0]\n",
    "\n",
    "    raw_counts: List[int] = []\n",
    "    for level_index in range(number_of_levels):\n",
    "        fraction = level_index / max(number_of_levels - 1, 1)\n",
    "        count = int(round(fraction * number_of_tokens))\n",
    "        raw_counts.append(count)\n",
    "\n",
    "    edit_counts = sorted(set(raw_counts))\n",
    "    return edit_counts\n",
    "\n",
    "\n",
    "from typing import Dict, Any, Sequence\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def key_distance_sweep_word_level(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    base_key: str,\n",
    "    edit_counts: Sequence[int],\n",
    "    samples_per_edit: int,\n",
    "    replacement_vocabulary: list[str],\n",
    "    model: Llama = llm,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For a fixed secret text e, prefix k' and base key k_base, create variants of k_base\n",
    "    using WORD-LEVEL replacements and measure:\n",
    "\n",
    "      - d_k_raw, d_k_norm between k_base and k_variant\n",
    "      - d_s_raw, d_s_norm between their corresponding stegotexts\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Precompute ranks for the secret once\n",
    "    ranks_for_secret = precompute_ranks_for_secret(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # 2. Stegotext for the base key\n",
    "    stegotext_base = generate_stegotext_from_ranks(\n",
    "        ranks=ranks_for_secret,\n",
    "        secret_key=base_key,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for edit_count in edit_counts:\n",
    "        for sample_index in range(samples_per_edit):\n",
    "            mutated_key = make_key_with_word_replacements(\n",
    "                base_key=base_key,\n",
    "                number_of_word_replacements=edit_count,\n",
    "                replacement_vocabulary=replacement_vocabulary,\n",
    "                random_seed=2000 + edit_count * 100 + sample_index,\n",
    "            )\n",
    "\n",
    "            stegotext_mutated = generate_stegotext_from_ranks(\n",
    "                ranks=ranks_for_secret,\n",
    "                secret_key=mutated_key,\n",
    "                model=model,\n",
    "            )\n",
    "\n",
    "            # Key distances\n",
    "            d_k_raw = Levenshtein.distance(base_key, mutated_key)\n",
    "            max_key_length = max(len(base_key), len(mutated_key), 1)\n",
    "            d_k_norm = d_k_raw / max_key_length\n",
    "\n",
    "            # Stegotext distances\n",
    "            d_s_raw = Levenshtein.distance(stegotext_base, stegotext_mutated)\n",
    "            max_stego_length = max(len(stegotext_base), len(stegotext_mutated), 1)\n",
    "            d_s_norm = d_s_raw / max_stego_length\n",
    "\n",
    "            records.append(\n",
    "                {\n",
    "                    \"base_key\": base_key,\n",
    "                    \"mutated_key\": mutated_key,\n",
    "                    \"edit_count\": edit_count,\n",
    "                    \"stegotext_base\": stegotext_base,\n",
    "                    \"stegotext_mutated\": stegotext_mutated,\n",
    "                    \"d_k_raw\": d_k_raw,\n",
    "                    \"d_k_norm\": d_k_norm,\n",
    "                    \"d_s_raw\": d_s_raw,\n",
    "                    \"d_s_norm\": d_s_norm,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return records\n",
    "\n",
    "def key_distance_sweep_style_variations(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    base_key: str,\n",
    "    number_of_variants: int,\n",
    "    model: Llama = llm,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For a fixed secret text e, prefix k' and base key k_base, create stylistic\n",
    "    variants of k_base and measure key and stegotext distances.\n",
    "    \"\"\"\n",
    "\n",
    "    ranks_for_secret = precompute_ranks_for_secret(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    stegotext_base = generate_stegotext_from_ranks(\n",
    "        ranks=ranks_for_secret,\n",
    "        secret_key=base_key,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for variant_index in range(number_of_variants):\n",
    "        mutated_key = make_key_with_style_variation(\n",
    "            base_key=base_key,\n",
    "            random_seed=3000 + variant_index,\n",
    "        )\n",
    "\n",
    "        stegotext_mutated = generate_stegotext_from_ranks(\n",
    "            ranks=ranks_for_secret,\n",
    "            secret_key=mutated_key,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "        d_k_raw = Levenshtein.distance(base_key, mutated_key)\n",
    "        max_key_length = max(len(base_key), len(mutated_key), 1)\n",
    "        d_k_norm = d_k_raw / max_key_length\n",
    "\n",
    "        d_s_raw = Levenshtein.distance(stegotext_base, stegotext_mutated)\n",
    "        max_stego_length = max(len(stegotext_base), len(stegotext_mutated), 1)\n",
    "        d_s_norm = d_s_raw / max_stego_length\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"base_key\": base_key,\n",
    "                \"mutated_key\": mutated_key,\n",
    "                \"variant_index\": variant_index,\n",
    "                \"stegotext_base\": stegotext_base,\n",
    "                \"stegotext_mutated\": stegotext_mutated,\n",
    "                \"d_k_raw\": d_k_raw,\n",
    "                \"d_k_norm\": d_k_norm,\n",
    "                \"d_s_raw\": d_s_raw,\n",
    "                \"d_s_norm\": d_s_norm,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sweep scatter plot to: /home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_word_level_sweep.png\n",
      "Saved sweep scatter plot to: /home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_style_variation_sweep.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_style_variation_sweep.png')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secret_text = \"The cats like to meow all the time. It is annoying.\"\n",
    "secret_prefix = \"\"  # or \"A text:\" if you want a k' prefix\n",
    "base_key = \"Puppies cats coding travel travel.\"\n",
    "\n",
    "# Word-level sweep\n",
    "word_edit_counts = build_uniform_word_edit_counts(\n",
    "    base_key=base_key,\n",
    "    number_of_levels=10,\n",
    ")\n",
    "\n",
    "samples_per_edit = 5\n",
    "\n",
    "word_sweep_records = key_distance_sweep_word_level(\n",
    "    secret_text=secret_text,\n",
    "    secret_prefix=secret_prefix,\n",
    "    base_key=base_key,\n",
    "    edit_counts=word_edit_counts,\n",
    "    samples_per_edit=samples_per_edit,\n",
    "    replacement_vocabulary=REPLACEMENT_VOCABULARY,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "plot_sweep_key_vs_stego_levenshtein(\n",
    "    records=word_sweep_records,\n",
    "    output_filename=\"key_vs_stego_word_level_sweep.png\",\n",
    ")\n",
    "\n",
    "# Style-variation sweep\n",
    "style_sweep_records = key_distance_sweep_style_variations(\n",
    "    secret_text=secret_text,\n",
    "    secret_prefix=secret_prefix,\n",
    "    base_key=base_key,\n",
    "    number_of_variants=40,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "plot_sweep_key_vs_stego_levenshtein(\n",
    "    records=style_sweep_records,\n",
    "    output_filename=\"key_vs_stego_style_variation_sweep.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_sweep_records' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     correlation = np.corrcoef(key_distances, stego_distances)[\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m]\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Pearson correlation d_k_norm vs d_s_norm = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrelation\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m summarize_correlation(\u001b[43mword_sweep_records\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mWord-level sweep\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m summarize_correlation(style_sweep_records, \u001b[33m\"\u001b[39m\u001b[33mStyle-variation sweep\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'word_sweep_records' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def summarize_correlation(records, label: str):\n",
    "    key_distances = np.array([record[\"d_k_norm\"] for record in records])\n",
    "    stego_distances = np.array([record[\"d_s_norm\"] for record in records])\n",
    "    correlation = np.corrcoef(key_distances, stego_distances)[0, 1]\n",
    "    print(f\"{label}: Pearson correlation d_k_norm vs d_s_norm = {correlation:.3f}\")\n",
    "\n",
    "summarize_correlation(word_sweep_records, \"Word-level sweep\")\n",
    "summarize_correlation(style_sweep_records, \"Style-variation sweep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next experiment: sweeping over message length and making a heatmap\n",
    "\n",
    "idea:\n",
    "\n",
    "Add secret message length as another axis.\n",
    "\n",
    "For each length L (say 10, 20, 40, 80, 160, 320 words):\n",
    "pick one or more messages e of that length,\n",
    "generate stegotexts for many keys,\n",
    "\n",
    "compute (d_k_norm, d_s_norm) for all key pairs,\n",
    "\n",
    "Bin d_k_norm into 10 bins (0-0.1, 0.1-0.2, ...),\n",
    "\n",
    "For each (length, bin) cell, average d_s_norm and show it as a heatmap.\n",
    "That is absolutely a good idea. Conceptually you will be looking at\n",
    "\n",
    "E[ d_s∣length(e)=L, d_k \\in bin_j ]\n",
    "\n",
    "which tells whether to longer hidden texts make the stegotext mapping 'more hashy' (you should expect distances to saturate as length grows).\n",
    "\n",
    "Using random spans from a book is a great way to get realistic messages without having to handcraft a vocabulary of messages. As long as the text is reasonably in‑distribution for your model (normal English free form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Dict, List, Sequence\n",
    "\n",
    "\n",
    "def sample_secret_text_spans(\n",
    "    corpus_text: str,\n",
    "    target_word_lengths: Sequence[int],\n",
    "    samples_per_length: int,\n",
    "    random_seed: int = 0,\n",
    ") -> Dict[int, List[str]]:\n",
    "    \"\"\"\n",
    "    For each target length in words, sample `samples_per_length` contiguous spans\n",
    "    from `corpus_text` and return them as secret messages.\n",
    "\n",
    "    Returns a dictionary: length_in_words -> list of secret texts.\n",
    "    \"\"\"\n",
    "    random_generator = random.Random(random_seed)\n",
    "    corpus_words = corpus_text.split()\n",
    "    total_words = len(corpus_words)\n",
    "\n",
    "    secret_texts_by_length: Dict[int, List[str]] = {}\n",
    "\n",
    "    for target_length in target_word_lengths:\n",
    "        if target_length <= 0:\n",
    "            continue\n",
    "        if target_length >= total_words:\n",
    "            raise ValueError(\n",
    "                f\"Target length {target_length} too large for corpus of {total_words} words.\"\n",
    "            )\n",
    "\n",
    "        secret_texts: List[str] = []\n",
    "        for _ in range(samples_per_length):\n",
    "            start_index = random_generator.randint(0, total_words - target_length - 1)\n",
    "            end_index = start_index + target_length\n",
    "            span_words = corpus_words[start_index:end_index]\n",
    "            secret_texts.append(\" \".join(span_words))\n",
    "\n",
    "        secret_texts_by_length[target_length] = secret_texts\n",
    "\n",
    "    return secret_texts_by_length\n",
    "\n",
    "from typing import Any, List\n",
    "\n",
    "\n",
    "def collect_length_key_distance_records(\n",
    "    secret_texts_by_length: Dict[int, List[str]],\n",
    "    secret_prefix: str,\n",
    "    keys: Sequence[str],\n",
    "    model: Llama = llm,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For each length L and each secret_text of that length:\n",
    "      - compute ranks for secret_text under secret_prefix,\n",
    "      - generate stegotext for each key,\n",
    "      - compute all pairwise (d_k, d_s),\n",
    "    and attach the length information to each record.\n",
    "\n",
    "    Returns a flat list of records, each with d_k_norm, d_s_norm, and\n",
    "    a field 'secret_length_words'.\n",
    "    \"\"\"\n",
    "    all_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for length_in_words, secret_text_list in secret_texts_by_length.items():\n",
    "        print(f\"Processing length {length_in_words} words, {len(secret_text_list)} texts\")\n",
    "\n",
    "        for secret_text in secret_text_list:\n",
    "            ranks_for_secret = precompute_ranks_for_secret(\n",
    "                secret_text=secret_text,\n",
    "                secret_prefix=secret_prefix,\n",
    "                model=model,\n",
    "            )\n",
    "\n",
    "            stegotext_by_key = generate_stegotexts_for_keys(\n",
    "                ranks_for_secret=ranks_for_secret,\n",
    "                keys=keys,\n",
    "                model=model,\n",
    "            )\n",
    "\n",
    "            pairwise_records = compute_pairwise_key_and_stego_distances(\n",
    "                keys=keys,\n",
    "                stegotext_by_key=stegotext_by_key,\n",
    "            )\n",
    "\n",
    "            for record in pairwise_records:\n",
    "                record[\"secret_length_words\"] = length_in_words\n",
    "                all_records.append(record)\n",
    "\n",
    "    return all_records\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_heatmap_from_records(\n",
    "    all_records: Sequence[Dict[str, Any]],\n",
    "    secret_lengths: Sequence[int],\n",
    "    number_of_key_distance_bins: int = 10,\n",
    ") -> tuple[np.ndarray, np.ndarray, List[int]]:\n",
    "    \"\"\"\n",
    "    Build a matrix heatmap[length_index, bin_index] = mean d_s_norm,\n",
    "    where rows correspond to secret_lengths and columns to key-distance bins.\n",
    "\n",
    "    Returns:\n",
    "      - heatmap_values (shape [num_lengths, num_bins])\n",
    "      - bin_edges (length num_bins + 1)\n",
    "      - sorted_lengths (row order)\n",
    "    \"\"\"\n",
    "    sorted_lengths = sorted(set(secret_lengths))\n",
    "    length_index_map = {length: index for index, length in enumerate(sorted_lengths)}\n",
    "\n",
    "    bin_edges = np.linspace(0.0, 1.0, number_of_key_distance_bins + 1)\n",
    "\n",
    "    sum_matrix = np.zeros((len(sorted_lengths), number_of_key_distance_bins), dtype=float)\n",
    "    count_matrix = np.zeros((len(sorted_lengths), number_of_key_distance_bins), dtype=int)\n",
    "\n",
    "    for record in all_records:\n",
    "        length_in_words = record[\"secret_length_words\"]\n",
    "        if length_in_words not in length_index_map:\n",
    "            continue\n",
    "\n",
    "        key_distance = record[\"d_k_norm\"]\n",
    "        stego_distance = record[\"d_s_norm\"]\n",
    "\n",
    "        row_index = length_index_map[length_in_words]\n",
    "\n",
    "        # Bin index in [0, number_of_key_distance_bins - 1]\n",
    "        bin_index = min(\n",
    "            number_of_key_distance_bins - 1,\n",
    "            max(0, int(key_distance * number_of_key_distance_bins)),\n",
    "        )\n",
    "\n",
    "        sum_matrix[row_index, bin_index] += stego_distance\n",
    "        count_matrix[row_index, bin_index] += 1\n",
    "\n",
    "    # Compute mean, leaving empty bins as NaN\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        heatmap_values = np.where(\n",
    "            count_matrix > 0,\n",
    "            sum_matrix / np.maximum(count_matrix, 1),\n",
    "            np.nan,\n",
    "        )\n",
    "\n",
    "    return heatmap_values, bin_edges, sorted_lengths\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_length_vs_key_distance_heatmap(\n",
    "    heatmap_values: np.ndarray,\n",
    "    bin_edges: np.ndarray,\n",
    "    secret_lengths: Sequence[int],\n",
    "    output_directory: Path = REPO_ROOT / \"results\",\n",
    "    output_filename: str = \"length_vs_key_distance_heatmap.png\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Visualize a heatmap where:\n",
    "      - x axis: binned key Levenshtein distance (normalized)\n",
    "      - y axis: secret text length (words)\n",
    "      - color: mean stegotext distance (normalized)\n",
    "    \"\"\"\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    number_of_key_distance_bins = heatmap_values.shape[1]\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    image = plt.imshow(\n",
    "        heatmap_values,\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        vmin=0.60, #or 0.0\n",
    "        vmax=1.0,\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "\n",
    "    plt.colorbar(image, label=\"Mean stegotext distance (normalized)\")\n",
    "\n",
    "    plt.xlabel(\"Key Levenshtein distance (normalized, binned)\")\n",
    "    plt.ylabel(\"Secret text length (words)\")\n",
    "\n",
    "    plt.xticks(\n",
    "        ticks=range(number_of_key_distance_bins),\n",
    "        labels=[f\"{center:.2f}\" for center in bin_centers],\n",
    "        rotation=45,\n",
    "    )\n",
    "\n",
    "    plt.yticks(\n",
    "        ticks=range(len(secret_lengths)),\n",
    "        labels=[str(length_value) for length_value in secret_lengths],\n",
    "    )\n",
    "\n",
    "    plt.title(\"Stegotext distance vs key distance and secret length\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path = output_directory / output_filename\n",
    "    plt.savefig(output_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved heatmap to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def build_keys_for_heatmap(\n",
    "    base_key: str,\n",
    "    number_of_levels: int,\n",
    "    samples_per_level: int,\n",
    "    replacement_vocabulary: List[str],\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Create a set of keys for the heatmap by taking a base key and generating\n",
    "    word-level variants across a range of edit counts.\n",
    "\n",
    "    This gives you key pairs spanning from very small to quite large\n",
    "    normalized Levenshtein distances.\n",
    "    \"\"\"\n",
    "    edit_counts = build_uniform_word_edit_counts(\n",
    "        base_key=base_key,\n",
    "        number_of_levels=number_of_levels,\n",
    "    )\n",
    "\n",
    "    keys: List[str] = [base_key]\n",
    "\n",
    "    for edit_count in edit_counts:\n",
    "        if edit_count == 0:\n",
    "            continue  # base key already included\n",
    "\n",
    "        for sample_index in range(samples_per_level):\n",
    "            mutated_key = make_key_with_word_replacements(\n",
    "                base_key=base_key,\n",
    "                number_of_word_replacements=edit_count,\n",
    "                replacement_vocabulary=replacement_vocabulary,\n",
    "                random_seed=10_000 + edit_count * 100 + sample_index,\n",
    "            )\n",
    "            keys.append(mutated_key)\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    seen = set()\n",
    "    unique_keys: List[str] = []\n",
    "    for key in keys:\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_keys.append(key)\n",
    "\n",
    "    return unique_keys\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alice in wonderland from https://gist.github.com/phillipj/4944029\n",
    "\n",
    "with open(REPO_ROOT / \"data\" / \"corpus.txt\", \"r\", encoding=\"utf-8\") as file_handle:\n",
    "    corpus_text = file_handle.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing length 10 words, 3 texts\n",
      "Processing length 20 words, 3 texts\n",
      "Processing length 40 words, 3 texts\n",
      "Processing length 80 words, 3 texts\n",
      "Processing length 160 words, 3 texts\n",
      "Processing length 320 words, 3 texts\n",
      "Saved heatmap to: /home/meow/Documents/repos/LlmStenoExplore/results/length_vs_key_distance_heatmap.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/meow/Documents/repos/LlmStenoExplore/results/length_vs_key_distance_heatmap.png')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Secret lengths in words\n",
    "target_word_lengths = [10, 20, 40, 80, 160, 320]\n",
    "samples_per_length = 3  # number of spans per length\n",
    "\n",
    "secret_texts_by_length = sample_secret_text_spans(\n",
    "    corpus_text=corpus_text,\n",
    "    target_word_lengths=target_word_lengths,\n",
    "    samples_per_length=samples_per_length,\n",
    "    random_seed=123,\n",
    ")\n",
    "\n",
    "base_key = \"Puppies cats coding travel time.\"\n",
    "\n",
    "\n",
    "keys = build_keys_for_heatmap(\n",
    "    base_key=base_key,\n",
    "    number_of_levels=12,\n",
    "    samples_per_level=4,\n",
    "    replacement_vocabulary=REPLACEMENT_VOCABULARY,\n",
    ")\n",
    "\n",
    "# Collect records and build heatmap as before\n",
    "secret_prefix = \"\"\n",
    "all_records = collect_length_key_distance_records(\n",
    "    secret_texts_by_length=secret_texts_by_length,\n",
    "    secret_prefix=secret_prefix,\n",
    "    keys=keys,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "heatmap_values, bin_edges, sorted_lengths = build_heatmap_from_records(\n",
    "    all_records=all_records,\n",
    "    secret_lengths=[10, 20, 40, 80, 160, 320],\n",
    "    number_of_key_distance_bins=10,\n",
    ")\n",
    "\n",
    "plot_length_vs_key_distance_heatmap(\n",
    "    heatmap_values=heatmap_values,\n",
    "    bin_edges=bin_edges,\n",
    "    secret_lengths=sorted_lengths,\n",
    "    output_filename=\"length_vs_key_distance_heatmap.png\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min d_k_norm: 0.11428571428571428\n",
      "max d_k_norm: 0.8787878787878788\n",
      "mean d_k_norm: 0.6089303989276882\n"
     ]
    }
   ],
   "source": [
    "base_key = \"Puppies cats coding travel travel.\"\n",
    "\n",
    "keys = build_keys_for_heatmap(\n",
    "    base_key=base_key,\n",
    "    number_of_levels=12,      # how finely you want to span 0..1\n",
    "    samples_per_level=4,      # number of variants per edit-count\n",
    "    replacement_vocabulary=REPLACEMENT_VOCABULARY,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "\n",
    "def inspect_key_distance_distribution(keys):\n",
    "    distances = []\n",
    "    for i in range(len(keys)):\n",
    "        for j in range(i + 1, len(keys)):\n",
    "            d_raw = Levenshtein.distance(keys[i], keys[j])\n",
    "            d_norm = d_raw / max(len(keys[i]), len(keys[j]), 1)\n",
    "            distances.append(d_norm)\n",
    "    distances = np.array(distances)\n",
    "    print(\"min d_k_norm:\", distances.min())\n",
    "    print(\"max d_k_norm:\", distances.max())\n",
    "    print(\"mean d_k_norm:\", distances.mean())\n",
    "\n",
    "inspect_key_distance_distribution(keys)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# token distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Sequence\n",
    "import Levenshtein\n",
    "from llama_cpp import Llama\n",
    "\n",
    "\n",
    "def tokenize_key_for_distance(key: str, model: Llama) -> List[int]:\n",
    "    \"\"\"\n",
    "    Tokenize a key using the same scheme as prompts in _make_prefix_ids,\n",
    "    but WITHOUT the BOS heuristic for the empty string.\n",
    "\n",
    "    For non-empty keys this mirrors _make_prefix_ids:\n",
    "      - add_bos=True, then drop the BOS token.\n",
    "    For empty keys, return an empty list.\n",
    "    \"\"\"\n",
    "    if not key:\n",
    "        return []\n",
    "\n",
    "    token_ids = model.tokenize(key.encode(\"utf-8\"), add_bos=True)\n",
    "    # drop BOS\n",
    "    return token_ids[1:]\n",
    "\n",
    "\n",
    "def tokenize_text_for_distance(text: str, model: Llama) -> List[int]:\n",
    "    \"\"\"\n",
    "    Tokenize a generic text (e.g. stegotext) in the same way\n",
    "    you already do in get_token_ranks_like_paper, i.e.:\n",
    "\n",
    "      tokenize(\" \" + text, add_bos=True)[1:]\n",
    "\n",
    "    This keeps distances consistent with the tokenization used\n",
    "    in the stenography protocol. :contentReference[oaicite:0]{index=0}\n",
    "    \"\"\"\n",
    "    text = text.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
    "    token_ids = model.tokenize((\" \" + text).encode(\"utf-8\"), add_bos=True)[1:]\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CODEPOINT_FOR_TOKENS = 0x10000  # safely above ASCII range\n",
    "\n",
    "\n",
    "def tokens_to_pseudo_string(token_ids: Sequence[int]) -> str:\n",
    "    \"\"\"\n",
    "    Map each token id to a single Unicode codepoint so that\n",
    "    python-Levenshtein can compute edit distance over tokens.\n",
    "    \"\"\"\n",
    "    return \"\".join(chr(BASE_CODEPOINT_FOR_TOKENS + int(token_id)) for token_id in token_ids)\n",
    "\n",
    "\n",
    "def token_levenshtein_raw(token_ids_one: Sequence[int], token_ids_two: Sequence[int]) -> int:\n",
    "    \"\"\"\n",
    "    Raw Levenshtein distance in TOKEN space.\n",
    "    \"\"\"\n",
    "    string_one = tokens_to_pseudo_string(token_ids_one)\n",
    "    string_two = tokens_to_pseudo_string(token_ids_two)\n",
    "    return Levenshtein.distance(string_one, string_two)\n",
    "\n",
    "\n",
    "def token_levenshtein_normalized(token_ids_one: Sequence[int], token_ids_two: Sequence[int]) -> float:\n",
    "    \"\"\"\n",
    "    Normalized token-level Levenshtein distance in [0, 1].\n",
    "    \"\"\"\n",
    "    raw_distance = token_levenshtein_raw(token_ids_one, token_ids_two)\n",
    "    maximum_length = max(len(token_ids_one), len(token_ids_two), 1)\n",
    "    return raw_distance / maximum_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def compute_pairwise_key_and_stego_distances_token_level(\n",
    "    keys: Sequence[str],\n",
    "    stegotext_by_key: Dict[str, str],\n",
    "    model: Llama = llm,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For all pairs of keys (k1, k2):\n",
    "\n",
    "      - compute token-level Levenshtein distance between keys\n",
    "      - compute token-level Levenshtein distance between stegotexts\n",
    "\n",
    "    Returns a list of records, one per pair, containing:\n",
    "      - key_one, key_two\n",
    "      - stego_one, stego_two\n",
    "      - d_k_token_raw, d_k_token_norm\n",
    "      - d_s_token_raw, d_s_token_norm\n",
    "    \"\"\"\n",
    "\n",
    "    # Cache tokenizations so we do not retokenize in the inner loop\n",
    "    key_tokens: Dict[str, List[int]] = {\n",
    "        key: tokenize_key_for_distance(key, model=model) for key in keys\n",
    "    }\n",
    "    stego_tokens: Dict[str, List[int]] = {\n",
    "        key: tokenize_text_for_distance(stegotext_by_key[key], model=model) for key in keys\n",
    "    }\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for key_one, key_two in combinations(keys, 2):\n",
    "        stego_one = stegotext_by_key[key_one]\n",
    "        stego_two = stegotext_by_key[key_two]\n",
    "\n",
    "        key_tokens_one = key_tokens[key_one]\n",
    "        key_tokens_two = key_tokens[key_two]\n",
    "\n",
    "        stego_tokens_one = stego_tokens[key_one]\n",
    "        stego_tokens_two = stego_tokens[key_two]\n",
    "\n",
    "        # Key distances (token-level)\n",
    "        d_k_token_raw = token_levenshtein_raw(key_tokens_one, key_tokens_two)\n",
    "        d_k_token_norm = token_levenshtein_normalized(key_tokens_one, key_tokens_two)\n",
    "\n",
    "        # Stegotext distances (token-level)\n",
    "        d_s_token_raw = token_levenshtein_raw(stego_tokens_one, stego_tokens_two)\n",
    "        d_s_token_norm = token_levenshtein_normalized(stego_tokens_one, stego_tokens_two)\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"key_one\": key_one,\n",
    "                \"key_two\": key_two,\n",
    "                \"stego_one\": stego_one,\n",
    "                \"stego_two\": stego_two,\n",
    "                \"d_k_token_raw\": d_k_token_raw,\n",
    "                \"d_k_token_norm\": d_k_token_norm,\n",
    "                \"d_s_token_raw\": d_s_token_raw,\n",
    "                \"d_s_token_norm\": d_s_token_norm,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_token_level_key_vs_stego_levenshtein(\n",
    "    pairwise_records: Sequence[Dict[str, Any]],\n",
    "    output_directory: Path = REPO_ROOT / \"results\",\n",
    "    output_filename: str = \"key_vs_stego_token_levenshtein_scatter.png\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Scatter plot of normalized token-level distances:\n",
    "\n",
    "      x axis: d_k_token_norm (keys)\n",
    "      y axis: d_s_token_norm (stegotexts)\n",
    "    \"\"\"\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    x_values = [record[\"d_k_token_norm\"] for record in pairwise_records]\n",
    "    y_values = [record[\"d_s_token_norm\"] for record in pairwise_records]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(x_values, y_values, alpha=0.7)\n",
    "    plt.xlabel(\"Key token-level Levenshtein distance (normalized)\")\n",
    "    plt.ylabel(\"Stegotext token-level Levenshtein distance (normalized)\")\n",
    "    plt.title(\"Sensitivity of stegotext to key (token-level distance)\")\n",
    "    plt.grid(True)\n",
    "    plt.xlim(0.0, 1.0)\n",
    "    plt.ylim(0.0, 1.05)\n",
    "\n",
    "\n",
    "    output_path = output_directory / output_filename\n",
    "    plt.savefig(output_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved token-level scatter plot to: {output_path}\")\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "key_one: Puppies cats coding travel travel.\n",
      "key_two: Music puppies happy dogs sad.\n",
      "d_k_token_norm: 0.8571428571428571\n",
      "d_s_token_norm: 1.0\n",
      "===\n",
      "key_one: Puppies cats coding travel travel.\n",
      "key_two: Summer kittens cats dogs coffee.\n",
      "d_k_token_norm: 0.7142857142857143\n",
      "d_s_token_norm: 1.0\n",
      "===\n",
      "key_one: Puppies cats coding travel travel.\n",
      "key_two: Travel rainy quiet cats happy.\n",
      "d_k_token_norm: 0.8571428571428571\n",
      "d_s_token_norm: 1.0\n",
      "Saved token-level scatter plot to: /home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_token_levenshtein_scatter.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_token_levenshtein_scatter.png')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixed secret text and prefix (k')\n",
    "secret_text = \"The cats like to meow all the time. It is annoying.\"\n",
    "secret_prefix = \"\"  # or \"A text:\" if you want to match the paper's examples\n",
    "\n",
    "#  Precompute ranks for the secret once\n",
    "ranks_for_secret = precompute_ranks_for_secret(\n",
    "    secret_text=secret_text,\n",
    "    secret_prefix=secret_prefix,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "#  Keys: same style as before (for example, 30 keys, 5 words each)\n",
    "number_of_keys = 30\n",
    "number_of_words_per_key = 5\n",
    "\n",
    "keys = generate_random_keys(\n",
    "    number_of_keys=number_of_keys,\n",
    "    number_of_words=number_of_words_per_key,\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "#  Generate stegotexts for each key using the fixed secret\n",
    "stegotext_by_key = generate_stegotexts_for_keys(\n",
    "    ranks_for_secret=ranks_for_secret,\n",
    "    keys=keys,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "#  Compute pairwise TOKEN-LEVEL distances\n",
    "pairwise_records_token = compute_pairwise_key_and_stego_distances_token_level(\n",
    "    keys=keys,\n",
    "    stegotext_by_key=stegotext_by_key,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# inspect a couple of records\n",
    "for record in pairwise_records_token[:3]:\n",
    "    print(\"===\")\n",
    "    print(\"key_one:\", record[\"key_one\"])\n",
    "    print(\"key_two:\", record[\"key_two\"])\n",
    "    print(\"d_k_token_norm:\", record[\"d_k_token_norm\"])\n",
    "    print(\"d_s_token_norm:\", record[\"d_s_token_norm\"])\n",
    "\n",
    "# Plot the scatter\n",
    "plot_token_level_key_vs_stego_levenshtein(pairwise_records_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def build_keys_for_token_range(\n",
    "    base_key: str,\n",
    "    number_of_levels: int,\n",
    "    samples_per_level: int,\n",
    "    replacement_vocabulary: List[str],\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Create a set of keys by taking `base_key` and generating word-level variants\n",
    "    across a range of replacement counts. This gives you pairs spanning small,\n",
    "    medium, and large token-level Levenshtein distances.\n",
    "    \"\"\"\n",
    "    # word-edit counts from 0 up to len(words)\n",
    "    edit_counts = build_uniform_word_edit_counts(\n",
    "        base_key=base_key,\n",
    "        number_of_levels=number_of_levels,\n",
    "    )\n",
    "\n",
    "    keys: List[str] = [base_key]\n",
    "\n",
    "    for edit_count in edit_counts:\n",
    "        if edit_count == 0:\n",
    "            continue  # base key already included\n",
    "\n",
    "        for sample_index in range(samples_per_level):\n",
    "            mutated_key = make_key_with_word_replacements(\n",
    "                base_key=base_key,\n",
    "                number_of_word_replacements=edit_count,\n",
    "                replacement_vocabulary=replacement_vocabulary,\n",
    "                random_seed=20_000 + edit_count * 100 + sample_index,\n",
    "            )\n",
    "            keys.append(mutated_key)\n",
    "\n",
    "    # Deduplicate, preserving order\n",
    "    seen = set()\n",
    "    unique_keys: List[str] = []\n",
    "    for key in keys:\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_keys.append(key)\n",
    "\n",
    "    return unique_keys\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def print_random_key_stego_examples(\n",
    "    pairwise_records: Sequence[Dict[str, Any]],\n",
    "    number_of_examples: int = 5,\n",
    "    random_seed: int = 0,\n",
    ") -> None:\n",
    "    random_generator = random.Random(random_seed)\n",
    "    if number_of_examples > len(pairwise_records):\n",
    "        number_of_examples = len(pairwise_records)\n",
    "\n",
    "    sampled_records = random_generator.sample(pairwise_records, number_of_examples)\n",
    "\n",
    "    for example_index, record in enumerate(sampled_records, start=1):\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Example {example_index}\")\n",
    "        print(f\"d_k_token_norm: {record['d_k_token_norm']:.3f}\")\n",
    "        print(f\"d_s_token_norm: {record['d_s_token_norm']:.3f}\")\n",
    "        print()\n",
    "        print(\"key_one:\")\n",
    "        print(record[\"key_one\"])\n",
    "        print()\n",
    "        print(\"key_two:\")\n",
    "        print(record[\"key_two\"])\n",
    "        print()\n",
    "        print(\"stego_one:\")\n",
    "        print(record[\"stego_one\"])\n",
    "        print()\n",
    "        print(\"stego_two:\")\n",
    "        print(record[\"stego_two\"])\n",
    "        print()\n",
    "\n",
    "def print_extreme_key_stego_examples(\n",
    "    pairwise_records: Sequence[Dict[str, Any]],\n",
    "    number_of_examples_per_side: int = 3,\n",
    ") -> None:\n",
    "    sorted_records = sorted(pairwise_records, key=lambda record: record[\"d_k_token_norm\"])\n",
    "    total = len(sorted_records)\n",
    "\n",
    "    # smallest key distances\n",
    "    print(\"\\n\" + \"#\" * 30 + \" Smallest key distances \" + \"#\" * 30)\n",
    "    for record in sorted_records[:number_of_examples_per_side]:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"d_k_token_norm: {record['d_k_token_norm']:.3f}\")\n",
    "        print(f\"d_s_token_norm: {record['d_s_token_norm']:.3f}\")\n",
    "        print(\"key_one:\", record[\"key_one\"])\n",
    "        print(\"key_two:\", record[\"key_two\"])\n",
    "        print()\n",
    "\n",
    "    # largest key distances\n",
    "    print(\"\\n\" + \"#\" * 30 + \" Largest key distances \" + \"#\" * 30)\n",
    "    for record in sorted_records[-number_of_examples_per_side:]:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"d_k_token_norm: {record['d_k_token_norm']:.3f}\")\n",
    "        print(f\"d_s_token_norm: {record['d_s_token_norm']:.3f}\")\n",
    "        print(\"key_one:\", record[\"key_one\"])\n",
    "        print(\"key_two:\", record[\"key_two\"])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min d_k_token_norm: 0.06666666666666667\n",
      "max d_k_token_norm: 0.9375\n",
      "mean d_k_token_norm: 0.7178985696842839\n",
      "Saved token-level scatter plot to: /home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_token_levenshtein_sweep.png\n",
      "================================================================================\n",
      "Example 1\n",
      "d_k_token_norm: 0.800\n",
      "d_s_token_norm: 1.000\n",
      "\n",
      "key_one:\n",
      "Puppies cats coding travel travel during long quiet travel evenings by the sea.\n",
      "\n",
      "key_two:\n",
      "Coding morning sunny tea puppies morning movies sunny winter rainy by running sea.\n",
      "\n",
      "stego_one:\n",
      "_LAYOUT довольно широкий шабдон, который может изменять свое полож\n",
      "\n",
      "stego_two:\n",
      ".cd===============END OF FOOTER===\");\n",
      "```\n",
      "And the resulting output\n",
      "\n",
      "================================================================================\n",
      "Example 2\n",
      "d_k_token_norm: 0.867\n",
      "d_s_token_norm: 1.000\n",
      "\n",
      "key_one:\n",
      "Puppies cats coding travel movies during long quiet morning puppies by the sea.\n",
      "\n",
      "key_two:\n",
      "Music summer books movies busy coding cats quiet summer morning rainy tea coding.\n",
      "\n",
      "stego_one:\n",
      "Stub clash free. a. [1] The film is the first\n",
      "\n",
      "stego_two:\n",
      "كس완يكسرانيك.\n",
      "\n",
      "I hope you can understand this.\n",
      "\n",
      "================================================================================\n",
      "Example 3\n",
      "d_k_token_norm: 0.800\n",
      "d_s_token_norm: 1.000\n",
      "\n",
      "key_one:\n",
      "Puppies cats quiet travel travel during long quiet summer evenings by the sea.\n",
      "\n",
      "key_two:\n",
      "Calm coding movies reading travel sunny evening quiet evening winter music summer happy.\n",
      "\n",
      "stego_one:\n",
      "}));\n",
      "\n",
      "HIGH_DESCRIPTION = '\n",
      "A classic summer evening by <strong>Santor\n",
      "\n",
      "stego_two:\n",
      "fırsat yöntem zaman. With the release of the new film \"La La\n",
      "\n",
      "================================================================================\n",
      "Example 4\n",
      "d_k_token_norm: 0.857\n",
      "d_s_token_norm: 1.000\n",
      "\n",
      "key_one:\n",
      "Summer cats summer movies travel tea long quiet music evenings by the sea.\n",
      "\n",
      "key_two:\n",
      "Quiet evening walking coffee coffee evening sunny winter puppies coding by coffee happy.\n",
      "\n",
      "stego_one:\n",
      "autofocus Pollys. life. - 1 year(s)\n",
      "    -\n",
      "\n",
      "stego_two:\n",
      "RectTransform  \n",
      "  \n",
      "DialogContent: text  \n",
      "🏳Description text  \n",
      "\n",
      "\n",
      "================================================================================\n",
      "Example 5\n",
      "d_k_token_norm: 0.733\n",
      "d_s_token_norm: 1.000\n",
      "\n",
      "key_one:\n",
      "Puppies cats coding travel movies during long quiet morning puppies by the sea.\n",
      "\n",
      "key_two:\n",
      "Puppies reading calm running travel calm running kittens busy calm by winter happy.\n",
      "\n",
      "stego_one:\n",
      "Stub clash free. a. [1] The film is the first\n",
      "\n",
      "stego_two:\n",
      ".github Singles read calm: running with puppies. Running, puppies reading,\n",
      "\n",
      "\n",
      "############################## Smallest key distances ##############################\n",
      "================================================================================\n",
      "d_k_token_norm: 0.067\n",
      "d_s_token_norm: 1.000\n",
      "key_one: Puppies cats coding travel travel during long quiet summer evenings by the sea.\n",
      "key_two: Puppies cats coding travel travel during long quiet travel evenings by the sea.\n",
      "\n",
      "================================================================================\n",
      "d_k_token_norm: 0.067\n",
      "d_s_token_norm: 1.000\n",
      "key_one: Puppies cats coding travel travel during long quiet summer evenings by the sea.\n",
      "key_two: Puppies cats coding travel travel during long books summer evenings by the sea.\n",
      "\n",
      "================================================================================\n",
      "d_k_token_norm: 0.067\n",
      "d_s_token_norm: 1.000\n",
      "key_one: Puppies cats coding travel travel during long quiet summer evenings by the sea.\n",
      "key_two: Puppies cats quiet travel travel during long quiet summer evenings by the sea.\n",
      "\n",
      "\n",
      "############################## Largest key distances ##############################\n",
      "================================================================================\n",
      "d_k_token_norm: 0.938\n",
      "d_s_token_norm: 1.000\n",
      "key_one: Kittens travel music kittens travel during puppies morning dogs walking travel kittens walking.\n",
      "key_two: Rainy quiet cats tea cats coffee long calm busy puppies kittens winter cats.\n",
      "\n",
      "================================================================================\n",
      "d_k_token_norm: 0.938\n",
      "d_s_token_norm: 1.000\n",
      "key_one: Kittens travel music kittens travel during puppies morning dogs walking travel kittens walking.\n",
      "key_two: Rainy movies cats morning coding sunny calm movies calm winter reading summer cats.\n",
      "\n",
      "================================================================================\n",
      "d_k_token_norm: 0.938\n",
      "d_s_token_norm: 1.000\n",
      "key_one: Kittens travel music kittens travel during puppies morning dogs walking travel kittens walking.\n",
      "key_two: Music busy running summer rainy summer music running travel summer busy summer reading.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "REPLACEMENT_VOCABULARY = [\n",
    "    \"cats\", \"kittens\", \"dogs\", \"puppies\",\n",
    "    \"music\", \"books\", \"coffee\", \"tea\",\n",
    "    \"travel\", \"coding\", \"movies\", \"reading\",\n",
    "    \"walking\", \"running\", \"summer\", \"winter\",\n",
    "    \"sunny\", \"rainy\", \"happy\", \"quiet\",\n",
    "    \"busy\", \"calm\", \"evening\", \"morning\",\n",
    "]\n",
    "\n",
    "base_key2 = (\n",
    "    \"Puppies cats coding travel travel during long quiet summer evenings by the sea.\"\n",
    ")\n",
    "\n",
    "keys = build_keys_for_token_range(\n",
    "    base_key=base_key2,\n",
    "    number_of_levels=16,       # many different edit counts\n",
    "    samples_per_level=6,       # a few variants per count\n",
    "    replacement_vocabulary=REPLACEMENT_VOCABULARY,\n",
    ")\n",
    "\n",
    "# fixed secret text and prefix\n",
    "secret_text = \"The cats like to meow all the time. It is annoying.\"\n",
    "secret_prefix = \"\"\n",
    "\n",
    "ranks_for_secret = precompute_ranks_for_secret(\n",
    "    secret_text=secret_text,\n",
    "    secret_prefix=secret_prefix,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "stegotext_by_key = generate_stegotexts_for_keys(\n",
    "    ranks_for_secret=ranks_for_secret,\n",
    "    keys=keys,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "pairwise_records_token = compute_pairwise_key_and_stego_distances_token_level(\n",
    "    keys=keys,\n",
    "    stegotext_by_key=stegotext_by_key,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "distances = np.array([r[\"d_k_token_norm\"] for r in pairwise_records_token])\n",
    "print(\"min d_k_token_norm:\", distances.min())\n",
    "print(\"max d_k_token_norm:\", distances.max())\n",
    "print(\"mean d_k_token_norm:\", distances.mean())\n",
    "\n",
    "\n",
    "# Scatter for the new key set (base_key2 variants)\n",
    "plot_token_level_key_vs_stego_levenshtein(\n",
    "    pairwise_records=pairwise_records_token,\n",
    "    output_directory=REPO_ROOT / \"results\",\n",
    "    output_filename=\"key_vs_stego_token_levenshtein_sweep.png\",\n",
    ")\n",
    "\n",
    "print_random_key_stego_examples(pairwise_records_token, number_of_examples=5, random_seed=123)\n",
    "\n",
    "print_extreme_key_stego_examples(pairwise_records_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any, Sequence\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def collect_length_key_distance_records_token_level(\n",
    "    secret_texts_by_length: Dict[int, List[str]],\n",
    "    secret_prefix: str,\n",
    "    keys: Sequence[str],\n",
    "    model: Llama = llm,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Token-level version of the length/key-distance experiment.\n",
    "\n",
    "    For each secret length L and each secret text e of that length:\n",
    "      1. Precompute ranks for e given secret_prefix k'.\n",
    "      2. Generate stegotext s(k; e) for every key in `keys`.\n",
    "      3. For all key pairs (k1, k2), compute:\n",
    "\n",
    "           - token-level Levenshtein between keys\n",
    "           - token-level Levenshtein between their stegotexts\n",
    "\n",
    "    Returns a flat list of records. Normalized token distances are stored in\n",
    "    fields `d_k_norm` and `d_s_norm` so they can be passed directly to\n",
    "    `build_heatmap_from_records`.\n",
    "    \"\"\"\n",
    "\n",
    "    all_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Tokenize keys once, since they do not depend on the secret text\n",
    "    key_tokens_by_key: Dict[str, List[int]] = {\n",
    "        key: tokenize_key_for_distance(key, model=model) for key in keys\n",
    "    }\n",
    "\n",
    "    for length_in_words, secret_text_list in secret_texts_by_length.items():\n",
    "        print(f\"Processing secret length {length_in_words} words \"\n",
    "              f\"({len(secret_text_list)} texts)\")\n",
    "\n",
    "        for secret_text in secret_text_list:\n",
    "            # 1. Ranks for this secret\n",
    "            ranks_for_secret = precompute_ranks_for_secret(\n",
    "                secret_text=secret_text,\n",
    "                secret_prefix=secret_prefix,\n",
    "                model=model,\n",
    "            )\n",
    "\n",
    "            # 2. Stegotexts for every key\n",
    "            stegotext_by_key: Dict[str, str] = generate_stegotexts_for_keys(\n",
    "                ranks_for_secret=ranks_for_secret,\n",
    "                keys=keys,\n",
    "                model=model,\n",
    "            )\n",
    "\n",
    "            # Tokenize stegotexts for this secret\n",
    "            stego_tokens_by_key: Dict[str, List[int]] = {\n",
    "                key: tokenize_text_for_distance(stegotext_by_key[key], model=model)\n",
    "                for key in keys\n",
    "            }\n",
    "\n",
    "            # 3. All pairwise token-level distances\n",
    "            for key_one, key_two in combinations(keys, 2):\n",
    "                key_tokens_one = key_tokens_by_key[key_one]\n",
    "                key_tokens_two = key_tokens_by_key[key_two]\n",
    "\n",
    "                stego_tokens_one = stego_tokens_by_key[key_one]\n",
    "                stego_tokens_two = stego_tokens_by_key[key_two]\n",
    "\n",
    "                # Key distances in token space\n",
    "                d_k_token_raw = token_levenshtein_raw(\n",
    "                    key_tokens_one,\n",
    "                    key_tokens_two,\n",
    "                )\n",
    "                d_k_token_norm = token_levenshtein_normalized(\n",
    "                    key_tokens_one,\n",
    "                    key_tokens_two,\n",
    "                )\n",
    "\n",
    "                # Stegotext distances in token space\n",
    "                d_s_token_raw = token_levenshtein_raw(\n",
    "                    stego_tokens_one,\n",
    "                    stego_tokens_two,\n",
    "                )\n",
    "                d_s_token_norm = token_levenshtein_normalized(\n",
    "                    stego_tokens_one,\n",
    "                    stego_tokens_two,\n",
    "                )\n",
    "\n",
    "                all_records.append(\n",
    "                    {\n",
    "                        \"secret_length_words\": length_in_words,\n",
    "                        \"key_one\": key_one,\n",
    "                        \"key_two\": key_two,\n",
    "                        \"stego_one\": stegotext_by_key[key_one],\n",
    "                        \"stego_two\": stegotext_by_key[key_two],\n",
    "                        \"d_k_token_raw\": d_k_token_raw,\n",
    "                        \"d_k_norm\": d_k_token_norm,   # token-level\n",
    "                        \"d_s_token_raw\": d_s_token_raw,\n",
    "                        \"d_s_norm\": d_s_token_norm,   # token-level\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return all_records\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_length_vs_key_distance_heatmap_token(\n",
    "    heatmap_values: np.ndarray,\n",
    "    bin_edges: np.ndarray,\n",
    "    secret_lengths: Sequence[int],\n",
    "    output_directory: Path = REPO_ROOT / \"results\",\n",
    "    output_filename: str = \"length_vs_key_distance_heatmap_token.png\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Same as your previous heatmap plotter, but labeled for token-level distances.\n",
    "    \"\"\"\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    number_of_key_distance_bins = heatmap_values.shape[1]\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    image = plt.imshow(\n",
    "        heatmap_values,\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "\n",
    "    plt.colorbar(image, label=\"Mean stegotext distance (token-level, normalized)\")\n",
    "\n",
    "    plt.xlabel(\"Key token-level Levenshtein distance (normalized, binned)\")\n",
    "    plt.ylabel(\"Secret text length (words)\")\n",
    "\n",
    "    plt.xticks(\n",
    "        ticks=range(number_of_key_distance_bins),\n",
    "        labels=[f\"{center:.2f}\" for center in bin_centers],\n",
    "        rotation=45,\n",
    "    )\n",
    "    plt.yticks(\n",
    "        ticks=range(len(secret_lengths)),\n",
    "        labels=[str(length_value) for length_value in secret_lengths],\n",
    "    )\n",
    "\n",
    "    plt.title(\"Stegotext distance vs key distance and secret length (token-level)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path = output_directory / output_filename\n",
    "    plt.savefig(output_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved token-level heatmap to: {output_path}\")\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys: 45\n",
      "Processing secret length 10 words (3 texts)\n",
      "Processing secret length 20 words (3 texts)\n",
      "Processing secret length 40 words (3 texts)\n",
      "Processing secret length 80 words (3 texts)\n",
      "Processing secret length 160 words (3 texts)\n",
      "Processing secret length 320 words (3 texts)\n",
      "Saved token-level heatmap to: /home/meow/Documents/repos/LlmStenoExplore/results/length_vs_key_distance_heatmap_token.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/meow/Documents/repos/LlmStenoExplore/results/length_vs_key_distance_heatmap_token.png')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a corpus and sample secret texts of various lengths\n",
    "\n",
    "with open(REPO_ROOT / \"data\" / \"corpus.txt\", \"r\", encoding=\"utf-8\") as file_handle:\n",
    "    corpus_text = file_handle.read()\n",
    "\n",
    "target_word_lengths = [10, 20, 40, 80, 160, 320]\n",
    "samples_per_length = 3\n",
    "\n",
    "secret_texts_by_length = sample_secret_text_spans(\n",
    "    corpus_text=corpus_text,\n",
    "    target_word_lengths=target_word_lengths,\n",
    "    samples_per_length=samples_per_length,\n",
    "    random_seed=123,\n",
    ")\n",
    "\n",
    "# Build structured keys that span a wide token-distance range\n",
    "\n",
    "REPLACEMENT_VOCABULARY = [\n",
    "    \"cats\", \"kittens\", \"dogs\", \"puppies\",\n",
    "    \"music\", \"books\", \"coffee\", \"tea\",\n",
    "    \"travel\", \"coding\", \"movies\", \"reading\",\n",
    "    \"walking\", \"running\", \"summer\", \"winter\",\n",
    "    \"sunny\", \"rainy\", \"happy\", \"quiet\",\n",
    "    \"busy\", \"calm\", \"evening\", \"morning\",\n",
    "]\n",
    "\n",
    "base_key2 = (\n",
    "    \"Puppies cats coding travel travel during long quiet summer evenings by the sea.\"\n",
    ")\n",
    "\n",
    "keys = build_keys_for_token_range(\n",
    "    base_key=base_key2,\n",
    "    number_of_levels=12,\n",
    "    samples_per_level=4,\n",
    "    replacement_vocabulary=REPLACEMENT_VOCABULARY,\n",
    ")\n",
    "print(\"Number of keys:\", len(keys))\n",
    "\n",
    "# Collect token-level key/stego distances across all lengths\n",
    "\n",
    "secret_prefix = \"\"  # or \"A text:\" if you want to match the paper's k' setup\n",
    "\n",
    "all_records_token = collect_length_key_distance_records_token_level(\n",
    "    secret_texts_by_length=secret_texts_by_length,\n",
    "    secret_prefix=secret_prefix,\n",
    "    keys=keys,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# Build the heatmap (bins over d_k_token_norm)\n",
    "\n",
    "secret_lengths_for_this_run = sorted(secret_texts_by_length.keys())\n",
    "\n",
    "heatmap_values_token, bin_edges_token, sorted_lengths_token = build_heatmap_from_records(\n",
    "    all_records=all_records_token,\n",
    "    secret_lengths=secret_lengths_for_this_run,\n",
    "    number_of_key_distance_bins=10,\n",
    ")\n",
    "\n",
    "# Plot token-level version of your \"length vs key distance\" figure\n",
    "\n",
    "plot_length_vs_key_distance_heatmap_token(\n",
    "    heatmap_values=heatmap_values_token,\n",
    "    bin_edges=bin_edges_token,\n",
    "    secret_lengths=sorted_lengths_token,\n",
    "    output_filename=\"length_vs_key_distance_heatmap_token.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# token Hamming under the Llama tokenizer\n",
    "\n",
    "all stegotexts have the same Llama token length and follow the same rank pattern. That makes Hamming distance\n",
    "\n",
    "dstok(s1,s2)= 1/n \\sum_i^n \\delta [t_i^(1) \\neq t_i^(2)]\n",
    "\n",
    "For each token position i, estimate\n",
    "\n",
    "p_i = P[t_i^(1) \\neq t_i^2(2)∣key differs]\n",
    "\n",
    "aggregated over many stegotext pairs. Plot p_i versus position. If the encoder behaves like a good avalanche system, you should see p_i close to 1/2 or higher across almost all positions. If some prefix is unusually stable, that is interesting for both security and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple\n",
    "from llama_cpp import Llama\n",
    "\n",
    "\n",
    "def token_hamming_raw_and_normalized_from_ids(\n",
    "    token_ids_one: Sequence[int],\n",
    "    token_ids_two: Sequence[int],\n",
    ") -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Raw and normalized Hamming distance between two token sequences.\n",
    "\n",
    "      raw  = number of positions where tokens differ\n",
    "      norm = raw / n, where n is the common length (or min length if unequal)\n",
    "\n",
    "    For the stenography protocol, stegotexts generated with the same rank\n",
    "    sequence should have identical lengths in token space, so n should match.\n",
    "    \"\"\"\n",
    "    if not token_ids_one or not token_ids_two:\n",
    "        return 0, 0.0\n",
    "\n",
    "    if len(token_ids_one) != len(token_ids_two):\n",
    "        # Defensive: in theory they should match, but clip to min just in case.\n",
    "        length = min(len(token_ids_one), len(token_ids_two))\n",
    "    else:\n",
    "        length = len(token_ids_one)\n",
    "\n",
    "    mismatches = sum(\n",
    "        1\n",
    "        for token_one, token_two in zip(token_ids_one[:length], token_ids_two[:length])\n",
    "        if token_one != token_two\n",
    "    )\n",
    "\n",
    "    normalized = mismatches / max(length, 1)\n",
    "    return mismatches, normalized\n",
    "\n",
    "\n",
    "def token_hamming_raw_and_normalized_for_stegotexts(\n",
    "    stego_one: str,\n",
    "    stego_two: str,\n",
    "    model: Llama,\n",
    ") -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: tokenize two stegotexts as you already do for\n",
    "    distance computations and return raw + normalized token-level Hamming.\n",
    "    \"\"\"\n",
    "    tokens_one = tokenize_text_for_distance(stego_one, model=model)\n",
    "    tokens_two = tokenize_text_for_distance(stego_two, model=model)\n",
    "    return token_hamming_raw_and_normalized_from_ids(tokens_one, tokens_two)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List, Sequence\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def compute_pairwise_key_and_stego_distances_token_hamming(\n",
    "    keys: Sequence[str],\n",
    "    stegotext_by_key: Dict[str, str],\n",
    "    model: Llama = llm,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    For all pairs of keys (k1, k2):\n",
    "\n",
    "      - d_k_token_norm: token-level Levenshtein distance between keys\n",
    "      - d_s_hamming_norm: token-level Hamming distance between stegotexts\n",
    "\n",
    "    Returns a list of records, one per pair, with fields:\n",
    "      key_one, key_two, stego_one, stego_two,\n",
    "      d_k_token_raw, d_k_token_norm,\n",
    "      d_s_hamming_raw, d_s_hamming_norm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cache tokenizations\n",
    "    key_tokens_by_key: Dict[str, List[int]] = {\n",
    "        key: tokenize_key_for_distance(key, model=model)\n",
    "        for key in keys\n",
    "    }\n",
    "    stego_tokens_by_key: Dict[str, List[int]] = {\n",
    "        key: tokenize_text_for_distance(stegotext_by_key[key], model=model)\n",
    "        for key in keys\n",
    "    }\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for key_one, key_two in combinations(keys, 2):\n",
    "        stego_one = stegotext_by_key[key_one]\n",
    "        stego_two = stegotext_by_key[key_two]\n",
    "\n",
    "        key_tokens_one = key_tokens_by_key[key_one]\n",
    "        key_tokens_two = key_tokens_by_key[key_two]\n",
    "\n",
    "        stego_tokens_one = stego_tokens_by_key[key_one]\n",
    "        stego_tokens_two = stego_tokens_by_key[key_two]\n",
    "\n",
    "        # Key distances: token-level Levenshtein (as before)\n",
    "        d_k_token_raw = token_levenshtein_raw(key_tokens_one, key_tokens_two)\n",
    "        d_k_token_norm = token_levenshtein_normalized(key_tokens_one, key_tokens_two)\n",
    "\n",
    "        # Stegotext distances: token-level Hamming\n",
    "        d_s_hamming_raw, d_s_hamming_norm = token_hamming_raw_and_normalized_from_ids(\n",
    "            stego_tokens_one,\n",
    "            stego_tokens_two,\n",
    "        )\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"key_one\": key_one,\n",
    "                \"key_two\": key_two,\n",
    "                \"stego_one\": stego_one,\n",
    "                \"stego_two\": stego_two,\n",
    "                \"d_k_token_raw\": d_k_token_raw,\n",
    "                \"d_k_token_norm\": d_k_token_norm,\n",
    "                \"d_s_hamming_raw\": d_s_hamming_raw,\n",
    "                \"d_s_hamming_norm\": d_s_hamming_norm,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_token_key_vs_stego_hamming(\n",
    "    pairwise_records: Sequence[Dict[str, Any]],\n",
    "    output_directory: Path = REPO_ROOT / \"results\",\n",
    "    output_filename: str = \"key_vs_stego_token_hamming_scatter.png\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Scatter plot:\n",
    "\n",
    "      x axis: d_k_token_norm (token-level Levenshtein between keys)\n",
    "      y axis: d_s_hamming_norm (token-level Hamming between stegotexts)\n",
    "    \"\"\"\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    x_values = [record[\"d_k_token_norm\"] for record in pairwise_records]\n",
    "    y_values = [record[\"d_s_hamming_norm\"] for record in pairwise_records]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(x_values, y_values, alpha=0.7)\n",
    "    plt.xlabel(\"Key token-level Levenshtein distance (normalized)\")\n",
    "    plt.ylabel(\"Stegotext token-level Hamming distance (normalized)\")\n",
    "    plt.title(\"Sensitivity of stegotext to key (token-level Hamming)\")\n",
    "    plt.grid(True)\n",
    "    plt.xlim(0.0, 1.0)\n",
    "    plt.ylim(0.0, 1.05)\n",
    "\n",
    "    output_path = output_directory / output_filename\n",
    "    plt.savefig(output_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved token-level Hamming scatter plot to: {output_path}\")\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys: 45\n",
      "===\n",
      "key_one: Puppies cats coding travel travel during long quiet summer evenings by the sea.\n",
      "key_two: Calm cats coding travel travel during long quiet summer evenings by the sea.\n",
      "d_k_token_norm: 0.13333333333333333\n",
      "d_s_hamming_norm: 1.0\n",
      "===\n",
      "key_one: Puppies cats coding travel travel during long quiet summer evenings by the sea.\n",
      "key_two: Puppies cats coding travel travel during long quiet travel evenings by the sea.\n",
      "d_k_token_norm: 0.06666666666666667\n",
      "d_s_hamming_norm: 1.0\n",
      "===\n",
      "key_one: Puppies cats coding travel travel during long quiet summer evenings by the sea.\n",
      "key_two: Puppies cats coding travel travel during long books summer evenings by the sea.\n",
      "d_k_token_norm: 0.06666666666666667\n",
      "d_s_hamming_norm: 1.0\n",
      "Saved token-level Hamming scatter plot to: /home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_token_hamming_scatter.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/meow/Documents/repos/LlmStenoExplore/results/key_vs_stego_token_hamming_scatter.png')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixed secret text and prefix\n",
    "secret_text = \"The cats like to meow all the time. It is annoying.\"\n",
    "secret_prefix = \"\"  # or \"A text:\" if you want to match the paper's examples\n",
    "\n",
    "# Precompute ranks for the secret once\n",
    "ranks_for_secret = precompute_ranks_for_secret(\n",
    "    secret_text=secret_text,\n",
    "    secret_prefix=secret_prefix,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# Keys: reuse your structured key set spanning a wide distance range\n",
    "REPLACEMENT_VOCABULARY = [\n",
    "    \"cats\", \"kittens\", \"dogs\", \"puppies\",\n",
    "    \"music\", \"books\", \"coffee\", \"tea\",\n",
    "    \"travel\", \"coding\", \"movies\", \"reading\",\n",
    "    \"walking\", \"running\", \"summer\", \"winter\",\n",
    "    \"sunny\", \"rainy\", \"happy\", \"quiet\",\n",
    "    \"busy\", \"calm\", \"evening\", \"morning\",\n",
    "]\n",
    "\n",
    "base_key2 = (\n",
    "    \"Puppies cats coding travel travel during long quiet summer evenings by the sea.\"\n",
    ")\n",
    "\n",
    "keys = build_keys_for_token_range(\n",
    "    base_key=base_key2,\n",
    "    number_of_levels=12,\n",
    "    samples_per_level=4,\n",
    "    replacement_vocabulary=REPLACEMENT_VOCABULARY,\n",
    ")\n",
    "print(\"Number of keys:\", len(keys))\n",
    "\n",
    "# Generate stegotexts for each key using the fixed secret\n",
    "stegotext_by_key = generate_stegotexts_for_keys(\n",
    "    ranks_for_secret=ranks_for_secret,\n",
    "    keys=keys,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# Compute pairwise distances: key (token-Levenshtein) vs stego (token-Hamming)\n",
    "pairwise_records_hamming = compute_pairwise_key_and_stego_distances_token_hamming(\n",
    "    keys=keys,\n",
    "    stegotext_by_key=stegotext_by_key,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# Inspect a few examples\n",
    "for record in pairwise_records_hamming[:3]:\n",
    "    print(\"===\")\n",
    "    print(\"key_one:\", record[\"key_one\"])\n",
    "    print(\"key_two:\", record[\"key_two\"])\n",
    "    print(\"d_k_token_norm:\", record[\"d_k_token_norm\"])\n",
    "    print(\"d_s_hamming_norm:\", record[\"d_s_hamming_norm\"])\n",
    "\n",
    "# Plot the scatter\n",
    "plot_token_key_vs_stego_hamming(pairwise_records_hamming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any, Sequence\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def collect_length_key_distance_records_token_hamming(\n",
    "    secret_texts_by_length: Dict[int, List[str]],\n",
    "    secret_prefix: str,\n",
    "    keys: Sequence[str],\n",
    "    model: Llama = llm,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Length–vs–key sweep using:\n",
    "      - key distance:   token-level Levenshtein (normalized)\n",
    "      - stegotext distance: token-level Hamming (normalized)\n",
    "\n",
    "    For each secret length L and each secret text e of that length:\n",
    "      1. precompute ranks for e under secret_prefix,\n",
    "      2. generate stegotexts s(k; e) for all keys,\n",
    "      3. for all key pairs (k1, k2):\n",
    "           d_k_token_norm  (Levenshtein on key tokens)\n",
    "           d_s_hamming_norm (Hamming on stegotext tokens)\n",
    "\n",
    "    Returns a flat list of records. We store:\n",
    "      d_k_norm = d_k_token_norm\n",
    "      d_s_norm = d_s_hamming_norm\n",
    "    so that build_heatmap_from_records can be reused unchanged.\n",
    "    \"\"\"\n",
    "\n",
    "    all_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Tokenize keys once, they do not depend on the secret\n",
    "    key_tokens_by_key: Dict[str, List[int]] = {\n",
    "        key: tokenize_key_for_distance(key, model=model) for key in keys\n",
    "    }\n",
    "\n",
    "    for length_in_words, secret_text_list in secret_texts_by_length.items():\n",
    "        print(f\"Processing secret length {length_in_words} words \"\n",
    "              f\"({len(secret_text_list)} texts)\")\n",
    "\n",
    "        for secret_text in secret_text_list:\n",
    "            # 1. ranks for this secret\n",
    "            ranks_for_secret = precompute_ranks_for_secret(\n",
    "                secret_text=secret_text,\n",
    "                secret_prefix=secret_prefix,\n",
    "                model=model,\n",
    "            )\n",
    "\n",
    "            # 2. stegotexts for every key\n",
    "            stegotext_by_key: Dict[str, str] = generate_stegotexts_for_keys(\n",
    "                ranks_for_secret=ranks_for_secret,\n",
    "                keys=keys,\n",
    "                model=model,\n",
    "            )\n",
    "\n",
    "            # tokenize stegotexts for this secret\n",
    "            stego_tokens_by_key: Dict[str, List[int]] = {\n",
    "                key: tokenize_text_for_distance(stegotext_by_key[key], model=model)\n",
    "                for key in keys\n",
    "            }\n",
    "\n",
    "            # 3. all pairwise distances\n",
    "            for key_one, key_two in combinations(keys, 2):\n",
    "                key_tokens_one = key_tokens_by_key[key_one]\n",
    "                key_tokens_two = key_tokens_by_key[key_two]\n",
    "\n",
    "                stego_tokens_one = stego_tokens_by_key[key_one]\n",
    "                stego_tokens_two = stego_tokens_by_key[key_two]\n",
    "\n",
    "                # key distances (token-level Levenshtein)\n",
    "                d_k_token_raw = token_levenshtein_raw(\n",
    "                    key_tokens_one,\n",
    "                    key_tokens_two,\n",
    "                )\n",
    "                d_k_token_norm = token_levenshtein_normalized(\n",
    "                    key_tokens_one,\n",
    "                    key_tokens_two,\n",
    "                )\n",
    "\n",
    "                # stegotext distances (token-level Hamming)\n",
    "                d_s_hamming_raw, d_s_hamming_norm = token_hamming_raw_and_normalized_from_ids(\n",
    "                    stego_tokens_one,\n",
    "                    stego_tokens_two,\n",
    "                )\n",
    "\n",
    "                all_records.append(\n",
    "                    {\n",
    "                        \"secret_length_words\": length_in_words,\n",
    "                        \"key_one\": key_one,\n",
    "                        \"key_two\": key_two,\n",
    "                        \"stego_one\": stegotext_by_key[key_one],\n",
    "                        \"stego_two\": stegotext_by_key[key_two],\n",
    "                        \"d_k_token_raw\": d_k_token_raw,\n",
    "                        \"d_k_norm\": d_k_token_norm,       # key distance\n",
    "                        \"d_s_hamming_raw\": d_s_hamming_raw,\n",
    "                        \"d_s_norm\": d_s_hamming_norm,     # stego distance (Hamming)\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return all_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_length_vs_key_distance_heatmap_token_hamming(\n",
    "    heatmap_values: np.ndarray,\n",
    "    bin_edges: np.ndarray,\n",
    "    secret_lengths: Sequence[int],\n",
    "    output_directory: Path = REPO_ROOT / \"results\",\n",
    "    output_filename: str = \"length_vs_key_distance_heatmap_token_hamming.png\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Heatmap where:\n",
    "      - rows: secret text length (words),\n",
    "      - columns: key token-level Levenshtein distance bins,\n",
    "      - color: mean stegotext token-level Hamming distance.\n",
    "    \"\"\"\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    number_of_key_distance_bins = heatmap_values.shape[1]\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    image = plt.imshow(\n",
    "        heatmap_values,\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "\n",
    "    plt.colorbar(image, label=\"Mean stegotext distance (token-level Hamming, normalized)\")\n",
    "\n",
    "    plt.xlabel(\"Key token-level Levenshtein distance (normalized, binned)\")\n",
    "    plt.ylabel(\"Secret text length (words)\")\n",
    "\n",
    "    plt.xticks(\n",
    "        ticks=range(number_of_key_distance_bins),\n",
    "        labels=[f\"{center:.2f}\" for center in bin_centers],\n",
    "        rotation=45,\n",
    "    )\n",
    "    plt.yticks(\n",
    "        ticks=range(len(secret_lengths)),\n",
    "        labels=[str(length_value) for length_value in secret_lengths],\n",
    "    )\n",
    "\n",
    "    plt.title(\"Stegotext distance vs key distance and secret length (token-level Hamming)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path = output_directory / output_filename\n",
    "    plt.savefig(output_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved token-level Hamming heatmap to: {output_path}\")\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys: 45\n",
      "Processing secret length 10 words (3 texts)\n",
      "Processing secret length 20 words (3 texts)\n",
      "Processing secret length 40 words (3 texts)\n",
      "Processing secret length 80 words (3 texts)\n",
      "Processing secret length 160 words (3 texts)\n",
      "Processing secret length 320 words (3 texts)\n",
      "Saved token-level Hamming heatmap to: /home/meow/Documents/repos/LlmStenoExplore/results/length_vs_key_distance_heatmap_token_hamming.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/meow/Documents/repos/LlmStenoExplore/results/length_vs_key_distance_heatmap_token_hamming.png')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Corpus and secret texts (same as before)\n",
    "\n",
    "with open(REPO_ROOT / \"data\" / \"corpus.txt\", \"r\", encoding=\"utf-8\") as file_handle:\n",
    "    corpus_text = file_handle.read()\n",
    "\n",
    "target_word_lengths = [10, 20, 40, 80, 160, 320]\n",
    "samples_per_length = 3\n",
    "\n",
    "secret_texts_by_length = sample_secret_text_spans(\n",
    "    corpus_text=corpus_text,\n",
    "    target_word_lengths=target_word_lengths,\n",
    "    samples_per_length=samples_per_length,\n",
    "    random_seed=123,\n",
    ")\n",
    "\n",
    "# Keys spanning a wide token-distance range (same base key as your token-level work)\n",
    "\n",
    "REPLACEMENT_VOCABULARY = [\n",
    "    \"cats\", \"kittens\", \"dogs\", \"puppies\",\n",
    "    \"music\", \"books\", \"coffee\", \"tea\",\n",
    "    \"travel\", \"coding\", \"movies\", \"reading\",\n",
    "    \"walking\", \"running\", \"summer\", \"winter\",\n",
    "    \"sunny\", \"rainy\", \"happy\", \"quiet\",\n",
    "    \"busy\", \"calm\", \"evening\", \"morning\",\n",
    "]\n",
    "\n",
    "base_key2 = (\n",
    "    \"Puppies cats coding travel travel during long quiet summer evenings by the sea.\"\n",
    ")\n",
    "\n",
    "keys = build_keys_for_token_range(\n",
    "    base_key=base_key2,\n",
    "    number_of_levels=12,\n",
    "    samples_per_level=4,\n",
    "    replacement_vocabulary=REPLACEMENT_VOCABULARY,\n",
    ")\n",
    "print(\"Number of keys:\", len(keys))\n",
    "\n",
    "# Collect token-Levenshtein vs Hamming distances across all lengths\n",
    "\n",
    "secret_prefix = \"\"  # or \"A text:\" if you want to include k' for the secret\n",
    "\n",
    "all_records_hamming = collect_length_key_distance_records_token_hamming(\n",
    "    secret_texts_by_length=secret_texts_by_length,\n",
    "    secret_prefix=secret_prefix,\n",
    "    keys=keys,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# Build heatmap (using the generic builder)\n",
    "\n",
    "secret_lengths_for_this_run = sorted(secret_texts_by_length.keys())\n",
    "\n",
    "heatmap_values_hamming, bin_edges_hamming, sorted_lengths_hamming = build_heatmap_from_records(\n",
    "    all_records=all_records_hamming,\n",
    "    secret_lengths=secret_lengths_for_this_run,\n",
    "    number_of_key_distance_bins=10,\n",
    ")\n",
    "\n",
    "# Plot token-level Hamming version of the length-vs-key figure\n",
    "\n",
    "plot_length_vs_key_distance_heatmap_token_hamming(\n",
    "    heatmap_values=heatmap_values_hamming,\n",
    "    bin_edges=bin_edges_hamming,\n",
    "    secret_lengths=sorted_lengths_hamming,\n",
    "    output_filename=\"length_vs_key_distance_heatmap_token_hamming.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def print_random_hamming_examples(\n",
    "    records,\n",
    "    number_of_examples: int = 5,\n",
    "    random_seed: int = 0,\n",
    "    truncate_stego_chars: int = 300,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Print a few random examples from the sweep:\n",
    "\n",
    "      - secret length\n",
    "      - keys k1, k2\n",
    "      - d_k_token_norm (key distance)\n",
    "      - d_s_hamming_norm (stegotext distance)\n",
    "      - stegotexts (optionally truncated for readability)\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        print(\"No records to display.\")\n",
    "        return\n",
    "\n",
    "    random_generator = random.Random(random_seed)\n",
    "    number_of_examples = min(number_of_examples, len(records))\n",
    "    sampled_records = random_generator.sample(records, number_of_examples)\n",
    "\n",
    "    for idx, record in enumerate(sampled_records, start=1):\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Example {idx}\")\n",
    "        print(f\"secret_length_words : {record['secret_length_words']}\")\n",
    "        print(f\"d_k_token_norm      : {record['d_k_norm']:.3f}\")\n",
    "        print(f\"d_s_hamming_norm    : {record['d_s_norm']:.3f}\")\n",
    "        print()\n",
    "\n",
    "        print(\"key_one:\")\n",
    "        print(record[\"key_one\"])\n",
    "        print()\n",
    "\n",
    "        print(\"key_two:\")\n",
    "        print(record[\"key_two\"])\n",
    "        print()\n",
    "\n",
    "        stego_one = record[\"stego_one\"]\n",
    "        stego_two = record[\"stego_two\"]\n",
    "\n",
    "        if truncate_stego_chars is not None:\n",
    "            if len(stego_one) > truncate_stego_chars:\n",
    "                stego_one = stego_one[:truncate_stego_chars] + \"...\"\n",
    "            if len(stego_two) > truncate_stego_chars:\n",
    "                stego_two = stego_two[:truncate_stego_chars] + \"...\"\n",
    "\n",
    "        print(\"stego_one:\")\n",
    "        print(stego_one)\n",
    "        print()\n",
    "\n",
    "        print(\"stego_two:\")\n",
    "        print(stego_two)\n",
    "        print()\n",
    "\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Example 1\n",
      "secret_length_words : 10\n",
      "d_k_token_norm      : 0.933\n",
      "d_s_hamming_norm    : 1.000\n",
      "\n",
      "key_one:\n",
      "Puppies cats evening calm tea summer long puppies summer walking busy the sea.\n",
      "\n",
      "key_two:\n",
      "Calm coding movies reading travel sunny evening quiet evening winter music summer happy.\n",
      "\n",
      "stego_one:\n",
      "thigh-high acrylic fake nails making a comeback. The trend. The latest trend for\n",
      "\n",
      "stego_two:\n",
      "usic happy.grants.using. 0.5 seconds.5  \n",
      "\n",
      "================================================================================\n",
      "Example 2\n",
      "secret_length_words : 40\n",
      "d_k_token_norm      : 0.929\n",
      "d_s_hamming_norm    : 0.983\n",
      "\n",
      "key_one:\n",
      "Busy coding coffee travel travel morning long puppies summer reading sunny the quiet.\n",
      "\n",
      "key_two:\n",
      "Movies walking busy evening puppies cats coding cats coffee running kittens summer travel.\n",
      "\n",
      "stego_one:\n",
      "notices vanished. . . : : W 1 9. . .\n",
      "    # Pascal programming language:\n",
      "#   let   = \"HelloWorld\" : : W 0 0. : : W 1 7. : :\n",
      "\n",
      "#   if 1 then let   := \"\n",
      "\n",
      "stego_two:\n",
      "Rubio forest. Cats. Walks every Sunday. Cats cats coffee cats coding programming programming().\"\n",
      "    # Remove punctuation and lowercase\n",
      "#    print text\n",
      "    text = ''.join([i.lower() for i in text])\n",
      "    text=re.sub(r'\\d+',\"0\",  text\n",
      "\n",
      "================================================================================\n",
      "Example 3\n",
      "secret_length_words : 10\n",
      "d_k_token_norm      : 0.933\n",
      "d_s_hamming_norm    : 1.000\n",
      "\n",
      "key_one:\n",
      "Dogs calm coding coding cats busy long quiet summer happy calm quiet books.\n",
      "\n",
      "key_two:\n",
      "Morning quiet evening morning puppies during movies morning reading coffee by running travel.\n",
      "\n",
      "stego_one:\n",
      "acceleration\n",
      "Paula Andrea Gómoz 104 9\n",
      "\n",
      "stego_two:\n",
      "فيس Ritchie by by by own. Have you not\n",
      "\n",
      "================================================================================\n",
      "Example 4\n",
      "secret_length_words : 160\n",
      "d_k_token_norm      : 0.533\n",
      "d_s_hamming_norm    : 1.000\n",
      "\n",
      "key_one:\n",
      "Quiet sunny books travel travel during long quiet summer evenings books the sea.\n",
      "\n",
      "key_two:\n",
      "Tea sunny puppies travel travel cats sunny puppies summer tea busy the sea.\n",
      "\n",
      "stego_one:\n",
      "Sega Hat quite books travel I travel during quiet happy holidays. Miner hat books are my favorite travel during quiet summer evenings. The sea a book about travel happy holidays. Miner quite hat books travel.\n",
      "The hat books travel quiet summer evening sunshine. Miner hat quiet travel books during hap...\n",
      "\n",
      "stego_two:\n",
      ";\n",
      "LABEL White;\n",
      "LABEL FROM \"https:∶//ihatemyjob.files.user- uploaded.com/white.jpg\";\n",
      "```\n",
      "\n",
      "# 2.8.2\n",
      "The code snippet above includes the steps to create an image using certain parameters. The image will be a sunny day with puppies playing on the beach. It consists of multiple parts, which include ...\n",
      "\n",
      "================================================================================\n",
      "Example 5\n",
      "secret_length_words : 40\n",
      "d_k_token_norm      : 0.867\n",
      "d_s_hamming_norm    : 1.000\n",
      "\n",
      "key_one:\n",
      "Tea sunny puppies travel travel cats sunny puppies summer tea busy the sea.\n",
      "\n",
      "key_two:\n",
      "Quiet evening walking coffee coffee evening sunny winter puppies coding by coffee happy.\n",
      "\n",
      "stego_one:\n",
      "Deadpool Pullover by Deadpool Teas...\n",
      "  4.-Teas with Love: Seal with Love Seal Tea by Seal Tea with a hint tea love tea with a hint seal with a hint of tea with a hint of Seal with a Hint of Tea by Tea with a Tea with Tea with\n",
      "\n",
      "stego_two:\n",
      "ה reviewer mentioned various positive.ה benefits she experienced while working from coffee.\n",
      "From this HIT data, you could extract a variety of features, as listed here:\n",
      "1. Positive sentiment\n",
      "2.\n",
      "3. Mention of coffee-related activities (working, walking)\n",
      "5. Mention specific emotions/emotional\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_random_hamming_examples(all_records_hamming, number_of_examples=5, random_seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
