{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Sequence, Dict, Any\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "#   notebook is in LlmStenoExplore/notebooks\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "MODEL_REGISTRY = {\n",
    "    \"phi3_mini_q4\": REPO_ROOT / \"models/phi3/Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "    \"llama3_8b_q4_k_m\": REPO_ROOT / \"models/llama3_8b/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\",\n",
    "}\n",
    "\n",
    "def load_language_model(model_key: str) -> Llama:\n",
    "    model_path = MODEL_REGISTRY[model_key]\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "    maximum_context_tokens = 8192 if \"llama3\" in model_key else 4096\n",
    "\n",
    "    return Llama(\n",
    "        model_path=str(model_path),\n",
    "        n_ctx=maximum_context_tokens,\n",
    "        n_gpu_layers=0,\n",
    "        n_threads=os.cpu_count() or 4,\n",
    "        n_batch=256,\n",
    "        logits_all=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "llm = load_language_model(\"llama3_8b_q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _make_prefix_ids(prefix: str, model: Llama) -> List[int]:\n",
    "    \"\"\"\n",
    "    Turn a textual prefix (k or k') into initial context token ids.\n",
    "\n",
    "    - If prefix is non-empty: tokenize it and drop the BOS token\n",
    "      (this matches the authors' implementation).\n",
    "    - If prefix is empty: use a single BOS token.\n",
    "\n",
    "    This is used both in encoding (get_token_ranks_like_paper)\n",
    "    and decoding (decode_from_ranks_like_paper), so empty/non-empty\n",
    "    keys are treated consistently everywhere.\n",
    "    \"\"\"\n",
    "    if prefix:\n",
    "        # Tokenize with BOS, then drop BOS (index 0)\n",
    "        token_ids = model.tokenize(prefix.encode(\"utf-8\"), add_bos=True)\n",
    "        return token_ids[1:]\n",
    "    else:\n",
    "        # No textual prefix: start context from BOS\n",
    "        return [model.token_bos()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_token_ranks_like_paper(\n",
    "    text: str,\n",
    "    model: Llama,\n",
    "    prefix: str = \"A text:\",\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Token-level rank computation following the paper's recipe:\n",
    "\n",
    "      1. Tokenize e and k' with the LLM tokenizer.\n",
    "      2. For each token e_i, compute its rank among ALL vocab tokens\n",
    "         under p(· | k', e_1,...,e_{i-1}).\n",
    "\n",
    "    This mirrors the authors' get_token_ranks_llama_cpp, but uses\n",
    "    _make_prefix_ids so it behaves sensibly even when prefix == \"\".\n",
    "    \"\"\"\n",
    "    # Initial context tokens from k' (or BOS if prefix == \"\")\n",
    "    prefix_ids = _make_prefix_ids(prefix, model)\n",
    "\n",
    "    # Ensure text is valid UTF-8 and tokenize with leading space, drop BOS\n",
    "    text = text.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")\n",
    "    text_ids = model.tokenize((\" \" + text).encode(\"utf-8\"), add_bos=True)[1:]\n",
    "\n",
    "    model.reset()\n",
    "    model.eval(prefix_ids)\n",
    "\n",
    "    ranks: List[int] = []\n",
    "\n",
    "    # One rank per token in text_ids\n",
    "    for token_id in text_ids:\n",
    "        # logits for next token given current context\n",
    "        logits = np.array(model.scores[model.n_tokens - 1], dtype=np.float32)\n",
    "\n",
    "        # rank of token_id among all vocab entries (1-based)\n",
    "        sorted_indices = np.argsort(logits)[::-1]\n",
    "        positions = np.where(sorted_indices == token_id)[0]\n",
    "        if positions.size == 0:\n",
    "            raise RuntimeError(f\"Token id {token_id} not found in logits\")\n",
    "        rank = int(positions[0]) + 1\n",
    "        ranks.append(rank)\n",
    "\n",
    "        # extend context with this token\n",
    "        model.eval([token_id])\n",
    "\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_from_ranks_like_paper(\n",
    "    prompt: str,\n",
    "    ranks: List[int],\n",
    "    model: Llama,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Token-level decoder matching the paper's scheme:\n",
    "\n",
    "      - Turn prompt k or k' into initial context via _make_prefix_ids.\n",
    "      - For each rank r_i:\n",
    "          * get logits for next token given current context\n",
    "          * pick the r_i-th most probable token\n",
    "          * feed it and append to the sequence\n",
    "      - Detokenize and, if prompt is non-empty, strip it from the front.\n",
    "    \"\"\"\n",
    "    prompt_ids = _make_prefix_ids(prompt, model)\n",
    "\n",
    "    model.reset()\n",
    "    model.eval(prompt_ids)\n",
    "\n",
    "    generated_ids = list(prompt_ids)\n",
    "\n",
    "    for rank in ranks:\n",
    "        logits = np.array(model.scores[model.n_tokens - 1], dtype=np.float32)\n",
    "\n",
    "        sorted_indices = np.argsort(logits)[::-1]\n",
    "        if rank < 1 or rank > len(sorted_indices):\n",
    "            raise ValueError(\n",
    "                f\"Rank {rank} out of range for vocabulary size {len(sorted_indices)}\"\n",
    "            )\n",
    "\n",
    "        next_token_id = int(sorted_indices[rank - 1])\n",
    "        generated_ids.append(next_token_id)\n",
    "\n",
    "        model.eval([next_token_id])\n",
    "\n",
    "    decoded_bytes = model.detokenize(generated_ids)\n",
    "    decoded_text = decoded_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # If we had a textual prompt, strip it; for empty prompt we only had BOS,\n",
    "    # which normally does not render as visible text.\n",
    "    if prompt and decoded_text.startswith(prompt):\n",
    "        decoded_text = decoded_text[len(prompt):].lstrip()\n",
    "\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hide_text_token_level(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    secret_key: str,\n",
    "    model: Llama = llm,\n",
    ") -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Encode pipeline (e -> ranks -> stegotext):\n",
    "\n",
    "      1. Compute ranks for secret_text e after prefix k'.\n",
    "      2. Generate stegotext s from key k by following those ranks.\n",
    "    \"\"\"\n",
    "    ranks = get_token_ranks_like_paper(\n",
    "        text=secret_text,\n",
    "        model=model,\n",
    "        prefix=secret_prefix,\n",
    "    )\n",
    "    stegotext = decode_from_ranks_like_paper(\n",
    "        prompt=secret_key,\n",
    "        ranks=ranks,\n",
    "        model=model,\n",
    "    )\n",
    "    return stegotext, ranks\n",
    "\n",
    "\n",
    "def reveal_text_token_level(\n",
    "    stegotext: str,\n",
    "    secret_prefix: str,\n",
    "    secret_key: str,\n",
    "    model: Llama = llm,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Decode pipeline (s -> ranks -> e):\n",
    "\n",
    "      1. From stegotext s and key k, recover the same ranks.\n",
    "      2. From those ranks and prefix k', reconstruct e.\n",
    "    \"\"\"\n",
    "    recovered_ranks = get_token_ranks_like_paper(\n",
    "        text=stegotext,\n",
    "        model=model,\n",
    "        prefix=secret_key,\n",
    "    )\n",
    "    recovered_text = decode_from_ranks_like_paper(\n",
    "        prompt=secret_prefix,\n",
    "        ranks=recovered_ranks,\n",
    "        model=model,\n",
    "    )\n",
    "    return recovered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    secret_key: str,\n",
    "    model: Llama = llm,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run one full encode/decode example and log everything consistently:\n",
    "      - secret text\n",
    "      - ranks_e and their length\n",
    "      - stegotext\n",
    "      - token counts for secret and stego (same tokenization as get_token_ranks_like_paper)\n",
    "      - recovered text and equality check\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Secret text e:\")\n",
    "    print(secret_text)\n",
    "    print()\n",
    "\n",
    "    # Encode: e -> (ranks_e) -> stegotext\n",
    "    stegotext, ranks_e = hide_text_token_level(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        secret_key=secret_key,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    print(\"ranks_e (len = {}):\".format(len(ranks_e)))\n",
    "    print(ranks_e)\n",
    "    print()\n",
    "\n",
    "    print(\"Stegotext s:\")\n",
    "    print(stegotext)\n",
    "    print()\n",
    "\n",
    "    # Same tokenization scheme as get_token_ranks_like_paper\n",
    "    secret_token_ids = model.tokenize((\" \" + secret_text).encode(\"utf-8\"), add_bos=True)[1:]\n",
    "    stego_token_ids  = model.tokenize((\" \" + stegotext).encode(\"utf-8\"), add_bos=True)[1:]\n",
    "\n",
    "    print(\"Secret tokens :\", len(secret_token_ids))\n",
    "    print(\"Stego tokens  :\", len(stego_token_ids))\n",
    "    print(\"len(ranks_e)  :\", len(ranks_e))\n",
    "    print()\n",
    "\n",
    "    # Sanity checks\n",
    "    assert len(secret_token_ids) == len(ranks_e), \"Token count for e does not match len(ranks_e)\"\n",
    "    assert len(stego_token_ids)  == len(ranks_e), \"Token count for s does not match len(ranks_e)\"\n",
    "\n",
    "    # Decode: s -> (ranks) -> e\n",
    "    recovered_text = reveal_text_token_level(\n",
    "        stegotext=stegotext,\n",
    "        secret_prefix=secret_prefix,\n",
    "        secret_key=secret_key,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    print(\"Recovered e:\")\n",
    "    print(recovered_text)\n",
    "    print(\"Recovered == secret_text:\", recovered_text == secret_text)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Secret text e:\n",
      "THE CURRENT SYSTEM HAS REPEATEDLY FAILED\n",
      "\n",
      "ranks_e (len = 9):\n",
      "[164, 639, 21, 10, 10, 17, 1, 1, 1]\n",
      "\n",
      "Stegotext s:\n",
      "Get sufficient roas tting time. The\n",
      "\n",
      "Secret tokens : 9\n",
      "Stego tokens  : 9\n",
      "len(ranks_e)  : 9\n",
      "\n",
      "Recovered e:\n",
      "THE CURRENT SYSTEM HAS REPEATEDLY FAILED\n",
      "Recovered == secret_text: True\n",
      "------------------------------------------------------------\n",
      "================================================================================\n",
      "Secret text e:\n",
      "The cats like to meow all the time. It is annoying.\n",
      "\n",
      "ranks_e (len = 14):\n",
      "[89803, 14969, 58, 1, 106, 1, 26, 1, 1, 1, 6, 2, 6, 1]\n",
      "\n",
      "Stegotext s:\n",
      "önemlidir Ringvorstellung Schriftgutachten. They have sharp claws\n",
      "\n",
      "Secret tokens : 14\n",
      "Stego tokens  : 14\n",
      "len(ranks_e)  : 14\n",
      "\n",
      "Recovered e:\n",
      "The cats like to meow all the time. It is annoying.\n",
      "Recovered == secret_text: True\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "secret_text_1  = \"THE CURRENT SYSTEM HAS REPEATEDLY FAILED\"\n",
    "secret_prefix_1 = \"A text:\"   # k'\n",
    "secret_key_1    = \"Here it is: the infamous British roasted boar with mint sauce. How to make it perfect.\"\n",
    "\n",
    "run_example(secret_text_1, secret_prefix_1, secret_key_1, model=llm)\n",
    "\n",
    "# Example 2\n",
    "secret_text_2  = \"The cats like to meow all the time. It is annoying.\"\n",
    "secret_prefix_2 = \"\"  # k'\n",
    "secret_key_2    = \"The cat is a feline member just like lions and tigers but much smaller.\"  # k\n",
    "\n",
    "run_example(secret_text_2, secret_prefix_2, secret_key_2, model=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# goal context\n",
    "\n",
    "conduct research on the 'sensitivity of the key in respect to the stenography text produced, the stego text'. For instance if I have a key eg, 'I like cats' and then another 'I like kittens' to calculate the distance between the keys but then to find the equivalent distance between the stego text to see how much different is the keys are and the mapping to the distance between the stego texts. To do this for a few examples to see how that can work out. so effectively some simple examples of the (d_k, d_s| e) for the same message 'e' and some key and produce the stego text 's' and the distance 'd_k' and 'd_s'.\n",
    "\n",
    "studying here is the map\n",
    "\n",
    "k -> s(k;e)\n",
    "\n",
    "fixed hidden message e, e: how much does changing the key k change the stegotext s? we want empirical pairs\n",
    "\n",
    "(dk(k1,k2), ds(s1,s2)∣e), with  si=s(ki;e)\n",
    "\n",
    "\n",
    "## Distances for keys and stegotexts\n",
    "\n",
    "1. Character level edit distance, Normalized Levenshtein:\n",
    "\n",
    "d_k^{char}(k1,k2) = edit_distance(k1,k2) / max⁡(∣k1∣,∣k2∣,1)\n",
    "\n",
    "says how much you had to literally edit the string.\n",
    "\n",
    "(%pip install python-Levenshtein)\n",
    "\n",
    "2. Token level distance using the same tokenizer as the LLM. Since the protocol is token based, it is natural to look at key distance in token space.\n",
    "\n",
    "Let key_token_ids(k) be the tokenization you already use for prompts (via _make_prefix_ids, but without the BOS heuristic). For two keys with token sequences of possibly different lengths we can use a normalized edit distance in token space.\n",
    "\n",
    "3. Embedding distance\n",
    "\n",
    "In a sentence embedding model (for example a small sentence transformer) we can also measure cosine distance between embeddings of keys:\n",
    "\n",
    "dkemb(k1,k2)=1−cos⁡(emb(k1),emb(k2))\n",
    "\n",
    "says you how far apart the prompts are semantically, not just lexically.\n",
    "\n",
    "4. Token level Hamming distance under the Llama tokenizer.\n",
    "\n",
    "For fixed e, all stegotexts have exactly the same number of tokens (always use the same rank sequence), very clean:\n",
    "\n",
    "dstok(s1,s2)= 1/n \\sum_i^n  \\delta[t_i^(1) \\neq t_i^(2)]\n",
    "\n",
    "where t_i^(j) is the i-th token of stegotext sj\tin the Llama tokenizer and n is the common length. This is a per position token mismatch rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def levenshtein_raw(a: str, b: str) -> int:\n",
    "    \"\"\"\n",
    "    Raw Levenshtein edit distance between two strings.\n",
    "    \"\"\"\n",
    "    return Levenshtein.distance(a, b)\n",
    "\n",
    "\n",
    "def levenshtein_normalized(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Normalized Levenshtein distance in [0, 1], using max length\n",
    "    as the normalization factor.\n",
    "\n",
    "    0.0 means identical strings, values closer to 1.0 mean more different.\n",
    "    \"\"\"\n",
    "    raw_distance = Levenshtein.distance(a, b)\n",
    "    maximum_length = max(len(a), len(b))\n",
    "    if maximum_length == 0:\n",
    "        return 0.0\n",
    "    return raw_distance / maximum_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_key_and_stego_distances(\n",
    "    secret_text: str,\n",
    "    secret_prefix: str,\n",
    "    secret_key_one: str,\n",
    "    secret_key_two: str,\n",
    "    model: Llama = llm,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    For a fixed secret message e and prefix k',\n",
    "    generate stegotexts for two keys and compute:\n",
    "\n",
    "      - Levenshtein distance between the keys\n",
    "      - Levenshtein distance between the stegotexts\n",
    "\n",
    "    Returns a dictionary with:\n",
    "      - secret_key_one, secret_key_two\n",
    "      - stegotext_one, stegotext_two\n",
    "      - d_k_raw, d_k_norm\n",
    "      - d_s_raw, d_s_norm\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate stegotext for key 1\n",
    "    stegotext_one, _ = hide_text_token_level(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        secret_key=secret_key_one,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # Generate stegotext for key 2\n",
    "    stegotext_two, _ = hide_text_token_level(\n",
    "        secret_text=secret_text,\n",
    "        secret_prefix=secret_prefix,\n",
    "        secret_key=secret_key_two,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # Distances between keys\n",
    "    d_k_raw = levenshtein_raw(secret_key_one, secret_key_two)\n",
    "    d_k_norm = levenshtein_normalized(secret_key_one, secret_key_two)\n",
    "\n",
    "    # Distances between stegotexts\n",
    "    d_s_raw = levenshtein_raw(stegotext_one, stegotext_two)\n",
    "    d_s_norm = levenshtein_normalized(stegotext_one, stegotext_two)\n",
    "\n",
    "    return {\n",
    "        \"secret_key_one\": secret_key_one,\n",
    "        \"secret_key_two\": secret_key_two,\n",
    "        \"stegotext_one\": stegotext_one,\n",
    "        \"stegotext_two\": stegotext_two,\n",
    "        \"d_k_raw\": d_k_raw,\n",
    "        \"d_k_norm\": d_k_norm,\n",
    "        \"d_s_raw\": d_s_raw,\n",
    "        \"d_s_norm\": d_s_norm,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_text = \"The cats like to meow all the time. It is annoying.\"\n",
    "secret_prefix = \"\"  # or \"A text:\" if you want to use a prefix\n",
    "\n",
    "secret_key_one = \"I like cats\"\n",
    "secret_key_two = \"I like kittens\"\n",
    "\n",
    "result = compute_key_and_stego_distances(\n",
    "    secret_text=secret_text,\n",
    "    secret_prefix=secret_prefix,\n",
    "    secret_key_one=secret_key_one,\n",
    "    secret_key_two=secret_key_two,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
